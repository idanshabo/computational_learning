<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>PS3_Attention_Please_2024_ID_318632312</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iNTEiIHZpZXdCb3g9IjAgMCAzOSA1MSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMTYzOCAtMjI4MSkiPgogICAgIDxnIGNsYXNzPSJqcC1qdXB5dGVyLWljb24tY29sb3IiIGZpbGw9IiNGMzc3MjYiPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjM5Ljc0IDIzMTEuOTgpIiBkPSJNIDE4LjI2NDYgNy4xMzQxMUMgMTAuNDE0NSA3LjEzNDExIDMuNTU4NzIgNC4yNTc2IDAgMEMgMS4zMjUzOSAzLjgyMDQgMy43OTU1NiA3LjEzMDgxIDcuMDY4NiA5LjQ3MzAzQyAxMC4zNDE3IDExLjgxNTIgMTQuMjU1NyAxMy4wNzM0IDE4LjI2OSAxMy4wNzM0QyAyMi4yODIzIDEzLjA3MzQgMjYuMTk2MyAxMS44MTUyIDI5LjQ2OTQgOS40NzMwM0MgMzIuNzQyNCA3LjEzMDgxIDM1LjIxMjYgMy44MjA0IDM2LjUzOCAwQyAzMi45NzA1IDQuMjU3NiAyNi4xMTQ4IDcuMTM0MTEgMTguMjY0NiA3LjEzNDExWiIvPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjM5LjczIDIyODUuNDgpIiBkPSJNIDE4LjI3MzMgNS45MzkzMUMgMjYuMTIzNSA1LjkzOTMxIDMyLjk3OTMgOC44MTU4MyAzNi41MzggMTMuMDczNEMgMzUuMjEyNiA5LjI1MzAzIDMyLjc0MjQgNS45NDI2MiAyOS40Njk0IDMuNjAwNEMgMjYuMTk2MyAxLjI1ODE4IDIyLjI4MjMgMCAxOC4yNjkgMEMgMTQuMjU1NyAwIDEwLjM0MTcgMS4yNTgxOCA3LjA2ODYgMy42MDA0QyAzLjc5NTU2IDUuOTQyNjIgMS4zMjUzOSA5LjI1MzAzIDAgMTMuMDczNEMgMy41Njc0NSA4LjgyNDYzIDEwLjQyMzIgNS45MzkzMSAxOC4yNzMzIDUuOTM5MzFaIi8+CiAgICA8L2c+CiAgICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjY5LjMgMjI4MS4zMSkiIGQ9Ik0gNS44OTM1MyAyLjg0NEMgNS45MTg4OSAzLjQzMTY1IDUuNzcwODUgNC4wMTM2NyA1LjQ2ODE1IDQuNTE2NDVDIDUuMTY1NDUgNS4wMTkyMiA0LjcyMTY4IDUuNDIwMTUgNC4xOTI5OSA1LjY2ODUxQyAzLjY2NDMgNS45MTY4OCAzLjA3NDQ0IDYuMDAxNTEgMi40OTgwNSA1LjkxMTcxQyAxLjkyMTY2IDUuODIxOSAxLjM4NDYzIDUuNTYxNyAwLjk1NDg5OCA1LjE2NDAxQyAwLjUyNTE3IDQuNzY2MzMgMC4yMjIwNTYgNC4yNDkwMyAwLjA4MzkwMzcgMy42Nzc1N0MgLTAuMDU0MjQ4MyAzLjEwNjExIC0wLjAyMTIzIDIuNTA2MTcgMC4xNzg3ODEgMS45NTM2NEMgMC4zNzg3OTMgMS40MDExIDAuNzM2ODA5IDAuOTIwODE3IDEuMjA3NTQgMC41NzM1MzhDIDEuNjc4MjYgMC4yMjYyNTkgMi4yNDA1NSAwLjAyNzU5MTkgMi44MjMyNiAwLjAwMjY3MjI5QyAzLjYwMzg5IC0wLjAzMDcxMTUgNC4zNjU3MyAwLjI0OTc4OSA0Ljk0MTQyIDAuNzgyNTUxQyA1LjUxNzExIDEuMzE1MzEgNS44NTk1NiAyLjA1Njc2IDUuODkzNTMgMi44NDRaIi8+CiAgICAgIDxwYXRoIHRyYW5zZm9ybT0idHJhbnNsYXRlKDE2MzkuOCAyMzIzLjgxKSIgZD0iTSA3LjQyNzg5IDMuNTgzMzhDIDcuNDYwMDggNC4zMjQzIDcuMjczNTUgNS4wNTgxOSA2Ljg5MTkzIDUuNjkyMTNDIDYuNTEwMzEgNi4zMjYwNyA1Ljk1MDc1IDYuODMxNTYgNS4yODQxMSA3LjE0NDZDIDQuNjE3NDcgNy40NTc2MyAzLjg3MzcxIDcuNTY0MTQgMy4xNDcwMiA3LjQ1MDYzQyAyLjQyMDMyIDcuMzM3MTIgMS43NDMzNiA3LjAwODcgMS4yMDE4NCA2LjUwNjk1QyAwLjY2MDMyOCA2LjAwNTIgMC4yNzg2MSA1LjM1MjY4IDAuMTA1MDE3IDQuNjMyMDJDIC0wLjA2ODU3NTcgMy45MTEzNSAtMC4wMjYyMzYxIDMuMTU0OTQgMC4yMjY2NzUgMi40NTg1NkMgMC40Nzk1ODcgMS43NjIxNyAwLjkzMTY5NyAxLjE1NzEzIDEuNTI1NzYgMC43MjAwMzNDIDIuMTE5ODMgMC4yODI5MzUgMi44MjkxNCAwLjAzMzQzOTUgMy41NjM4OSAwLjAwMzEzMzQ0QyA0LjU0NjY3IC0wLjAzNzQwMzMgNS41MDUyOSAwLjMxNjcwNiA2LjIyOTYxIDAuOTg3ODM1QyA2Ljk1MzkzIDEuNjU4OTYgNy4zODQ4NCAyLjU5MjM1IDcuNDI3ODkgMy41ODMzOEwgNy40Mjc4OSAzLjU4MzM4WiIvPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjM4LjM2IDIyODYuMDYpIiBkPSJNIDIuMjc0NzEgNC4zOTYyOUMgMS44NDM2MyA0LjQxNTA4IDEuNDE2NzEgNC4zMDQ0NSAxLjA0Nzk5IDQuMDc4NDNDIDAuNjc5MjY4IDMuODUyNCAwLjM4NTMyOCAzLjUyMTE0IDAuMjAzMzcxIDMuMTI2NTZDIDAuMDIxNDEzNiAyLjczMTk4IC0wLjA0MDM3OTggMi4yOTE4MyAwLjAyNTgxMTYgMS44NjE4MUMgMC4wOTIwMDMxIDEuNDMxOCAwLjI4MzIwNCAxLjAzMTI2IDAuNTc1MjEzIDAuNzEwODgzQyAwLjg2NzIyMiAwLjM5MDUxIDEuMjQ2OTEgMC4xNjQ3MDggMS42NjYyMiAwLjA2MjA1OTJDIDIuMDg1NTMgLTAuMDQwNTg5NyAyLjUyNTYxIC0wLjAxNTQ3MTQgMi45MzA3NiAwLjEzNDIzNUMgMy4zMzU5MSAwLjI4Mzk0MSAzLjY4NzkyIDAuNTUxNTA1IDMuOTQyMjIgMC45MDMwNkMgNC4xOTY1MiAxLjI1NDYyIDQuMzQxNjkgMS42NzQzNiA0LjM1OTM1IDIuMTA5MTZDIDQuMzgyOTkgMi42OTEwNyA0LjE3Njc4IDMuMjU4NjkgMy43ODU5NyAzLjY4NzQ2QyAzLjM5NTE2IDQuMTE2MjQgMi44NTE2NiA0LjM3MTE2IDIuMjc0NzEgNC4zOTYyOUwgMi4yNzQ3MSA0LjM5NjI5WiIvPgogICAgPC9nPgogIDwvZz4+Cjwvc3ZnPgo=);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iLTEwIC0xMCAxMzEuMTYxMzYxNjk0MzM1OTQgMTMyLjM4ODk5OTkzODk2NDg0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzA2OTk4IiBkPSJNIDU0LjkxODc4NSw5LjE5Mjc0MjFlLTQgQyA1MC4zMzUxMzIsMC4wMjIyMTcyNyA0NS45NTc4NDYsMC40MTMxMzY5NyA0Mi4xMDYyODUsMS4wOTQ2NjkzIDMwLjc2MDA2OSwzLjA5OTE3MzEgMjguNzAwMDM2LDcuMjk0NzcxNCAyOC43MDAwMzUsMTUuMDMyMTY5IHYgMTAuMjE4NzUgaCAyNi44MTI1IHYgMy40MDYyNSBoIC0yNi44MTI1IC0xMC4wNjI1IGMgLTcuNzkyNDU5LDAgLTE0LjYxNTc1ODgsNC42ODM3MTcgLTE2Ljc0OTk5OTgsMTMuNTkzNzUgLTIuNDYxODE5OTgsMTAuMjEyOTY2IC0yLjU3MTAxNTA4LDE2LjU4NjAyMyAwLDI3LjI1IDEuOTA1OTI4Myw3LjkzNzg1MiA2LjQ1NzU0MzIsMTMuNTkzNzQ4IDE0LjI0OTk5OTgsMTMuNTkzNzUgaCA5LjIxODc1IHYgLTEyLjI1IGMgMCwtOC44NDk5MDIgNy42NTcxNDQsLTE2LjY1NjI0OCAxNi43NSwtMTYuNjU2MjUgaCAyNi43ODEyNSBjIDcuNDU0OTUxLDAgMTMuNDA2MjUzLC02LjEzODE2NCAxMy40MDYyNSwtMTMuNjI1IHYgLTI1LjUzMTI1IGMgMCwtNy4yNjYzMzg2IC02LjEyOTk4LC0xMi43MjQ3NzcxIC0xMy40MDYyNSwtMTMuOTM3NDk5NyBDIDY0LjI4MTU0OCwwLjMyNzk0Mzk3IDU5LjUwMjQzOCwtMC4wMjAzNzkwMyA1NC45MTg3ODUsOS4xOTI3NDIxZS00IFogbSAtMTQuNSw4LjIxODc1MDEyNTc5IGMgMi43Njk1NDcsMCA1LjAzMTI1LDIuMjk4NjQ1NiA1LjAzMTI1LDUuMTI0OTk5NiAtMmUtNiwyLjgxNjMzNiAtMi4yNjE3MDMsNS4wOTM3NSAtNS4wMzEyNSw1LjA5Mzc1IC0yLjc3OTQ3NiwtMWUtNiAtNS4wMzEyNSwtMi4yNzc0MTUgLTUuMDMxMjUsLTUuMDkzNzUgLTEwZS03LC0yLjgyNjM1MyAyLjI1MTc3NCwtNS4xMjQ5OTk2IDUuMDMxMjUsLTUuMTI0OTk5NiB6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2ZmZDQzYiIgZD0ibSA4NS42Mzc1MzUsMjguNjU3MTY5IHYgMTEuOTA2MjUgYyAwLDkuMjMwNzU1IC03LjgyNTg5NSwxNi45OTk5OTkgLTE2Ljc1LDE3IGggLTI2Ljc4MTI1IGMgLTcuMzM1ODMzLDAgLTEzLjQwNjI0OSw2LjI3ODQ4MyAtMTMuNDA2MjUsMTMuNjI1IHYgMjUuNTMxMjQ3IGMgMCw3LjI2NjM0NCA2LjMxODU4OCwxMS41NDAzMjQgMTMuNDA2MjUsMTMuNjI1MDA0IDguNDg3MzMxLDIuNDk1NjEgMTYuNjI2MjM3LDIuOTQ2NjMgMjYuNzgxMjUsMCA2Ljc1MDE1NSwtMS45NTQzOSAxMy40MDYyNTMsLTUuODg3NjEgMTMuNDA2MjUsLTEzLjYyNTAwNCBWIDg2LjUwMDkxOSBoIC0yNi43ODEyNSB2IC0zLjQwNjI1IGggMjYuNzgxMjUgMTMuNDA2MjU0IGMgNy43OTI0NjEsMCAxMC42OTYyNTEsLTUuNDM1NDA4IDEzLjQwNjI0MSwtMTMuNTkzNzUgMi43OTkzMywtOC4zOTg4ODYgMi42ODAyMiwtMTYuNDc1Nzc2IDAsLTI3LjI1IC0xLjkyNTc4LC03Ljc1NzQ0MSAtNS42MDM4NywtMTMuNTkzNzUgLTEzLjQwNjI0MSwtMTMuNTkzNzUgeiBtIC0xNS4wNjI1LDY0LjY1NjI1IGMgMi43Nzk0NzgsM2UtNiA1LjAzMTI1LDIuMjc3NDE3IDUuMDMxMjUsNS4wOTM3NDcgLTJlLTYsMi44MjYzNTQgLTIuMjUxNzc1LDUuMTI1MDA0IC01LjAzMTI1LDUuMTI1MDA0IC0yLjc2OTU1LDAgLTUuMDMxMjUsLTIuMjk4NjUgLTUuMDMxMjUsLTUuMTI1MDA0IDJlLTYsLTIuODE2MzMgMi4yNjE2OTcsLTUuMDkzNzQ3IDUuMDMxMjUsLTUuMDkzNzQ3IHoiLz4KPC9zdmc+Cg==);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMTUwIDE1MCA1NDEuOSAyOTUuMyI+CiAgPGcgY2xhc3M9ImpwLWljb24tYnJhbmQyIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxREFGQiI+CiAgICA8cGF0aCBkPSJNNjY2LjMgMjk2LjVjMC0zMi41LTQwLjctNjMuMy0xMDMuMS04Mi40IDE0LjQtNjMuNiA4LTExNC4yLTIwLjItMTMwLjQtNi41LTMuOC0xNC4xLTUuNi0yMi40LTUuNnYyMi4zYzQuNiAwIDguMy45IDExLjQgMi42IDEzLjYgNy44IDE5LjUgMzcuNSAxNC45IDc1LjctMS4xIDkuNC0yLjkgMTkuMy01LjEgMjkuNC0xOS42LTQuOC00MS04LjUtNjMuNS0xMC45LTEzLjUtMTguNS0yNy41LTM1LjMtNDEuNi01MCAzMi42LTMwLjMgNjMuMi00Ni45IDg0LTQ2LjlWNzhjLTI3LjUgMC02My41IDE5LjYtOTkuOSA1My42LTM2LjQtMzMuOC03Mi40LTUzLjItOTkuOS01My4ydjIyLjNjMjAuNyAwIDUxLjQgMTYuNSA4NCA0Ni42LTE0IDE0LjctMjggMzEuNC00MS4zIDQ5LjktMjIuNiAyLjQtNDQgNi4xLTYzLjYgMTEtMi4zLTEwLTQtMTkuNy01LjItMjktNC43LTM4LjIgMS4xLTY3LjkgMTQuNi03NS44IDMtMS44IDYuOS0yLjYgMTEuNS0yLjZWNzguNWMtOC40IDAtMTYgMS44LTIyLjYgNS42LTI4LjEgMTYuMi0zNC40IDY2LjctMTkuOSAxMzAuMS02Mi4yIDE5LjItMTAyLjcgNDkuOS0xMDIuNyA4Mi4zIDAgMzIuNSA0MC43IDYzLjMgMTAzLjEgODIuNC0xNC40IDYzLjYtOCAxMTQuMiAyMC4yIDEzMC40IDYuNSAzLjggMTQuMSA1LjYgMjIuNSA1LjYgMjcuNSAwIDYzLjUtMTkuNiA5OS45LTUzLjYgMzYuNCAzMy44IDcyLjQgNTMuMiA5OS45IDUzLjIgOC40IDAgMTYtMS44IDIyLjYtNS42IDI4LjEtMTYuMiAzNC40LTY2LjcgMTkuOS0xMzAuMSA2Mi0xOS4xIDEwMi41LTQ5LjkgMTAyLjUtODIuM3ptLTEzMC4yLTY2LjdjLTMuNyAxMi45LTguMyAyNi4yLTEzLjUgMzkuNS00LjEtOC04LjQtMTYtMTMuMS0yNC00LjYtOC05LjUtMTUuOC0xNC40LTIzLjQgMTQuMiAyLjEgMjcuOSA0LjcgNDEgNy45em0tNDUuOCAxMDYuNWMtNy44IDEzLjUtMTUuOCAyNi4zLTI0LjEgMzguMi0xNC45IDEuMy0zMCAyLTQ1LjIgMi0xNS4xIDAtMzAuMi0uNy00NS0xLjktOC4zLTExLjktMTYuNC0yNC42LTI0LjItMzgtNy42LTEzLjEtMTQuNS0yNi40LTIwLjgtMzkuOCA2LjItMTMuNCAxMy4yLTI2LjggMjAuNy0zOS45IDcuOC0xMy41IDE1LjgtMjYuMyAyNC4xLTM4LjIgMTQuOS0xLjMgMzAtMiA0NS4yLTIgMTUuMSAwIDMwLjIuNyA0NSAxLjkgOC4zIDExLjkgMTYuNCAyNC42IDI0LjIgMzggNy42IDEzLjEgMTQuNSAyNi40IDIwLjggMzkuOC02LjMgMTMuNC0xMy4yIDI2LjgtMjAuNyAzOS45em0zMi4zLTEzYzUuNCAxMy40IDEwIDI2LjggMTMuOCAzOS44LTEzLjEgMy4yLTI2LjkgNS45LTQxLjIgOCA0LjktNy43IDkuOC0xNS42IDE0LjQtMjMuNyA0LjYtOCA4LjktMTYuMSAxMy0yNC4xek00MjEuMiA0MzBjLTkuMy05LjYtMTguNi0yMC4zLTI3LjgtMzIgOSAuNCAxOC4yLjcgMjcuNS43IDkuNCAwIDE4LjctLjIgMjcuOC0uNy05IDExLjctMTguMyAyMi40LTI3LjUgMzJ6bS03NC40LTU4LjljLTE0LjItMi4xLTI3LjktNC43LTQxLTcuOSAzLjctMTIuOSA4LjMtMjYuMiAxMy41LTM5LjUgNC4xIDggOC40IDE2IDEzLjEgMjQgNC43IDggOS41IDE1LjggMTQuNCAyMy40ek00MjAuNyAxNjNjOS4zIDkuNiAxOC42IDIwLjMgMjcuOCAzMi05LS40LTE4LjItLjctMjcuNS0uNy05LjQgMC0xOC43LjItMjcuOC43IDktMTEuNyAxOC4zLTIyLjQgMjcuNS0zMnptLTc0IDU4LjljLTQuOSA3LjctOS44IDE1LjYtMTQuNCAyMy43LTQuNiA4LTguOSAxNi0xMyAyNC01LjQtMTMuNC0xMC0yNi44LTEzLjgtMzkuOCAxMy4xLTMuMSAyNi45LTUuOCA0MS4yLTcuOXptLTkwLjUgMTI1LjJjLTM1LjQtMTUuMS01OC4zLTM0LjktNTguMy01MC42IDAtMTUuNyAyMi45LTM1LjYgNTguMy01MC42IDguNi0zLjcgMTgtNyAyNy43LTEwLjEgNS43IDE5LjYgMTMuMiA0MCAyMi41IDYwLjktOS4yIDIwLjgtMTYuNiA0MS4xLTIyLjIgNjAuNi05LjktMy4xLTE5LjMtNi41LTI4LTEwLjJ6TTMxMCA0OTBjLTEzLjYtNy44LTE5LjUtMzcuNS0xNC45LTc1LjcgMS4xLTkuNCAyLjktMTkuMyA1LjEtMjkuNCAxOS42IDQuOCA0MSA4LjUgNjMuNSAxMC45IDEzLjUgMTguNSAyNy41IDM1LjMgNDEuNiA1MC0zMi42IDMwLjMtNjMuMiA0Ni45LTg0IDQ2LjktNC41LS4xLTguMy0xLTExLjMtMi43em0yMzcuMi03Ni4yYzQuNyAzOC4yLTEuMSA2Ny45LTE0LjYgNzUuOC0zIDEuOC02LjkgMi42LTExLjUgMi42LTIwLjcgMC01MS40LTE2LjUtODQtNDYuNiAxNC0xNC43IDI4LTMxLjQgNDEuMy00OS45IDIyLjYtMi40IDQ0LTYuMSA2My42LTExIDIuMyAxMC4xIDQuMSAxOS44IDUuMiAyOS4xem0zOC41LTY2LjdjLTguNiAzLjctMTggNy0yNy43IDEwLjEtNS43LTE5LjYtMTMuMi00MC0yMi41LTYwLjkgOS4yLTIwLjggMTYuNi00MS4xIDIyLjItNjAuNiA5LjkgMy4xIDE5LjMgNi41IDI4LjEgMTAuMiAzNS40IDE1LjEgNTguMyAzNC45IDU4LjMgNTAuNi0uMSAxNS43LTIzIDM1LjYtNTguNCA1MC42ek0zMjAuOCA3OC40eiIvPgogICAgPGNpcmNsZSBjeD0iNDIwLjkiIGN5PSIyOTYuNSIgcj0iNDUuNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KIDxnIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzQxNDE0MSI+CiAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiA8L2c+CiA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSguNDMgLjA0MDEpIiBmaWxsPSIjZmZmIj4KICA8cGF0aCBkPSJtNC4xNCA4Ljc2cTAuMDY4Mi0xLjg5IDIuNDItMS44OSAxLjE2IDAgMS42OCAwLjQyIDAuNTY3IDAuNDEgMC41NjcgMS4xNnYzLjQ3cTAgMC40NjIgMC41MTQgMC40NjIgMC4xMDMgMCAwLjItMC4wMjMxdjAuNzE0cS0wLjM5OSAwLjEwMy0wLjY1MSAwLjEwMy0wLjQ1MiAwLTAuNjkzLTAuMjItMC4yMzEtMC4yLTAuMjg0LTAuNjYyLTAuOTU2IDAuODcyLTIgMC44NzItMC45MDMgMC0xLjQ3LTAuNDcyLTAuNTI1LTAuNDcyLTAuNTI1LTEuMjYgMC0wLjI2MiAwLjA0NTItMC40NzIgMC4wNTY3LTAuMjIgMC4xMTYtMC4zNzggMC4wNjgyLTAuMTY4IDAuMjMxLTAuMzA0IDAuMTU4LTAuMTQ3IDAuMjYyLTAuMjQyIDAuMTE2LTAuMDkxNCAwLjM2OC0wLjE2OCAwLjI2Mi0wLjA5MTQgMC4zOTktMC4xMjYgMC4xMzYtMC4wNDUyIDAuNDcyLTAuMTAzIDAuMzM2LTAuMDU3OCAwLjUwNC0wLjA3OTggMC4xNTgtMC4wMjMxIDAuNTY3LTAuMDc5OCAwLjU1Ni0wLjA2ODIgMC43NzctMC4yMjEgMC4yMi0wLjE1MiAwLjIyLTAuNDQxdi0wLjI1MnEwLTAuNDMtMC4zNTctMC42NjItMC4zMzYtMC4yMzEtMC45NzYtMC4yMzEtMC42NjIgMC0wLjk5OCAwLjI2Mi0wLjMzNiAwLjI1Mi0wLjM5OSAwLjc5OHptMS44OSAzLjY4cTAuNzg4IDAgMS4yNi0wLjQxIDAuNTA0LTAuNDIgMC41MDQtMC45MDN2LTEuMDVxLTAuMjg0IDAuMTM2LTAuODYxIDAuMjMxLTAuNTY3IDAuMDkxNC0wLjk4NyAwLjE1OC0wLjQyIDAuMDY4Mi0wLjc2NiAwLjMyNi0wLjMzNiAwLjI1Mi0wLjMzNiAwLjcwNHQwLjMwNCAwLjcwNCAwLjg2MSAwLjI1MnoiIHN0cm9rZS13aWR0aD0iMS4wNSIvPgogIDxwYXRoIGQ9Im0xMCA0LjU2aDAuOTQ1djMuMTVxMC42NTEtMC45NzYgMS44OS0wLjk3NiAxLjE2IDAgMS44OSAwLjg0IDAuNjgyIDAuODQgMC42ODIgMi4zMSAwIDEuNDctMC43MDQgMi40Mi0wLjcwNCAwLjg4Mi0xLjg5IDAuODgyLTEuMjYgMC0xLjg5LTEuMDJ2MC43NjZoLTAuODV6bTIuNjIgMy4wNHEtMC43NDYgMC0xLjE2IDAuNjQtMC40NTIgMC42My0wLjQ1MiAxLjY4IDAgMS4wNSAwLjQ1MiAxLjY4dDEuMTYgMC42M3EwLjc3NyAwIDEuMjYtMC42MyAwLjQ5NC0wLjY0IDAuNDk0LTEuNjggMC0xLjA1LTAuNDcyLTEuNjgtMC40NjItMC42NC0xLjI2LTAuNjR6IiBzdHJva2Utd2lkdGg9IjEuMDUiLz4KICA8cGF0aCBkPSJtMi43MyAxNS44IDEzLjYgMC4wMDgxYzAuMDA2OSAwIDAtMi42IDAtMi42IDAtMC4wMDc4LTEuMTUgMC0xLjE1IDAtMC4wMDY5IDAtMC4wMDgzIDEuNS0wLjAwODMgMS41LTJlLTMgLTAuMDAxNC0xMS4zLTAuMDAxNC0xMS4zLTAuMDAxNGwtMC4wMDU5Mi0xLjVjMC0wLjAwNzgtMS4xNyAwLjAwMTMtMS4xNyAwLjAwMTN6IiBzdHJva2Utd2lkdGg9Ii45NzUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                messageStyle: 'none',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Neural-Machine-Translation-with-Attention">Neural Machine Translation with Attention<a class="anchor-link" href="#Neural-Machine-Translation-with-Attention"></a></h1><p>Advanced Learning Fall 2024.<br/>
Last updated: 2025-01-12</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>For SUBMISSION:</p>
<p>Please upload the complete and executed <code>ipynb</code> to your git repository. Verify that all of your output can be viewed directly from github, and provide a link to that git file below.</p>
<pre><code>STUDENT ID: 318632312
</code></pre>
<pre><code>STUDENT GIT LINK: https://github.com/idanshabo/computational_learning.git
</code></pre>
<p>In Addition, don't forget to add your ID to the files, and upload to moodle the html version:</p>
<p><code>PS3_Attention_2024_ID_[318632312].html</code></p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>In this problem set we are going to jump into the depths of <code>seq2seq</code> and <code>attention</code> and build a couple of PyTorch translation mechanisms with some  twists.</p>
<ul>
<li>Part 1 consists of a somewhat unorthodox <code>seq2seq</code> model for simple arithmetics</li>
<li>Part 2 consists of an <code>seq2seq - attention</code> language translation model. We will use it for Hebrew and English.</li>
</ul>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<hr/>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>A <strong>seq2seq</strong> model (sequence-to-sequence model) is a type of neural network designed specifically to handle sequences of data. The model converts input sequences into other sequences of data. This makes them particularly useful for tasks involving language, where the input and output are naturally sequences of words.</p>
<p>Here's a breakdown of how <code>seq2seq</code> models work:</p>
<ul>
<li><p>The encoder takes the input sequence, like a sentence in English, and processes it to capture its meaning and context.</p>
</li>
<li><p>information is then passed to the decoder, which uses it to generate the output sequence, like a translation in French.</p>
</li>
<li><p>Attention mechanism (optional): Some <code>seq2seq</code> models also incorporate an attention mechanism. This allows the decoder to focus on specific parts of the input sequence that are most relevant to generating the next element in the output sequence.</p>
</li>
</ul>
<p><code>seq2seq</code> models are used in many natural language processing (NLP) tasks.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>imports: (feel free to add)</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># from __future__ import unicode_literals, print_function, division</span>
<span class="c1"># from io import open</span>
<span class="c1"># import unicodedata</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>

<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span>

<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span><span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">RepeatVector</span><span class="p">,</span> <span class="n">TimeDistributed</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.losses</span> <span class="kn">import</span> <span class="n">SparseCategoricalCrossentropy</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Bidirectional</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">RepeatVector</span><span class="p">,</span> <span class="n">TimeDistributed</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">'/content/drive'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Mounted at /content/drive
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Part-1:-Seq2Seq-Arithmetic-model">Part 1: Seq2Seq Arithmetic model<a class="anchor-link" href="#Part-1:-Seq2Seq-Arithmetic-model"></a></h2>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>Using RNN <code>seq2seq</code> model to "learn" simple arithmetics!</strong></p>
<blockquote>
<p>Given the string "54-7", the model should return a prediction: "47".<br/>
Given the string "10+20", the model should return a prediction: "30".</p>
</blockquote>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ul>
<li>Watch Lukas Biewald's short <a href="https://youtu.be/MqugtGD605k?si=rAH34ZTJyYDj-XJ1">video</a> explaining <code>seq2seq</code> models and his toy application (somewhat outdated).</li>
<li>You can find the code for his example <a href="https://github.com/lukas/ml-class/blob/master/videos/seq2seq/train.py">here</a>.</li>
</ul>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.1) Using Lukas' code, implement a <code>seq2seq</code> network that can learn how to solve <strong>addition AND substraction</strong> of two numbers of maximum length of 4, using the following steps (similar to the example):</p>
<ul>
<li>Generate data; X: queries (two numbers), and Y: answers</li>
<li>One-hot encode X and Y,</li>
<li>Build a <code>seq2seq</code> network (with LSTM, RepeatVector, and TimeDistributed layers)</li>
<li>Train the model.</li>
<li>While training, sample from the validation set at random so we can visualize the generated solutions against the true solutions.</li>
</ul>
<p>Notes:</p>
<ul>
<li>The code in the example is quite old and based on Keras. You might have to adapt some of the code to overcome methods/code that is not supported anymore. Hint: for the evaluation part, review the type and format of the "correct" output - this will help you fix the unsupported "model.predict_classes".</li>
<li>Please use the parameters in the code cell below to train the model.</li>
<li>Instead of using a <code>wandb.config</code> object, please use a simple dictionary instead.</li>
<li>You don't need to run the model for more than 50 iterations (epochs) to get a gist of what is happening and what the algorithm is doing.</li>
<li>Extra credit if you can implement the network in PyTorch (this is not difficult).</li>
<li>Extra credit if you are able to significantly improve the model.</li>
</ul>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.2).</p>
<p>a) Do you think this model performs well?  Why or why not?<br/>
b) What are its limitations?<br/>
c) What would you do to improve it?<br/>
d) Can you apply an attention mechanism to this model? Why or why not?</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.3).</p>
<p>Add attention to the model. Evaluate the performance against the <code>seq2seq</code> you trained above. Which one is performing better?</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.4)</p>
<p>Using any neural network architecture of your liking, build  a model with the aim to beat the best performing model in 1.1 or 1.3. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">config</span><span class="p">[</span><span class="s2">"training_size"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">40000</span>
<span class="n">config</span><span class="p">[</span><span class="s2">"digits"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">config</span><span class="p">[</span><span class="s2">"hidden_size"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">config</span><span class="p">[</span><span class="s2">"batch_size"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">config</span><span class="p">[</span><span class="s2">"iterations"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">chars</span> <span class="o">=</span> <span class="s1">'0123456789-+ '</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>SOLUTION:</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">config_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"training_size"</span><span class="p">:</span> <span class="mi">40000</span><span class="p">,</span>
    <span class="s2">"digits"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">"hidden_size"</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">"iterations"</span> <span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s2">"chars"</span> <span class="p">:</span> <span class="s1">'0123456789-+ '</span>
<span class="p">}</span>

<span class="k">class</span> <span class="nc">CharacterTable</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Given a set of characters:</span>
<span class="sd">    + Encode them to a one hot integer representation</span>
<span class="sd">    + Decode the one hot integer representation to their character output</span>
<span class="sd">    + Decode a vector of probabilities to their character output</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chars</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Initialize character table.</span>
<span class="sd">        # Arguments</span>
<span class="sd">            chars: Characters that can appear in the input.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">char_indices</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices_char</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">num_rows</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""One hot encode given string C.</span>
<span class="sd">        # Arguments</span>
<span class="sd">            num_rows: Number of rows in the returned one hot encoding. This is</span>
<span class="sd">                used to keep the # of rows for each data the same.</span>
<span class="sd">        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_rows</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
            <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">char_indices</span><span class="p">[</span><span class="n">c</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">calc_argmax</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">calc_argmax</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices_char</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span>


<span class="n">maxlen</span> <span class="o">=</span> <span class="n">config_dict</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">config_dict</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span>

<span class="n">chars</span> <span class="o">=</span> <span class="n">config_dict</span><span class="p">[</span><span class="s2">"chars"</span><span class="p">]</span>
<span class="n">ctable</span> <span class="o">=</span> <span class="n">CharacterTable</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="n">questions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">expected</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Generating data...'</span><span class="p">)</span>

<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">config_dict</span><span class="p">[</span><span class="s1">'training_size'</span><span class="p">]:</span>
    <span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="s1">'0123456789'</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config_dict</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))))</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">f</span><span class="p">(),</span> <span class="n">f</span><span class="p">()</span>
    <span class="n">operation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s1">'+'</span><span class="p">,</span> <span class="s1">'-'</span><span class="p">])</span>

    <span class="c1"># Skip questions we've seen</span>
    <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">operation</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">seen</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

    <span class="c1"># Create the question string</span>
    <span class="n">q</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">a</span><span class="si">}{</span><span class="n">operation</span><span class="si">}{</span><span class="n">b</span><span class="si">}</span><span class="s2">"</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">q</span> <span class="o">+</span> <span class="s1">' '</span> <span class="o">*</span> <span class="p">(</span><span class="n">maxlen</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
    <span class="c1"># Compute answer</span>
    <span class="n">ans</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="k">if</span> <span class="n">operation</span> <span class="o">==</span> <span class="s1">'+'</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">ans</span> <span class="o">+=</span> <span class="s1">' '</span> <span class="o">*</span> <span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">ans</span><span class="p">))</span>

    <span class="n">questions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">expected</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ans</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Total addition questions:'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Vectorization...'</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">),</span> <span class="n">maxlen</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'chars'</span><span class="p">])),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">),</span> <span class="n">config_dict</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'chars'</span><span class="p">])),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">questions</span><span class="p">):</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">expected</span><span class="p">):</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">config_dict</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="n">split_at</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">//</span> <span class="mi">10</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_val</span><span class="p">)</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">split_at</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">split_at</span><span class="p">:]</span>
<span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">split_at</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">split_at</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Generating data...
Total addition questions: 40000
Vectorization...
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[50]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Building a seq2seq network</span>
<span class="c1"># Defining the model</span>
<span class="n">model1</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'hidden_size'</span><span class="p">]</span> <span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">))))</span>
<span class="n">model1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">RepeatVector</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">model1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'hidden_size'</span><span class="p">],</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">model1</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)))</span>
<span class="n">model1</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">model1</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c1"># data:</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s1">'best_model.h5'</span><span class="p">,</span>
                             <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span>
                             <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span>
                             <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'iterations'</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'-'</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Iteration'</span><span class="p">,</span> <span class="n">iteration</span><span class="p">)</span>

    <span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
              <span class="n">batch_size</span><span class="o">=</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'batch_size'</span><span class="p">],</span>
              <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
              <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">])</span>
    <span class="c1"># Select 10 samples from the validation set at random</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_val</span><span class="p">))</span>
        <span class="n">rowx</span><span class="p">,</span> <span class="n">rowy</span> <span class="o">=</span> <span class="n">x_val</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span class="p">])],</span> <span class="n">y_val</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span class="p">])]</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">rowx</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowy</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">guess</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">calc_argmax</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'Q'</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'T'</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">correct</span> <span class="o">==</span> <span class="n">guess</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)                         </span><span style="font-weight: bold"> Output Shape                </span><span style="font-weight: bold">         Param # </span>

 lstm (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                           <span style="color: #00af00; text-decoration-color: #00af00">72,704</span> 

 repeat_vector (<span style="color: #0087ff; text-decoration-color: #0087ff">RepeatVector</span>)          (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                             <span style="color: #00af00; text-decoration-color: #00af00">0</span> 

 lstm_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                         (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">131,584</span> 

 time_distributed (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)    (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">1,677</span> 

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">205,965</span> (804.55 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">205,965</span> (804.55 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
--------------------------------------------------
Iteration 0
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 61ms/step - accuracy: 0.3131 - loss: 2.0255
Epoch 1: val_loss improved from inf to 1.69362, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">24s</span> 65ms/step - accuracy: 0.3134 - loss: 2.0242 - val_accuracy: 0.3862 - val_loss: 1.6936
Q 942-53    T 889    166  
Q 56+53     T 109    16   
Q 16+2088   T 2104   116  
Q 23-1613   T -1590  -333 
Q 7599+1818 T 9417   1166 
Q 6839-70   T 6769   116  
Q 76-310    T -234   -36  
Q 23-225    T -202   -33  
Q 7+8754    T 8761   166  
Q 6+160     T 166    16   

--------------------------------------------------
Iteration 1
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 81ms/step - accuracy: 0.3955 - loss: 1.6507
Epoch 1: val_loss improved from 1.69362 to 1.61064, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">24s</span> 86ms/step - accuracy: 0.3955 - loss: 1.6506 - val_accuracy: 0.4126 - val_loss: 1.6106
Q 995+530   T 1525   1009 
Q 9-37      T -28    -33  
Q 82+9346   T 9428   3309 
Q 3+150     T 153    120  
Q 73-238    T -165   -330 
Q 8-8219    T -8211  -7779
Q 623+6     T 629    332  
Q 62-7      T 55     42   
Q 31-1681   T -1650  -1110
Q 8835+85   T 8920   1009 

--------------------------------------------------
Iteration 2
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 65ms/step - accuracy: 0.4206 - loss: 1.5820
Epoch 1: val_loss improved from 1.61064 to 1.54532, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">20s</span> 70ms/step - accuracy: 0.4206 - loss: 1.5820 - val_accuracy: 0.4313 - val_loss: 1.5453
Q 265+648   T 913    166  
Q 98-20     T 78     11   
Q 87+63     T 150    86   
Q 513-21    T 492    223  
Q 8+8888    T 8896   8882 
Q 176-5561  T -5385  -6666
Q 419-482   T -63    -11  
Q 38+8      T 46     11   
Q 7296+9    T 7305   1922 
Q 6966-74   T 6892   6666 

--------------------------------------------------
Iteration 3
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.4330 - loss: 1.5402
Epoch 1: val_loss improved from 1.54532 to 1.49569, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 62ms/step - accuracy: 0.4330 - loss: 1.5401 - val_accuracy: 0.4500 - val_loss: 1.4957
Q 23-225    T -202   -221 
Q 0+987     T 987    987  
Q 7789-8166 T -377   8755 
Q 3+210     T 213    222  
Q 2+119     T 121    111  
Q 5138+5    T 5143   455  
Q 121-6678  T -6557  -1111
Q 8+432     T 440    445  
Q 82+5199   T 5281   105  
Q 50+822    T 872    555  

--------------------------------------------------
Iteration 4
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 62ms/step - accuracy: 0.4466 - loss: 1.4899
Epoch 1: val_loss improved from 1.49569 to 1.49155, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">19s</span> 67ms/step - accuracy: 0.4467 - loss: 1.4898 - val_accuracy: 0.4624 - val_loss: 1.4916
Q 7205-5418 T 1787   6121 
Q 700-7074  T -6374  -711 
Q 4506-900  T 3606   311  
Q 48+59     T 107    84   
Q 976-7984  T -7008  -7011
Q 78-50     T 28     61   
Q 7553+336  T 7889   6511 
Q 968+111   T 1079   101  
Q 9608-9    T 9599   998  
Q 9-2119    T -2110  -111 

--------------------------------------------------
Iteration 5
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 58ms/step - accuracy: 0.4676 - loss: 1.4350
Epoch 1: val_loss improved from 1.49155 to 1.39643, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 60ms/step - accuracy: 0.4677 - loss: 1.4349 - val_accuracy: 0.4822 - val_loss: 1.3964
Q 449+1     T 450    447  
Q 76-3532   T -3456  -6228
Q 5023+8    T 5031   5668 
Q 210+4     T 214    21   
Q 2791+474  T 3265   4772 
Q 43+325    T 368    477  
Q 4-971     T -967   -980 
Q 7229+164  T 7393   7278 
Q 604+814   T 1418   7028 
Q 96-960    T -864   -905 

--------------------------------------------------
Iteration 6
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 58ms/step - accuracy: 0.4897 - loss: 1.3774
Epoch 1: val_loss improved from 1.39643 to 1.35818, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 61ms/step - accuracy: 0.4897 - loss: 1.3773 - val_accuracy: 0.4970 - val_loss: 1.3582
Q 9576-60   T 9516   5561 
Q 7+1623    T 1630   1171 
Q 45+24     T 69     55   
Q 2-2624    T -2622  -2226
Q 167-700   T -533   -166 
Q 582+13    T 595    577  
Q 391+478   T 869    417  
Q 5666-5    T 5661   5569 
Q 81-628    T -547   -611 
Q 4+111     T 115    111  

--------------------------------------------------
Iteration 7
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 65ms/step - accuracy: 0.5092 - loss: 1.3261
Epoch 1: val_loss improved from 1.35818 to 1.28006, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">19s</span> 68ms/step - accuracy: 0.5092 - loss: 1.3260 - val_accuracy: 0.5228 - val_loss: 1.2801
Q 5741-8955 T -3214  -4217
Q 5803+322  T 6125   5262 
Q 28+513    T 541    528  
Q 416+72    T 488    412  
Q 532-9     T 523    596  
Q 657-15    T 642    518  
Q 408-3234  T -2826  -3185
Q 2969+8464 T 11433  10225
Q 38+5290   T 5328   5666 
Q 206+6     T 212    265  

--------------------------------------------------
Iteration 8
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step - accuracy: 0.5316 - loss: 1.2645
Epoch 1: val_loss improved from 1.28006 to 1.23402, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 61ms/step - accuracy: 0.5316 - loss: 1.2645 - val_accuracy: 0.5394 - val_loss: 1.2340
Q 6+890     T 896    988  
Q 9+36      T 45     49   
Q 72-55     T 17     52   
Q 14-4      T 10     4    
Q 7297-504  T 6793   7255 
Q 944-3     T 941    949  
Q 96+8      T 104    90   
Q 59-768    T -709   -634 
Q 6-4530    T -4524  -4438
Q 721-79    T 642    735  

--------------------------------------------------
Iteration 9
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 58ms/step - accuracy: 0.5511 - loss: 1.2065
Epoch 1: val_loss improved from 1.23402 to 1.18801, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 60ms/step - accuracy: 0.5511 - loss: 1.2065 - val_accuracy: 0.5544 - val_loss: 1.1880
Q 57+7742   T 7799   7730 
Q 3984+4989 T 8973   1004 
Q 734+469   T 1203   114  
Q 9638-6492 T 3146   9301 
Q 5+31      T 36     35   
Q 40+1174   T 1214   1148 
Q 1382-6500 T -5118  -3550
Q 0+703     T 703    711  
Q 7+60      T 67     60   
Q 4358+4788 T 9146   4004 

--------------------------------------------------
Iteration 10
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 58ms/step - accuracy: 0.5664 - loss: 1.1620
Epoch 1: val_loss improved from 1.18801 to 1.14032, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 60ms/step - accuracy: 0.5665 - loss: 1.1619 - val_accuracy: 0.5731 - val_loss: 1.1403
Q 214-6     T 208    214  
Q 976+578   T 1554   1444 
Q 838-9634  T -8796  -8442
Q 5741-8955 T -3214  -4448
Q 74+7740   T 7814   7599 
Q 0-309     T -309   -308 
Q 4511-35   T 4476   4543 
Q 515-23    T 492    414  
Q 61+53     T 114    114  
Q 713+63    T 776    714  

--------------------------------------------------
Iteration 11
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step - accuracy: 0.5811 - loss: 1.1221
Epoch 1: val_loss improved from 1.14032 to 1.12298, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 61ms/step - accuracy: 0.5811 - loss: 1.1221 - val_accuracy: 0.5789 - val_loss: 1.1230
Q 610-88    T 522    566  
Q 7960+6068 T 14028  15660
Q 318-9926  T -9608  -9400
Q 5138+5    T 5143   5100 
Q 9439-146  T 9293   8465 
Q 34+8442   T 8476   8400 
Q 771-643   T 128    460  
Q 5-832     T -827   -830 
Q 459-293   T 166    210  
Q 8180-5    T 8175   8100 

--------------------------------------------------
Iteration 12
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 58ms/step - accuracy: 0.5910 - loss: 1.0930
Epoch 1: val_loss improved from 1.12298 to 1.09144, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 61ms/step - accuracy: 0.5910 - loss: 1.0929 - val_accuracy: 0.5868 - val_loss: 1.0914
Q 4048-8    T 4040   4004 
Q 71-7410   T -7339  -7497
Q 77-86     T -9     -1   
Q 112-1     T 111    118  
Q 342+3     T 345    333  
Q 573+2495  T 3068   3833 
Q 199-6     T 193    197  
Q 985+846   T 1831   1037 
Q 2411+0    T 2411   2413 
Q 987+3     T 990    983  

--------------------------------------------------
Iteration 13
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step - accuracy: 0.6041 - loss: 1.0600
Epoch 1: val_loss improved from 1.09144 to 1.07775, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 63ms/step - accuracy: 0.6041 - loss: 1.0600 - val_accuracy: 0.5929 - val_loss: 1.0778
Q 544-563   T -19    -10  
Q 69+48     T 117    121  
Q 0-44      T -44    -44  
Q 4400-8    T 4392   4303 
Q 9+452     T 461    457  
Q 3733-816  T 2917   240  
Q 4376-537  T 3839   370  
Q 3548+6    T 3554   3567 
Q 1+661     T 662    663  
Q 1191+704  T 1895   1721 

--------------------------------------------------
Iteration 14
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.6105 - loss: 1.0388
Epoch 1: val_loss improved from 1.07775 to 1.04432, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 62ms/step - accuracy: 0.6105 - loss: 1.0387 - val_accuracy: 0.6070 - val_loss: 1.0443
Q 63+8349   T 8412   8333 
Q 321-0     T 321    313  
Q 7735+8    T 7743   7738 
Q 499+4     T 503    497  
Q 9-815     T -806   -705 
Q 705-8445  T -7740  -7988
Q 470+978   T 1448   1448 
Q 8525-6391 T 2134   111  
Q 40-78     T -38    -45  
Q 135+2     T 137    137  

--------------------------------------------------
Iteration 15
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.6197 - loss: 1.0160
Epoch 1: val_loss improved from 1.04432 to 1.02038, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 63ms/step - accuracy: 0.6197 - loss: 1.0159 - val_accuracy: 0.6127 - val_loss: 1.0204
Q 75+908    T 983    900  
Q 3+907     T 910    913  
Q 5+1386    T 1391   1390 
Q 2+72      T 74     73   
Q 334+83    T 417    424  
Q 3931-4    T 3927   3934 
Q 30+747    T 777    786  
Q 308-17    T 291    296  
Q 7092-6918 T 174    111  
Q 49-556    T -507   -530 

--------------------------------------------------
Iteration 16
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 62ms/step - accuracy: 0.6302 - loss: 0.9924
Epoch 1: val_loss improved from 1.02038 to 0.99692, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">19s</span> 66ms/step - accuracy: 0.6302 - loss: 0.9924 - val_accuracy: 0.6195 - val_loss: 0.9969
Q 2+4139    T 4141   4141 
Q 74-4102   T -4028  -4953
Q 803+3796  T 4599   4166 
Q 3548+6    T 3554   3589 
Q 7+2291    T 2298   2291 
Q 6+2507    T 2513   2518 
Q 92-15     T 77     75   
Q 98-826    T -728   -731 
Q 5+189     T 194    296  
Q 4826-589  T 4237   4099 

--------------------------------------------------
Iteration 17
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.6373 - loss: 0.9674
Epoch 1: val_loss improved from 0.99692 to 0.98484, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 62ms/step - accuracy: 0.6373 - loss: 0.9674 - val_accuracy: 0.6272 - val_loss: 0.9848
Q 936+99    T 1035   1012 
Q 3+673     T 676    676  
Q 54+984    T 1038   1030 
Q 915+8     T 923    919  
Q 9067+8    T 9075   9160 
Q 340+551   T 891    895  
Q 518-6040  T -5522  -5960
Q 3791-74   T 3717   3755 
Q 997-2813  T -1816  -1555
Q 69+48     T 117    125  

--------------------------------------------------
Iteration 18
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.6448 - loss: 0.9487
Epoch 1: val_loss improved from 0.98484 to 0.96495, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 63ms/step - accuracy: 0.6448 - loss: 0.9487 - val_accuracy: 0.6328 - val_loss: 0.9649
Q 6124+90   T 6214   6248 
Q 271+2     T 273    273  
Q 381+7     T 388    380  
Q 353+12    T 365    359  
Q 663-986   T -323   -398 
Q 8317+92   T 8409   8393 
Q 771-643   T 128    191  
Q 97+5056   T 5153   5188 
Q 41-122    T -81    -94  
Q 236-7     T 229    239  

--------------------------------------------------
Iteration 19
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.6514 - loss: 0.9316
Epoch 1: val_loss improved from 0.96495 to 0.96316, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 65ms/step - accuracy: 0.6514 - loss: 0.9316 - val_accuracy: 0.6384 - val_loss: 0.9632
Q 4157+77   T 4234   4205 
Q 8+101     T 109    101  
Q 76-630    T -554   -577 
Q 9-2119    T -2110  -2109
Q 219-656   T -437   -430 
Q 694-50    T 644    611  
Q 624+16    T 640    633  
Q 340+551   T 891    808  
Q 87+1997   T 2084   2063 
Q 835-9     T 826    828  

--------------------------------------------------
Iteration 20
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.6585 - loss: 0.9168
Epoch 1: val_loss improved from 0.96316 to 0.93375, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 62ms/step - accuracy: 0.6585 - loss: 0.9168 - val_accuracy: 0.6472 - val_loss: 0.9338
Q 2289-914  T 1375   1470 
Q 768-552   T 216    202  
Q 7+1623    T 1630   1620 
Q 642+7995  T 8637   9099 
Q 8705-5956 T 2749   2295 
Q 75-2      T 73     75   
Q 55-229    T -174   -170 
Q 19+9      T 28     29   
Q 9-6277    T -6268  -6279
Q 6169-6718 T -549   -10  

--------------------------------------------------
Iteration 21
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.6659 - loss: 0.8979
Epoch 1: val_loss improved from 0.93375 to 0.92666, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 62ms/step - accuracy: 0.6659 - loss: 0.8979 - val_accuracy: 0.6490 - val_loss: 0.9267
Q 43-0      T 43     43   
Q 8666+702  T 9368   9596 
Q 864-1655  T -791   -116 
Q 2876-4    T 2872   2886 
Q 41-7      T 34     34   
Q 7-5066    T -5059  -5000
Q 30+3497   T 3527   3506 
Q 34+8442   T 8476   8452 
Q 40+1174   T 1214   1186 
Q 0+2920    T 2920   2924 

--------------------------------------------------
Iteration 22
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.6697 - loss: 0.8891
Epoch 1: val_loss improved from 0.92666 to 0.91655, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 62ms/step - accuracy: 0.6697 - loss: 0.8891 - val_accuracy: 0.6522 - val_loss: 0.9165
Q 94-70     T 24     12   
Q 219+308   T 527    565  
Q 9-93      T -84    -83  
Q 70-2      T 68     68   
Q 2289-914  T 1375   1470 
Q 3-865     T -862   -862 
Q 487-8     T 479    480  
Q 8511-33   T 8478   8455 
Q 825+1935  T 2760   2187 
Q 25-2      T 23     21   

--------------------------------------------------
Iteration 23
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.6744 - loss: 0.8728
Epoch 1: val_loss improved from 0.91655 to 0.89915, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 62ms/step - accuracy: 0.6744 - loss: 0.8728 - val_accuracy: 0.6574 - val_loss: 0.8992
Q 4389-329  T 4060   3161 
Q 64+775    T 839    839  
Q 306-818   T -512   -536 
Q 118-3     T 115    118  
Q 1+4787    T 4788   4785 
Q 1-488     T -487   -488 
Q 3634+496  T 4130   3021 
Q 1769-538  T 1231   1322 
Q 153+83    T 236    232  
Q 4-7035    T -7031  -7025

--------------------------------------------------
Iteration 24
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.6798 - loss: 0.8587
Epoch 1: val_loss improved from 0.89915 to 0.89306, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 65ms/step - accuracy: 0.6798 - loss: 0.8587 - val_accuracy: 0.6603 - val_loss: 0.8931
Q 28-200    T -172   -171 
Q 905-7     T 898    895  
Q 3537-333  T 3204   3171 
Q 2255-9    T 2246   2250 
Q 770-4     T 766    768  
Q 8628+730  T 9358   9412 
Q 82-9057   T -8975  -9027
Q 842-9062  T -8220  -8533
Q 6397-163  T 6234   6221 
Q 767-8     T 759    768  

--------------------------------------------------
Iteration 25
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 58ms/step - accuracy: 0.6872 - loss: 0.8427
Epoch 1: val_loss did not improve from 0.89306
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 60ms/step - accuracy: 0.6872 - loss: 0.8427 - val_accuracy: 0.6568 - val_loss: 0.9041
Q 0-3899    T -3899  -3896
Q 642-461   T 181    201  
Q 42-48     T -6     -1   
Q 270-0     T 270    270  
Q 195+6     T 201    201  
Q 4+9513    T 9517   9520 
Q 9-5982    T -5973  -5977
Q 5073-72   T 5001   5091 
Q 4471+333  T 4804   4751 
Q 93+6558   T 6651   6630 

--------------------------------------------------
Iteration 26
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.6901 - loss: 0.8320
Epoch 1: val_loss improved from 0.89306 to 0.87861, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 61ms/step - accuracy: 0.6901 - loss: 0.8320 - val_accuracy: 0.6670 - val_loss: 0.8786
Q 4526-68   T 4458   4370 
Q 8966-5710 T 3256   4495 
Q 5506-601  T 4905   4967 
Q 793-8     T 785    784  
Q 5-5064    T -5059  -5057
Q 349+6     T 355    353  
Q 9608-9    T 9599   9609 
Q 5403-335  T 5068   5969 
Q 752-13    T 739    736  
Q 868-3309  T -2441  -2633

--------------------------------------------------
Iteration 27
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.6938 - loss: 0.8227
Epoch 1: val_loss improved from 0.87861 to 0.87279, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 63ms/step - accuracy: 0.6938 - loss: 0.8227 - val_accuracy: 0.6653 - val_loss: 0.8728
Q 916-3     T 913    916  
Q 19-90     T -71    -73  
Q 4-3973    T -3969  -3962
Q 25-2      T 23     23   
Q 72-300    T -228   -224 
Q 703-110   T 593    590  
Q 147+2     T 149    158  
Q 93+95     T 188    186  
Q 6+4273    T 4279   4279 
Q 3467-776  T 2691   3609 

--------------------------------------------------
Iteration 28
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 57ms/step - accuracy: 0.6974 - loss: 0.8142
Epoch 1: val_loss improved from 0.87279 to 0.85467, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 60ms/step - accuracy: 0.6974 - loss: 0.8141 - val_accuracy: 0.6730 - val_loss: 0.8547
Q 425-7     T 418    416  
Q 10-82     T -72    -79  
Q 37+5      T 42     41   
Q 2+4139    T 4141   4142 
Q 666-39    T 627    628  
Q 918+4     T 922    929  
Q 68+800    T 868    878  
Q 5021-4431 T 590    166  
Q 470+978   T 1448   1446 
Q 450+2     T 452    452  

--------------------------------------------------
Iteration 29
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.7020 - loss: 0.8013
Epoch 1: val_loss improved from 0.85467 to 0.84585, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 62ms/step - accuracy: 0.7020 - loss: 0.8013 - val_accuracy: 0.6772 - val_loss: 0.8459
Q 237-15    T 222    233  
Q 7+966     T 973    971  
Q 749-5990  T -5241  -5790
Q 912-89    T 823    835  
Q 2573+925  T 3498   3437 
Q 344+506   T 850    893  
Q 3030+93   T 3123   3104 
Q 4+36      T 40     30   
Q 384-4     T 380    384  
Q 78+129    T 207    215  

--------------------------------------------------
Iteration 30
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.7091 - loss: 0.7855
Epoch 1: val_loss improved from 0.84585 to 0.84412, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 61ms/step - accuracy: 0.7090 - loss: 0.7855 - val_accuracy: 0.6773 - val_loss: 0.8441
Q 6016+62   T 6078   6078 
Q 89-7853   T -7764  -7759
Q 4245+1    T 4246   4244 
Q 6-5612    T -5606  -5608
Q 5+9       T 14     14   
Q 6+2185    T 2191   2182 
Q 1+6421    T 6422   6422 
Q 33+320    T 353    357  
Q 1432-7    T 1425   1329 
Q 3684+24   T 3708   3797 

--------------------------------------------------
Iteration 31
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step - accuracy: 0.7127 - loss: 0.7763
Epoch 1: val_loss improved from 0.84412 to 0.83880, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 61ms/step - accuracy: 0.7127 - loss: 0.7763 - val_accuracy: 0.6844 - val_loss: 0.8388
Q 7+83      T 90     91   
Q 926+154   T 1080   1088 
Q 47-42     T 5      1    
Q 6397-163  T 6234   6259 
Q 2-4409    T -4407  -4414
Q 7987-9215 T -1228  -1983
Q 3-15      T -12    -13  
Q 4979-993  T 3986   4991 
Q 523-7     T 516    515  
Q 47-101    T -54    -49  

--------------------------------------------------
Iteration 32
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step - accuracy: 0.7154 - loss: 0.7678
Epoch 1: val_loss improved from 0.83880 to 0.83701, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 64ms/step - accuracy: 0.7154 - loss: 0.7678 - val_accuracy: 0.6806 - val_loss: 0.8370
Q 59-1826   T -1767  -1785
Q 6575+759  T 7334   7339 
Q 5144-6    T 5138   5138 
Q 603+4     T 607    605  
Q 8982-89   T 8893   8885 
Q 3301+4    T 3305   3304 
Q 75+3953   T 4028   4022 
Q 107+99    T 206    297  
Q 88-5341   T -5253  -5235
Q 7456-4    T 7452   7465 

--------------------------------------------------
Iteration 33
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step - accuracy: 0.7177 - loss: 0.7641
Epoch 1: val_loss improved from 0.83701 to 0.82663, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">19s</span> 66ms/step - accuracy: 0.7177 - loss: 0.7641 - val_accuracy: 0.6848 - val_loss: 0.8266
Q 3368+5    T 3373   3371 
Q 8162-0    T 8162   8167 
Q 7173-689  T 6484   6632 
Q 57-6608   T -6551  -6580
Q 2490+17   T 2507   2507 
Q 3-728     T -725   -726 
Q 156-8534  T -8378  -8200
Q 1930+727  T 2657   2527 
Q 745+54    T 799    792  
Q 86+3199   T 3285   3272 

--------------------------------------------------
Iteration 34
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.7213 - loss: 0.7539
Epoch 1: val_loss improved from 0.82663 to 0.81968, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 62ms/step - accuracy: 0.7213 - loss: 0.7539 - val_accuracy: 0.6881 - val_loss: 0.8197
Q 8-834     T -826   -826 
Q 5553+968  T 6521   6538 
Q 709-522   T 187    12   
Q 7+9400    T 9407   9402 
Q 43+8019   T 8062   8055 
Q 921+0     T 921    921  
Q 14+8276   T 8290   8299 
Q 2321+0    T 2321   2331 
Q 9508+58   T 9566   9595 
Q 452+312   T 764    688  

--------------------------------------------------
Iteration 35
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.7263 - loss: 0.7405
Epoch 1: val_loss improved from 0.81968 to 0.81741, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 62ms/step - accuracy: 0.7263 - loss: 0.7405 - val_accuracy: 0.6857 - val_loss: 0.8174
Q 5952+32   T 5984   5909 
Q 262-8     T 254    257  
Q 6180-369  T 5811   5825 
Q 4+78      T 82     82   
Q 9136+90   T 9226   9305 
Q 23+8561   T 8584   8568 
Q 53+7071   T 7124   7139 
Q 3690-8    T 3682   3687 
Q 264-555   T -291   -100 
Q 47+745    T 792    792  

--------------------------------------------------
Iteration 36
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 62ms/step - accuracy: 0.7302 - loss: 0.7310
Epoch 1: val_loss improved from 0.81741 to 0.81502, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 64ms/step - accuracy: 0.7302 - loss: 0.7310 - val_accuracy: 0.6878 - val_loss: 0.8150
Q 2-22      T -20    -20  
Q 62-9634   T -9572  -9587
Q 10+4538   T 4548   4546 
Q 155+9     T 164    162  
Q 23+8561   T 8584   8560 
Q 64+775    T 839    839  
Q 8-9253    T -9245  -9250
Q 986-1     T 985    985  
Q 2049-6890 T -4841  -4555
Q 7483+9623 T 17106  17622

--------------------------------------------------
Iteration 37
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.7320 - loss: 0.7235
Epoch 1: val_loss improved from 0.81502 to 0.80229, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 61ms/step - accuracy: 0.7320 - loss: 0.7235 - val_accuracy: 0.6930 - val_loss: 0.8023
Q 8886-7    T 8879   8875 
Q 746-18    T 728    722  
Q 316-698   T -382   -350 
Q 0+7459    T 7459   7459 
Q 1+2511    T 2512   2511 
Q 6034+780  T 6814   6882 
Q 42-56     T -14    -10  
Q 4+436     T 440    449  
Q 394+1876  T 2270   2199 
Q 2812+91   T 2903   2902 

--------------------------------------------------
Iteration 38
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.7374 - loss: 0.7118
Epoch 1: val_loss improved from 0.80229 to 0.80212, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 62ms/step - accuracy: 0.7373 - loss: 0.7119 - val_accuracy: 0.6935 - val_loss: 0.8021
Q 167+79    T 246    244  
Q 485+3175  T 3660   3791 
Q 598-24    T 574    667  
Q 96-3      T 93     92   
Q 7+877     T 884    883  
Q 585-8     T 577    570  
Q 775-33    T 742    742  
Q 9-2455    T -2446  -2446
Q 91-7302   T -7211  -7233
Q 1114-10   T 1104   1108 

--------------------------------------------------
Iteration 39
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.7431 - loss: 0.7009
Epoch 1: val_loss improved from 0.80212 to 0.80029, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 62ms/step - accuracy: 0.7431 - loss: 0.7009 - val_accuracy: 0.6920 - val_loss: 0.8003
Q 379+49    T 428    424  
Q 384-89    T 295    298  
Q 6117+7170 T 13287  13888
Q 52-36     T 16     17   
Q 19-173    T -154   -145 
Q 5-487     T -482   -481 
Q 480+379   T 859    864  
Q 7292-288  T 7004   6925 
Q 26-1      T 25     25   
Q 62-7      T 55     54   

--------------------------------------------------
Iteration 40
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 63ms/step - accuracy: 0.7450 - loss: 0.6933
Epoch 1: val_loss improved from 0.80029 to 0.78719, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">19s</span> 68ms/step - accuracy: 0.7450 - loss: 0.6933 - val_accuracy: 0.6994 - val_loss: 0.7872
Q 253+6619  T 6872   6877 
Q 6486+3    T 6489   6497 
Q 67+463    T 530    524  
Q 7-1128    T -1121  -1129
Q 34+24     T 58     57   
Q 3-665     T -662   -661 
Q 6774+85   T 6859   7747 
Q 8398-9    T 8389   8397 
Q 3-2       T 1      1    
Q 500+559   T 1059   1046 

--------------------------------------------------
Iteration 41
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step - accuracy: 0.7485 - loss: 0.6827
Epoch 1: val_loss improved from 0.78719 to 0.78679, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 63ms/step - accuracy: 0.7485 - loss: 0.6827 - val_accuracy: 0.7016 - val_loss: 0.7868
Q 32+898    T 930    131  
Q 9846+8    T 9854   9843 
Q 804+732   T 1536   1596 
Q 947-81    T 866    855  
Q 0-3899    T -3899  -3899
Q 12-50     T -38    -38  
Q 55+9      T 64     64   
Q 1+8301    T 8302   8312 
Q 67+3      T 70     70   
Q 54-3      T 51     52   

--------------------------------------------------
Iteration 42
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.7509 - loss: 0.6779
Epoch 1: val_loss did not improve from 0.78679
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 62ms/step - accuracy: 0.7508 - loss: 0.6779 - val_accuracy: 0.6989 - val_loss: 0.7872
Q 2508-6    T 2502   2504 
Q 3523+1370 T 4893   5858 
Q 81+9043   T 9124   9189 
Q 7+7867    T 7874   7804 
Q 99+9      T 108    107  
Q 24-3102   T -3078  -3098
Q 2591-22   T 2569   2581 
Q 3+5920    T 5923   5928 
Q 178+11    T 189    185  
Q 118-1     T 117    117  

--------------------------------------------------
Iteration 43
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 64ms/step - accuracy: 0.7536 - loss: 0.6679
Epoch 1: val_loss improved from 0.78679 to 0.77405, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">20s</span> 69ms/step - accuracy: 0.7536 - loss: 0.6679 - val_accuracy: 0.7018 - val_loss: 0.7740
Q 61+6933   T 6994   6902 
Q 179-2     T 177    179  
Q 7110+73   T 7183   7185 
Q 5338-416  T 4922   4869 
Q 552+4     T 556    556  
Q 969-56    T 913    912  
Q 70-664    T -594   -584 
Q 12-710    T -698   -696 
Q 3075-2716 T 359    -60  
Q 352+73    T 425    407  

--------------------------------------------------
Iteration 44
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.7571 - loss: 0.6574
Epoch 1: val_loss improved from 0.77405 to 0.76790, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 63ms/step - accuracy: 0.7571 - loss: 0.6574 - val_accuracy: 0.7067 - val_loss: 0.7679
Q 118-3     T 115    115  
Q 2-733     T -731   -731 
Q 3434-844  T 2590   2585 
Q 801+9967  T 10768  10666
Q 517+7330  T 7847   7783 
Q 7214+9    T 7223   7221 
Q 47-74     T -27    -28  
Q 169-8058  T -7889  -7833
Q 8430+3762 T 12192  11442
Q 3786+4866 T 8652   7534 

--------------------------------------------------
Iteration 45
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 58ms/step - accuracy: 0.7623 - loss: 0.6492
Epoch 1: val_loss did not improve from 0.76790
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 60ms/step - accuracy: 0.7623 - loss: 0.6492 - val_accuracy: 0.7047 - val_loss: 0.7693
Q 5406+689  T 6095   6071 
Q 1909-0    T 1909   1909 
Q 909+74    T 983    994  
Q 2973+61   T 3034   3029 
Q 295+7567  T 7862   7833 
Q 6-55      T -49    -49  
Q 1622-22   T 1600   1503 
Q 5-22      T -17    -17  
Q 4260-7426 T -3166  -2844
Q 3482+791  T 4273   4178 

--------------------------------------------------
Iteration 46
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 61ms/step - accuracy: 0.7648 - loss: 0.6421
Epoch 1: val_loss did not improve from 0.76790
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">19s</span> 66ms/step - accuracy: 0.7648 - loss: 0.6421 - val_accuracy: 0.7063 - val_loss: 0.7696
Q 515-23    T 492    501  
Q 221-591   T -370   -260 
Q 465+6773  T 7238   7220 
Q 0+9796    T 9796   9775 
Q 462+8     T 470    469  
Q 1565-3450 T -1885  -1133
Q 4176-9    T 4167   4165 
Q 214-74    T 140    140  
Q 9754-3    T 9751   9758 
Q 890+8503  T 9393   9441 

--------------------------------------------------
Iteration 47
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step - accuracy: 0.7689 - loss: 0.6309
Epoch 1: val_loss did not improve from 0.76790
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">18s</span> 63ms/step - accuracy: 0.7689 - loss: 0.6309 - val_accuracy: 0.7066 - val_loss: 0.7688
Q 95-18     T 77     73   
Q 9585+8645 T 18230  17311
Q 964+6315  T 7279   7217 
Q 460+75    T 535    522  
Q 2250+473  T 2723   2889 
Q 8804-543  T 8261   8245 
Q 668-73    T 595    501  
Q 409-0     T 409    408  
Q 973+6027  T 7000   7010 
Q 90+73     T 163    15   

--------------------------------------------------
Iteration 48
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 59ms/step - accuracy: 0.7698 - loss: 0.6268
Epoch 1: val_loss improved from 0.76790 to 0.75735, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">17s</span> 62ms/step - accuracy: 0.7698 - loss: 0.6269 - val_accuracy: 0.7115 - val_loss: 0.7574
Q 7557-2378 T 5179   5281 
Q 260-3     T 257    256  
Q 61-4675   T -4614  -4616
Q 9866+9    T 9875   9873 
Q 6279-7    T 6272   6270 
Q 8968+6    T 8974   8963 
Q 2776+3    T 2779   2779 
Q 76+6539   T 6615   6630 
Q 265-1747  T -1482  -1399
Q 1107+6    T 1113   1106 

--------------------------------------------------
Iteration 49
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step - accuracy: 0.7729 - loss: 0.6170
Epoch 1: val_loss improved from 0.75735 to 0.74865, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">19s</span> 66ms/step - accuracy: 0.7729 - loss: 0.6170 - val_accuracy: 0.7150 - val_loss: 0.7486
Q 7886+0    T 7886   7877 
Q 724+2     T 726    726  
Q 6-166     T -160   -161 
Q 6298+5    T 6303   6335 
Q 9811-7296 T 2515   1457 
Q 16-6      T 10     1    
Q 8-8888    T -8880  -8882
Q 38+91     T 129    120  
Q 6525+8788 T 15313  15122
Q 9-8749    T -8740  -8743
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>a) The model seems to perform fairly well, achieving a training accuracy of around 80% and a validation accuracy of about 73%. These results indicate that the model can learn and generalize to some degree. However, its possible that the model is not entirely capturing the key relationships between inputs and outputs, which is reflected in some of the prediction errors. Interestingly, in the validation examples, the model tends to perform well in estimating the correct answers most of the time.</p>
<p>b)
A limitation of this model is its inability to effectively capture long-range dependencies or focus on the most relevant parts of the input sequence for each prediction. Without an attention mechanism, the model has difficulty prioritizing different parts of the input selectively, which is essential in tasks like this one, where the input sequence can vary greatly and require different areas of focus for each prediction. This lack of flexibility can impair performance, especially when handling complex or nuanced patterns. Furthermore, the basic encoder-decoder structure restricts the models capacity to manage more intricate dependencies, limiting its ability to generalize to more complex inputs.</p>
<p>c)  Improvements:
To enhance the model, I would recommend incorporating an attention mechanism. Attention would enable the model to focus on different parts of the input sequence, which is particularly beneficial for sequence-to-sequence tasks where key information might be spread across the entire sequence.</p>
<p>Additionally, hyperparameter tuning could improve performance by adjusting settings like the learning rate, increasing the number of layers, or adding more neurons per layer. Implementing regularization techniques, such as dropout, could help prevent overfitting and improve generalization.</p>
<p>Expanding the training data would also contribute to better generalization across a wider range of inputs. We could balance the dataset between operations to ensure that the model performs well with both subtraction and addition tasks.</p>
<p>d) Yes, applying an attention mechanism to this model would be highly beneficial for solving arithmetic equations. The current simple encoder-decoder structure struggles with long input sequences and varying complexity in equations, as it lacks the flexibility to focus on the most important parts of the input.</p>
<p>By incorporating an attention mechanism, the model would be able to selectively focus on relevant elements of the input sequence (like specific digits or operators) during prediction. This would eliminate the reliance on a fixed-length representation of the entire sequence, allowing the model to handle longer and more complex sequences more efficiently. As a result, the model would likely improve its ability to generalize to larger numbers and reduce errors in performing arithmetic operations.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Q1---PART-3">Q1 - PART 3<a class="anchor-link" href="#Q1---PART-3"></a></h1>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Defining model 2 - adding attention</span>
<span class="c1"># Defining input layer</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)))</span>

<span class="c1"># First LSTM layer - Output sequences to feed into attention</span>
<span class="n">lstm_1</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'hidden_size'</span><span class="p">],</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">()([</span><span class="n">lstm_1</span><span class="p">,</span> <span class="n">lstm_1</span><span class="p">])</span>

<span class="c1"># Flattenning the attention output</span>
<span class="n">attention_flattened</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">attention</span><span class="p">)</span>

<span class="c1"># Repeat Vector layer to ensure that the output has the same shape</span>
<span class="n">repeat_vector</span> <span class="o">=</span> <span class="n">RepeatVector</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'digits'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)(</span><span class="n">attention_flattened</span><span class="p">)</span>

<span class="c1"># Second LSTM layer - Output sequences</span>
<span class="n">lstm_2</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'hidden_size'</span><span class="p">],</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">repeat_vector</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">))(</span><span class="n">lstm_2</span><span class="p">)</span>

<span class="c1"># Create the model</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>

<span class="c1"># Compile the model</span>
<span class="n">model2</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>

<span class="n">model2</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c1"># Train the model</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s1">'best_model.h5'</span><span class="p">,</span>
                             <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span>
                             <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span>
                             <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'iterations'</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'-'</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Iteration'</span><span class="p">,</span> <span class="n">iteration</span><span class="p">)</span>

    <span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
              <span class="n">batch_size</span><span class="o">=</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'batch_size'</span><span class="p">],</span>
              <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
              <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">])</span>
    <span class="c1"># Select 10 samples from the validation set at random</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_val</span><span class="p">))</span>
        <span class="n">rowx</span><span class="p">,</span> <span class="n">rowy</span> <span class="o">=</span> <span class="n">x_val</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span class="p">])],</span> <span class="n">y_val</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span class="p">])]</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">rowx</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowy</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">guess</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">calc_argmax</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'Q'</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'T'</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">correct</span> <span class="o">==</span> <span class="n">guess</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)              </span><span style="font-weight: bold"> Output Shape           </span><span style="font-weight: bold">        Param # </span><span style="font-weight: bold"> Connected to           </span>

 input_layer (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)   (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">9</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>)                        <span style="color: #00af00; text-decoration-color: #00af00">0</span>  -                      

 lstm (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">9</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                  <span style="color: #00af00; text-decoration-color: #00af00">72,704</span>  input_layer[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]      

 attention (<span style="color: #0087ff; text-decoration-color: #0087ff">Attention</span>)      (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">9</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">0</span>  lstm[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>], lstm[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] 

 flatten (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)          (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1152</span>)                         <span style="color: #00af00; text-decoration-color: #00af00">0</span>  attention[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]        

 repeat_vector              (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">1152</span>)                      <span style="color: #00af00; text-decoration-color: #00af00">0</span>  flatten[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]          
 (<span style="color: #0087ff; text-decoration-color: #0087ff">RepeatVector</span>)                                                                            

 lstm_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)              (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                 <span style="color: #00af00; text-decoration-color: #00af00">655,872</span>  repeat_vector[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]    

 time_distributed           (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">1,677</span>  lstm_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           
 (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)                                                                         

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">730,253</span> (2.79 MB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">730,253</span> (2.79 MB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
--------------------------------------------------
Iteration 0
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.3333 - loss: 1.9581
Epoch 1: val_loss improved from inf to 1.59145, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">8s</span> 9ms/step - accuracy: 0.3336 - loss: 1.9568 - val_accuracy: 0.4308 - val_loss: 1.5915
Q 582-87    T 495    778  
Q 4+85      T 89     55   
Q 26-7743   T -7717  -7733
Q 9157-7    T 9150   177  
Q 93-50     T 43     -3   
Q 79-4      T 75     17   
Q 2+316     T 318    33   
Q 6445-9033 T -2588  -553 
Q 82+5      T 87     11   
Q 31-85     T -54    -1   

--------------------------------------------------
Iteration 1
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.4331 - loss: 1.5694
Epoch 1: val_loss improved from 1.59145 to 1.50072, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.4331 - loss: 1.5691 - val_accuracy: 0.4510 - val_loss: 1.5007
Q 203+59    T 262    222  
Q 544-2772  T -2228  -444 
Q 179+817   T 996    100  
Q 35-808    T -773   -882 
Q 858-1     T 857    88   
Q 76+84     T 160    762  
Q 6984+72   T 7056   1622 
Q 92+469    T 561    902  
Q 2-1493    T -1491  -2292
Q 37+4160   T 4197   130  

--------------------------------------------------
Iteration 2
<span class="ansi-bold">276/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.4526 - loss: 1.4877
Epoch 1: val_loss improved from 1.50072 to 1.45192, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.4527 - loss: 1.4873 - val_accuracy: 0.4668 - val_loss: 1.4519
Q 8587-25   T 8562   8512 
Q 524+8     T 532    55   
Q 3531+3806 T 7337   3166 
Q 749+1     T 750    777  
Q 6503-9    T 6494   6555 
Q 43-611    T -568   -13  
Q 6-459     T -453   -655 
Q 353+6440  T 6793   4188 
Q 807-632   T 175    -62  
Q 360-6803  T -6443  -6333

--------------------------------------------------
Iteration 3
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.4696 - loss: 1.4357
Epoch 1: val_loss improved from 1.45192 to 1.38377, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.4697 - loss: 1.4356 - val_accuracy: 0.4870 - val_loss: 1.3838
Q 3-275     T -272   -222 
Q 241+81    T 322    223  
Q 2248+846  T 3094   2103 
Q 900+43    T 943    901  
Q 363-87    T 276    333  
Q 60-1      T 59     66   
Q 0+2138    T 2138   2202 
Q 909+279   T 1188   108  
Q 954+67    T 1021   102  
Q 2-2455    T -2453  -2222

--------------------------------------------------
Iteration 4
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.4907 - loss: 1.3744
Epoch 1: val_loss improved from 1.38377 to 1.32171, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.4908 - loss: 1.3742 - val_accuracy: 0.5100 - val_loss: 1.3217
Q 221+794   T 1015   144  
Q 6627+3444 T 10071  13111
Q 5830+74   T 5904   5888 
Q 3+2999    T 3002   9999 
Q 58+7      T 65     82   
Q 71+13     T 84     14   
Q 76-93     T -17    -1   
Q 963-77    T 886    684  
Q 3709-2653 T 1056   -204 
Q 29+48     T 77     11   

--------------------------------------------------
Iteration 5
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.5151 - loss: 1.3043
Epoch 1: val_loss improved from 1.32171 to 1.26116, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.5151 - loss: 1.3043 - val_accuracy: 0.5315 - val_loss: 1.2612
Q 9-884     T -875   -871 
Q 6-756     T -750   -655 
Q 447+4     T 451    477  
Q 6310+81   T 6391   6318 
Q 17-2      T 15     16   
Q 510+4     T 514    515  
Q 3+5344    T 5347   4447 
Q 652+4456  T 5108   1109 
Q 2388+0    T 2388   3288 
Q 98-5      T 93     86   

--------------------------------------------------
Iteration 6
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.5401 - loss: 1.2389
Epoch 1: val_loss improved from 1.26116 to 1.20922, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.5401 - loss: 1.2389 - val_accuracy: 0.5517 - val_loss: 1.2092
Q 9960+601  T 10561  10555
Q 2832+7666 T 10498  1111 
Q 514+6452  T 6966   6009 
Q 4839-652  T 4187   4865 
Q 259+828   T 1087   110  
Q 738+77    T 815    750  
Q 98+315    T 413    320  
Q 470-447   T 23     300  
Q 277-1856  T -1579  -107 
Q 143+403   T 546    466  

--------------------------------------------------
Iteration 7
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.5668 - loss: 1.1784
Epoch 1: val_loss improved from 1.20922 to 1.15176, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.5668 - loss: 1.1783 - val_accuracy: 0.5749 - val_loss: 1.1518
Q 474-5     T 469    477  
Q 107+8     T 115    107  
Q 5+78      T 83     86   
Q 1312-135  T 1177   127  
Q 1-6390    T -6389  -6693
Q 9615-7    T 9608   9655 
Q 1915+8    T 1923   1995 
Q 7+624     T 631    673  
Q 3283+2330 T 5613   3656 
Q 730-25    T 705    763  

--------------------------------------------------
Iteration 8
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.5870 - loss: 1.1275
Epoch 1: val_loss improved from 1.15176 to 1.09454, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.5870 - loss: 1.1274 - val_accuracy: 0.5941 - val_loss: 1.0945
Q 775+3692  T 4467   3008 
Q 559-722   T -163   -266 
Q 65-38     T 27     28   
Q 72+6      T 78     73   
Q 9-84      T -75    -79  
Q 1-248     T -247   -247 
Q 202+152   T 354    277  
Q 8724-96   T 8628   7738 
Q 6781-3599 T 3182   5967 
Q 45+65     T 110    101  

--------------------------------------------------
Iteration 9
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.6047 - loss: 1.0786
Epoch 1: val_loss improved from 1.09454 to 1.06079, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6048 - loss: 1.0785 - val_accuracy: 0.6071 - val_loss: 1.0608
Q 49+5527   T 5576   5602 
Q 917+6894  T 7811   7677 
Q 662+760   T 1422   1322 
Q 22-97     T -75    -68  
Q 9699+3    T 9702   9602 
Q 1800+65   T 1865   1857 
Q 64-1      T 63     64   
Q 7+8747    T 8754   8788 
Q 54-532    T -478   -496 
Q 78-5631   T -5553  -5497

--------------------------------------------------
Iteration 10
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.6183 - loss: 1.0384
Epoch 1: val_loss improved from 1.06079 to 1.03824, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6183 - loss: 1.0384 - val_accuracy: 0.6140 - val_loss: 1.0382
Q 635-4209  T -3574  -2767
Q 622+27    T 649    699  
Q 6+620     T 626    627  
Q 345-7     T 338    330  
Q 649+8     T 657    690  
Q 1619+2    T 1621   1619 
Q 846-91    T 755    877  
Q 307-3150  T -2843  -2873
Q 1150-25   T 1125   1111 
Q 575+554   T 1129   1101 

--------------------------------------------------
Iteration 11
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.6292 - loss: 1.0088
Epoch 1: val_loss improved from 1.03824 to 1.01628, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.6292 - loss: 1.0088 - val_accuracy: 0.6214 - val_loss: 1.0163
Q 2488-1    T 2487   2484 
Q 9-220     T -211   -210 
Q 709-6     T 703    704  
Q 669-8356  T -7687  -8711
Q 5898-405  T 5493   5133 
Q 50+1      T 51     51   
Q 896-91    T 805    808  
Q 379+3169  T 3548   3804 
Q 3475-1    T 3474   3473 
Q 733-37    T 696    664  

--------------------------------------------------
Iteration 12
<span class="ansi-bold">275/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.6420 - loss: 0.9754
Epoch 1: val_loss improved from 1.01628 to 0.98318, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6420 - loss: 0.9753 - val_accuracy: 0.6370 - val_loss: 0.9832
Q 2010-2    T 2008   2001 
Q 215+491   T 706    602  
Q 645-93    T 552    582  
Q 9888+6    T 9894   9894 
Q 6503-9    T 6494   6507 
Q 20+44     T 64     68   
Q 23-2397   T -2374  -2375
Q 7274-209  T 7065   7186 
Q 6-36      T -30    -30  
Q 44-40     T 4      3    

--------------------------------------------------
Iteration 13
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.6530 - loss: 0.9452
Epoch 1: val_loss improved from 0.98318 to 0.95686, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6530 - loss: 0.9452 - val_accuracy: 0.6450 - val_loss: 0.9569
Q 5029-10   T 5019   5099 
Q 49+5527   T 5576   5566 
Q 809+7223  T 8032   8620 
Q 42-9168   T -9126  -9168
Q 5-935     T -930   -939 
Q 535-7103  T -6568  -6666
Q 7874-9185 T -1311  -1177
Q 16-781    T -765   -767 
Q 7+226     T 233    239  
Q 16-142    T -126   -139 

--------------------------------------------------
Iteration 14
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.6615 - loss: 0.9199
Epoch 1: val_loss improved from 0.95686 to 0.93927, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6616 - loss: 0.9198 - val_accuracy: 0.6502 - val_loss: 0.9393
Q 887-9     T 878    879  
Q 2+883     T 885    889  
Q 245-728   T -483   -532 
Q 381-8656  T -8275  -8484
Q 2706-180  T 2526   2899 
Q 9+5752    T 5761   5746 
Q 7+2132    T 2139   2139 
Q 494-671   T -177   -187 
Q 991-79    T 912    803  
Q 61+97     T 158    168  

--------------------------------------------------
Iteration 15
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 11ms/step - accuracy: 0.6712 - loss: 0.8956
Epoch 1: val_loss improved from 0.93927 to 0.91952, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 12ms/step - accuracy: 0.6712 - loss: 0.8955 - val_accuracy: 0.6571 - val_loss: 0.9195
Q 7+7741    T 7748   7747 
Q 5159-71   T 5088   5165 
Q 702-25    T 677    677  
Q 4544-82   T 4462   4377 
Q 6899-9    T 6890   6889 
Q 9862+49   T 9911   9905 
Q 3-275     T -272   -271 
Q 7+669     T 676    675  
Q 915+158   T 1073   1037 
Q 447-7     T 440    458  

--------------------------------------------------
Iteration 16
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.6806 - loss: 0.8687
Epoch 1: val_loss improved from 0.91952 to 0.90231, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6806 - loss: 0.8687 - val_accuracy: 0.6639 - val_loss: 0.9023
Q 3-938     T -935   -930 
Q 960-686   T 274    232  
Q 9-2743    T -2734  -2734
Q 1+577     T 578    578  
Q 68+440    T 508    416  
Q 66-5461   T -5395  -5494
Q 149-62    T 87     13   
Q 5943+13   T 5956   5972 
Q 882+3768  T 4650   4655 
Q 45+2930   T 2975   2929 

--------------------------------------------------
Iteration 17
<span class="ansi-bold">276/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.6863 - loss: 0.8517
Epoch 1: val_loss improved from 0.90231 to 0.88979, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6863 - loss: 0.8515 - val_accuracy: 0.6689 - val_loss: 0.8898
Q 922+7     T 929    929  
Q 3+580     T 583    582  
Q 4762-39   T 4723   4634 
Q 352+301   T 653    733  
Q 2722-964  T 1758   1227 
Q 610+6185  T 6795   7222 
Q 70-8881   T -8811  -8799
Q 33-23     T 10     1    
Q 1009+9    T 1018   1019 
Q 785-7     T 778    782  

--------------------------------------------------
Iteration 18
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.6958 - loss: 0.8289
Epoch 1: val_loss improved from 0.88979 to 0.86702, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.6958 - loss: 0.8289 - val_accuracy: 0.6764 - val_loss: 0.8670
Q 95-6      T 89     80   
Q 70-961    T -891   -877 
Q 1-37      T -36    -36  
Q 752-8     T 744    746  
Q 2036+3    T 2039   2008 
Q 9741+756  T 10497  10568
Q 44+211    T 255    266  
Q 59-8881   T -8822  -8700
Q 97-447    T -350   -363 
Q 898-1     T 897    897  

--------------------------------------------------
Iteration 19
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 11ms/step - accuracy: 0.7030 - loss: 0.8051
Epoch 1: val_loss improved from 0.86702 to 0.85484, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.7030 - loss: 0.8051 - val_accuracy: 0.6791 - val_loss: 0.8548
Q 338+2761  T 3099   3124 
Q 4889+353  T 5242   5133 
Q 24+8820   T 8844   8946 
Q 9136+4    T 9140   9147 
Q 948+683   T 1631   1519 
Q 86-4      T 82     81   
Q 7042+9710 T 16752  16000
Q 3-275     T -272   -273 
Q 3626-5669 T -2043  -1004
Q 6612-6    T 6606   6604 

--------------------------------------------------
Iteration 20
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7120 - loss: 0.7837
Epoch 1: val_loss improved from 0.85484 to 0.83872, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 9ms/step - accuracy: 0.7120 - loss: 0.7837 - val_accuracy: 0.6836 - val_loss: 0.8387
Q 9908-660  T 9248   9393 
Q 6567-548  T 6019   5023 
Q 423+8140  T 8563   8666 
Q 8612-74   T 8538   8655 
Q 4091-18   T 4073   4088 
Q 8+8639    T 8647   8644 
Q 7909+38   T 7947   7909 
Q 85+27     T 112    110  
Q 1493-909  T 584    829  
Q 3-7029    T -7026  -7044

--------------------------------------------------
Iteration 21
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.7186 - loss: 0.7670
Epoch 1: val_loss improved from 0.83872 to 0.83364, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7186 - loss: 0.7670 - val_accuracy: 0.6853 - val_loss: 0.8336
Q 233-0     T 233    223  
Q 5472+2    T 5474   5575 
Q 599-439   T 160    445  
Q 2251+9    T 2260   2250 
Q 1+6580    T 6581   6580 
Q 3733+92   T 3825   3802 
Q 1732-7826 T -6094  -6955
Q 71+13     T 84     94   
Q 6858-70   T 6788   6822 
Q 3-245     T -242   -243 

--------------------------------------------------
Iteration 22
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.7270 - loss: 0.7450
Epoch 1: val_loss improved from 0.83364 to 0.81819, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 12ms/step - accuracy: 0.7270 - loss: 0.7451 - val_accuracy: 0.6927 - val_loss: 0.8182
Q 82+513    T 595    503  
Q 8979-9384 T -405   -11  
Q 761-52    T 709    716  
Q 15-67     T -52    -53  
Q 0-447     T -447   -447 
Q 70+4536   T 4606   4622 
Q 9-90      T -81    -81  
Q 983+922   T 1905   1877 
Q 81+8816   T 8897   8930 
Q 6612-6    T 6606   6602 

--------------------------------------------------
Iteration 23
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.7342 - loss: 0.7294
Epoch 1: val_loss improved from 0.81819 to 0.79196, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7342 - loss: 0.7294 - val_accuracy: 0.7020 - val_loss: 0.7920
Q 9+250     T 259    258  
Q 45-4      T 41     40   
Q 58-512    T -454   -445 
Q 210-276   T -66    -52  
Q 846-718   T 128    16   
Q 2022-144  T 1878   1979 
Q 1768-9    T 1759   1768 
Q 361-68    T 293    288  
Q 67-7      T 60     60   
Q 2-1493    T -1491  -1491

--------------------------------------------------
Iteration 24
<span class="ansi-bold">276/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.7407 - loss: 0.7075
Epoch 1: val_loss did not improve from 0.79196
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7407 - loss: 0.7075 - val_accuracy: 0.7011 - val_loss: 0.7937
Q 3+117     T 120    120  
Q 68+956    T 1024   1042 
Q 85+9      T 94     95   
Q 6809-0    T 6809   6800 
Q 5+2191    T 2196   2116 
Q 5670+62   T 5732   6722 
Q 98+28     T 126    117  
Q 365-8991  T -8626  -8544
Q 561-710   T -149   -167 
Q 744-2270  T -1526  -1667

--------------------------------------------------
Iteration 25
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7467 - loss: 0.6948
Epoch 1: val_loss improved from 0.79196 to 0.77708, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7467 - loss: 0.6948 - val_accuracy: 0.7057 - val_loss: 0.7771
Q 203-2     T 201    202  
Q 2178+566  T 2744   2844 
Q 5-7285    T -7280  -7271
Q 7872-558  T 7314   7265 
Q 384+8837  T 9221   9221 
Q 485-7     T 478    480  
Q 8-2462    T -2454  -2435
Q 3475-1    T 3474   3474 
Q 95-6      T 89     89   
Q 1467-60   T 1407   1498 

--------------------------------------------------
Iteration 26
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.7534 - loss: 0.6739
Epoch 1: val_loss improved from 0.77708 to 0.75846, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7534 - loss: 0.6739 - val_accuracy: 0.7138 - val_loss: 0.7585
Q 66+63     T 129    129  
Q 3374-213  T 3161   3241 
Q 9+5330    T 5339   5331 
Q 1763-3865 T -2102  -2599
Q 4866-48   T 4818   4818 
Q 81-7      T 74     74   
Q 5044+0    T 5044   5045 
Q 6935-9    T 6926   6935 
Q 82+9      T 91     91   
Q 0+9957    T 9957   9956 

--------------------------------------------------
Iteration 27
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.7601 - loss: 0.6570
Epoch 1: val_loss did not improve from 0.75846
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7601 - loss: 0.6570 - val_accuracy: 0.7084 - val_loss: 0.7746
Q 43+7968   T 8011   7924 
Q 5-9252    T -9247  -9247
Q 582-259   T 323    276  
Q 16+52     T 68     77   
Q 1792+205  T 1997   1918 
Q 880-8     T 872    872  
Q 5664-989  T 4675   4766 
Q 606+29    T 635    637  
Q 7115+4831 T 11946  11556
Q 4298+3865 T 8163   8335 

--------------------------------------------------
Iteration 28
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7682 - loss: 0.6375
Epoch 1: val_loss did not improve from 0.75846
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7682 - loss: 0.6375 - val_accuracy: 0.7142 - val_loss: 0.7590
Q 417+2     T 419    419  
Q 8417-11   T 8406   8410 
Q 50-788    T -738   -729 
Q 328+88    T 416    416  
Q 743+826   T 1569   1688 
Q 4200-81   T 4119   4110 
Q 6-548     T -542   -542 
Q 882+3768  T 4650   4555 
Q 82+7090   T 7172   7180 
Q 73+55     T 128    127  

--------------------------------------------------
Iteration 29
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.7755 - loss: 0.6191
Epoch 1: val_loss improved from 0.75846 to 0.72556, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7755 - loss: 0.6191 - val_accuracy: 0.7249 - val_loss: 0.7256
Q 14+13     T 27     27   
Q 3401-95   T 3306   3024 
Q 36-386    T -350   -353 
Q 6949-79   T 6870   6822 
Q 7907+8    T 7915   7906 
Q 238+21    T 259    251  
Q 96-572    T -476   -465 
Q 43-4399   T -4356  -4355
Q 2+48      T 50     40   
Q 97-50     T 47     95   

--------------------------------------------------
Iteration 30
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.7831 - loss: 0.6034
Epoch 1: val_loss did not improve from 0.72556
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7830 - loss: 0.6035 - val_accuracy: 0.7244 - val_loss: 0.7290
Q 432-29    T 403    392  
Q 22-6106   T -6084  -6085
Q 12-3      T 9      9    
Q 54+8532   T 8586   8507 
Q 7058-4    T 7054   7061 
Q 2263-1058 T 1205   156  
Q 9+5330    T 5339   5340 
Q 931-9     T 922    923  
Q 56-77     T -21    -22  
Q 563-7     T 556    546  

--------------------------------------------------
Iteration 31
<span class="ansi-bold">275/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.7873 - loss: 0.5889
Epoch 1: val_loss improved from 0.72556 to 0.70982, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.7873 - loss: 0.5889 - val_accuracy: 0.7298 - val_loss: 0.7098
Q 9975+2149 T 12124  11077
Q 11-2      T 9      9    
Q 7-7824    T -7817  -7817
Q 911+9     T 920    910  
Q 633-412   T 221    391  
Q 21+531    T 552    543  
Q 512+7     T 519    518  
Q 0-2210    T -2210  -2211
Q 866+29    T 895    895  
Q 459+1460  T 1919   1086 

--------------------------------------------------
Iteration 32
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7964 - loss: 0.5670
Epoch 1: val_loss improved from 0.70982 to 0.69729, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7964 - loss: 0.5670 - val_accuracy: 0.7357 - val_loss: 0.6973
Q 797+93    T 890    886  
Q 8-609     T -601   -601 
Q 3344+499  T 3843   3833 
Q 0-7558    T -7558  -5585
Q 110-9     T 101    103  
Q 2038-3    T 2035   2033 
Q 61+187    T 248    249  
Q 17-888    T -871   -870 
Q 7-462     T -455   -455 
Q 93-85     T 8      -9   

--------------------------------------------------
Iteration 33
<span class="ansi-bold">276/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.8019 - loss: 0.5510
Epoch 1: val_loss improved from 0.69729 to 0.69518, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.8018 - loss: 0.5511 - val_accuracy: 0.7401 - val_loss: 0.6952
Q 40+5704   T 5744   5745 
Q 440+729   T 1169   1131 
Q 211-79    T 132    11   
Q 30+32     T 62     63   
Q 745+3308  T 4053   4915 
Q 50-3506   T -3456  -3455
Q 5986+517  T 6503   6002 
Q 9365-1065 T 8300   8271 
Q 1324-2    T 1322   1321 
Q 415+4     T 419    419  

--------------------------------------------------
Iteration 34
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8056 - loss: 0.5426
Epoch 1: val_loss improved from 0.69518 to 0.68126, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8056 - loss: 0.5426 - val_accuracy: 0.7463 - val_loss: 0.6813
Q 4-2155    T -2151  -2152
Q 9908-14   T 9894   9864 
Q 0-6288    T -6288  -6288
Q 85+7      T 92     92   
Q 51+4966   T 5017   5979 
Q 2+378     T 380    380  
Q 1952+83   T 2035   2046 
Q 5-5400    T -5395  -5405
Q 9970+178  T 10148  10077
Q 245-728   T -483   -434 

--------------------------------------------------
Iteration 35
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8156 - loss: 0.5212
Epoch 1: val_loss improved from 0.68126 to 0.67613, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8155 - loss: 0.5213 - val_accuracy: 0.7464 - val_loss: 0.6761
Q 2754-294  T 2460   2130 
Q 764+9095  T 9859   9812 
Q 890-9014  T -8124  -8105
Q 289+5723  T 6012   5111 
Q 671-30    T 641    620  
Q 643+4     T 647    647  
Q 488+1     T 489    489  
Q 65+5778   T 5843   5843 
Q 9-40      T -31    -31  
Q 591+7578  T 8169   8068 

--------------------------------------------------
Iteration 36
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 11ms/step - accuracy: 0.8206 - loss: 0.5057
Epoch 1: val_loss improved from 0.67613 to 0.65928, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.8205 - loss: 0.5057 - val_accuracy: 0.7529 - val_loss: 0.6593
Q 567-492   T 75     -85  
Q 298+1351  T 1649   1888 
Q 73-37     T 36     34   
Q 861+5     T 866    867  
Q 897+85    T 982    192  
Q 7+3079    T 3086   3086 
Q 53-988    T -935   -944 
Q 2482-8564 T -6082  -6167
Q 2+52      T 54     54   
Q 4+4975    T 4979   4989 

--------------------------------------------------
Iteration 37
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8270 - loss: 0.4898
Epoch 1: val_loss improved from 0.65928 to 0.65140, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8270 - loss: 0.4899 - val_accuracy: 0.7594 - val_loss: 0.6514
Q 614+4     T 618    618  
Q 7999+6928 T 14927  17000
Q 37+58     T 95     85   
Q 574-304   T 270    391  
Q 74+5376   T 5450   5420 
Q 2070-194  T 1876   1945 
Q 929+18    T 947    937  
Q 9-6697    T -6688  -6688
Q 366+72    T 438    418  
Q 781+6     T 787    776  

--------------------------------------------------
Iteration 38
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8329 - loss: 0.4751
Epoch 1: val_loss did not improve from 0.65140
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8328 - loss: 0.4751 - val_accuracy: 0.7561 - val_loss: 0.6624
Q 882+3768  T 4650   4655 
Q 388-349   T 39     19   
Q 4620+1938 T 6558   6701 
Q 8+673     T 681    680  
Q 48+77     T 125    125  
Q 5-5400    T -5395  -5494
Q 8279+5986 T 14265  14776
Q 5-842     T -837   -827 
Q 91+86     T 177    177  
Q 3793+7538 T 11331  11012

--------------------------------------------------
Iteration 39
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.8408 - loss: 0.4591
Epoch 1: val_loss did not improve from 0.65140
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.8408 - loss: 0.4591 - val_accuracy: 0.7569 - val_loss: 0.6569
Q 473-6872  T -6399  -6500
Q 6201+239  T 6440   6330 
Q 0+51      T 51     51   
Q 58-2325   T -2267  -2376
Q 896-20    T 876    878  
Q 436-7     T 429    429  
Q 3-641     T -638   -638 
Q 0-942     T -942   -942 
Q 42-9168   T -9126  -9144
Q 642+1820  T 2462   2342 

--------------------------------------------------
Iteration 40
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8444 - loss: 0.4481
Epoch 1: val_loss improved from 0.65140 to 0.62957, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8443 - loss: 0.4482 - val_accuracy: 0.7669 - val_loss: 0.6296
Q 180-1     T 179    189  
Q 94+623    T 717    727  
Q 50+3161   T 3211   3221 
Q 6595-9    T 6586   6585 
Q 5003-7252 T -2249  -2727
Q 6197+18   T 6215   6205 
Q 727+65    T 792    791  
Q 6437+4326 T 10763  10043
Q 773+8572  T 9345   9635 
Q 95-1909   T -1814  -1854

--------------------------------------------------
Iteration 41
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8473 - loss: 0.4388
Epoch 1: val_loss did not improve from 0.62957
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8472 - loss: 0.4389 - val_accuracy: 0.7663 - val_loss: 0.6417
Q 9888+6    T 9894   9894 
Q 86+0      T 86     86   
Q 32-764    T -732   -741 
Q 295+94    T 389    389  
Q 9+531     T 540    540  
Q 922+7     T 929    929  
Q 6940-5    T 6935   6945 
Q 55-74     T -19    -29  
Q 2431-290  T 2141   2921 
Q 5098+3219 T 8317   8538 

--------------------------------------------------
Iteration 42
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8520 - loss: 0.4253
Epoch 1: val_loss improved from 0.62957 to 0.62566, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 9ms/step - accuracy: 0.8520 - loss: 0.4254 - val_accuracy: 0.7703 - val_loss: 0.6257
Q 113-5889  T -5776  -5766
Q 48-4891   T -4843  -4853
Q 8417-11   T 8406   8415 
Q 70-36     T 34     36   
Q 0-1684    T -1684  -1864
Q 5375+55   T 5430   5400 
Q 55+49     T 104    103  
Q 7-2682    T -2675  -2685
Q 6353-57   T 6296   6387 
Q 7405+16   T 7421   7450 

--------------------------------------------------
Iteration 43
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8611 - loss: 0.4074
Epoch 1: val_loss improved from 0.62566 to 0.60981, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.8611 - loss: 0.4074 - val_accuracy: 0.7743 - val_loss: 0.6098
Q 45-57     T -12    -12  
Q 4061-5941 T -1880  -591 
Q 738+615   T 1353   1383 
Q 687-88    T 599    509  
Q 5+406     T 411    412  
Q 226+0     T 226    226  
Q 9616-17   T 9599   9619 
Q 73-87     T -14    -14  
Q 480-6     T 474    474  
Q 52-43     T 9      1    

--------------------------------------------------
Iteration 44
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8659 - loss: 0.3946
Epoch 1: val_loss did not improve from 0.60981
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8659 - loss: 0.3946 - val_accuracy: 0.7767 - val_loss: 0.6106
Q 842-3     T 839    849  
Q 8273-2    T 8271   8271 
Q 576-8     T 568    579  
Q 8643+6135 T 14778  14888
Q 7-709     T -702   -702 
Q 70+679    T 749    739  
Q 4267+7    T 4274   4264 
Q 235+11    T 246    247  
Q 20-523    T -503   -402 
Q 3979-81   T 3898   3800 

--------------------------------------------------
Iteration 45
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8692 - loss: 0.3878
Epoch 1: val_loss improved from 0.60981 to 0.60865, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8692 - loss: 0.3878 - val_accuracy: 0.7767 - val_loss: 0.6087
Q 9645+9979 T 19624  18544
Q 5+710     T 715    715  
Q 6-73      T -67    -66  
Q 1763-3865 T -2102  -2799
Q 62+95     T 157    156  
Q 1178+0    T 1178   1177 
Q 5027-41   T 4986   4976 
Q 1400-0    T 1400   1000 
Q 59-51     T 8      1    
Q 9501+661  T 10162  10002

--------------------------------------------------
Iteration 46
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8772 - loss: 0.3693
Epoch 1: val_loss improved from 0.60865 to 0.60848, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8772 - loss: 0.3694 - val_accuracy: 0.7787 - val_loss: 0.6085
Q 3+117     T 120    120  
Q 84-5163   T -5079  -5092
Q 887-9     T 878    878  
Q 9+1658    T 1667   1667 
Q 1241-7387 T -6146  -5066
Q 86-3253   T -3167  -3157
Q 81-6      T 75     74   
Q 2-9489    T -9487  -9487
Q 1+5976    T 5977   5577 
Q 5664-989  T 4675   4667 

--------------------------------------------------
Iteration 47
<span class="ansi-bold">275/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8801 - loss: 0.3605
Epoch 1: val_loss improved from 0.60848 to 0.59812, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8800 - loss: 0.3606 - val_accuracy: 0.7841 - val_loss: 0.5981
Q 9-3794    T -3785  -3786
Q 63-35     T 28     28   
Q 6-548     T -542   -542 
Q 431+179   T 610    611  
Q 705-862   T -157   -17  
Q 27+9      T 36     36   
Q 2+316     T 318    319  
Q 431+179   T 610    611  
Q 44+1083   T 1127   1117 
Q 5+839     T 844    844  

--------------------------------------------------
Iteration 48
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8829 - loss: 0.3533
Epoch 1: val_loss improved from 0.59812 to 0.59384, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8829 - loss: 0.3534 - val_accuracy: 0.7853 - val_loss: 0.5938
Q 390-1     T 389    389  
Q 18+9      T 27     27   
Q 2163+9323 T 11486  11466
Q 9301-137  T 9164   8274 
Q 2+75      T 77     77   
Q 9699+3    T 9702   9601 
Q 0+3662    T 3662   3662 
Q 2181-81   T 2100   2089 
Q 2+316     T 318    318  
Q 8591-6    T 8585   8585 

--------------------------------------------------
Iteration 49
<span class="ansi-bold">276/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 7ms/step - accuracy: 0.8902 - loss: 0.3356
Epoch 1: val_loss improved from 0.59384 to 0.59329, saving model to best_model.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 8ms/step - accuracy: 0.8901 - loss: 0.3357 - val_accuracy: 0.7875 - val_loss: 0.5933
Q 599+2201  T 2800   2800 
Q 84+8      T 92     92   
Q 782-2     T 780    781  
Q 8083-9225 T -1142  -310 
Q 1959-962  T 997    723  
Q 78-3538   T -3460  -3450
Q 6-945     T -939   -939 
Q 57+92     T 149    159  
Q 2251+9    T 2260   2260 
Q 81+85     T 166    156  
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>In the initial model (without attention), the accuracy was around 77%, with a validation accuracy of approximately 70%. The model struggled with large numbers and showed inconsistent performance on basic arithmetic tasks.</p>
<p>However, after integrating the attention mechanism, the accuracy surged to 90%, and the validation accuracy improved to 81%, with a noticeable decrease in loss. These results reflect a substantial boost in performance, particularly in terms of accuracy and loss. The attention mechanism allowed the model to focus on the most relevant parts of the input sequence, which helped it manage more complex arithmetic operations and enhance its generalization capabilities.</p>
<p>Overall, the attention-based model outperformed the initial model across both training and validation sets, demonstrating that the attention mechanism significantly improved the models ability to solve arithmetic problems with greater accuracy.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Q1---PART-4">Q1 - PART 4<a class="anchor-link" href="#Q1---PART-4"></a></h1>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">GRU</span><span class="p">,</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">GlobalMaxPooling1D</span><span class="p">,</span> <span class="n">RepeatVector</span><span class="p">,</span> <span class="n">TimeDistributed</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">config_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"training_size"</span><span class="p">:</span> <span class="mi">40000</span><span class="p">,</span>
    <span class="s2">"digits"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">"hidden_size"</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">"batch_size"</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">"iterations"</span> <span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s2">"chars"</span> <span class="p">:</span> <span class="s1">'0123456789-+ '</span>
<span class="p">}</span>

<span class="c1"># Alternative model architecture using GRU and Convolutional Layers</span>
<span class="k">def</span> <span class="nf">build_model_3</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">n_chars</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="c1"># 1D Convolutional Layer - Extract features from input sequences</span>
    <span class="n">conv_1</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'hidden_size'</span><span class="p">],</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># GRU layer - Capture temporal dependencies (no need for Bidirectional for now)</span>
    <span class="n">gru_1</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'hidden_size'</span><span class="p">],</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">conv_1</span><span class="p">)</span>

    <span class="c1"># Dropout for regularization</span>
    <span class="n">gru_1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">gru_1</span><span class="p">)</span>

    <span class="c1"># Attention mechanism</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">()([</span><span class="n">gru_1</span><span class="p">,</span> <span class="n">gru_1</span><span class="p">])</span>

    <span class="c1"># Apply Global Max Pooling to the attention output</span>
    <span class="n">attention_pooled</span> <span class="o">=</span> <span class="n">GlobalMaxPooling1D</span><span class="p">()(</span><span class="n">attention</span><span class="p">)</span>

    <span class="c1"># Repeat vector to match the output sequence length</span>
    <span class="n">repeat_vector</span> <span class="o">=</span> <span class="n">RepeatVector</span><span class="p">(</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])(</span><span class="n">attention_pooled</span><span class="p">)</span>

    <span class="c1"># Decoder GRU layer</span>
    <span class="n">gru_2</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'hidden_size'</span><span class="p">],</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">repeat_vector</span><span class="p">)</span>

    <span class="c1"># Dropout layer for regularization</span>
    <span class="n">gru_2</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">gru_2</span><span class="p">)</span>

    <span class="c1"># TimeDistributed Dense layer to generate the final predictions</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_chars</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">))(</span><span class="n">gru_2</span><span class="p">)</span>

    <span class="n">model_3</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">model_3</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">model_3</span>

<span class="c1"># Input shape: (maxlen, len(chars)), Output shape: (digits + 1, len(chars))</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="c1"># Build and summarize the model</span>
<span class="n">model_3</span> <span class="o">=</span> <span class="n">build_model_3</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'chars'</span><span class="p">]))</span>
<span class="n">model_3</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c1"># Training the model and saving the best model</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s1">'best_model_3.h5'</span><span class="p">,</span>
                             <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_loss'</span><span class="p">,</span>
                             <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">mode</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span>
                             <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'iterations'</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'-'</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Iteration'</span><span class="p">,</span> <span class="n">iteration</span><span class="p">)</span>

    <span class="n">model_3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">config_dict</span><span class="p">[</span><span class="s1">'batch_size'</span><span class="p">],</span>
                <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">])</span>

    <span class="c1"># Select 10 samples from the validation set at random for error visualization</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_val</span><span class="p">))</span>
        <span class="n">rowx</span><span class="p">,</span> <span class="n">rowy</span> <span class="o">=</span> <span class="n">x_val</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span class="p">])],</span> <span class="n">y_val</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ind</span><span class="p">])]</span>

        <span class="n">preds</span> <span class="o">=</span> <span class="n">model_3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">rowx</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowx</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">rowy</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">guess</span> <span class="o">=</span> <span class="n">ctable</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">calc_argmax</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">'Q'</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'T'</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">correct</span> <span class="o">==</span> <span class="n">guess</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">' '</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_1"</span>
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold"> Layer (type)              </span><span style="font-weight: bold"> Output Shape           </span><span style="font-weight: bold">        Param # </span><span style="font-weight: bold"> Connected to           </span>

 input_layer_1              (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">9</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>)                        <span style="color: #00af00; text-decoration-color: #00af00">0</span>  -                      
 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)                                                                              

 conv1d (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv1D</span>)            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">9</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                   <span style="color: #00af00; text-decoration-color: #00af00">5,120</span>  input_layer_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]    

 gru (<span style="color: #0087ff; text-decoration-color: #0087ff">GRU</span>)                  (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">9</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                  <span style="color: #00af00; text-decoration-color: #00af00">99,072</span>  conv1d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           

 dropout (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)          (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">9</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">0</span>  gru[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]              

 attention_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Attention</span>)    (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">9</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">0</span>  dropout[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],         
                                                                    dropout[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]          

 global_max_pooling1d       (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                          <span style="color: #00af00; text-decoration-color: #00af00">0</span>  attention_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]      
 (<span style="color: #0087ff; text-decoration-color: #0087ff">GlobalMaxPooling1D</span>)                                                                      

 repeat_vector_1            (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">0</span>  global_max_pooling1d[<span style="color: #00af00; text-decoration-color: #00af00"></span> 
 (<span style="color: #0087ff; text-decoration-color: #0087ff">RepeatVector</span>)                                                                            

 gru_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">GRU</span>)                (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                  <span style="color: #00af00; text-decoration-color: #00af00">99,072</span>  repeat_vector_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]  

 dropout_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)        (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)                       <span style="color: #00af00; text-decoration-color: #00af00">0</span>  gru_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]            

 time_distributed_1         (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>)                    <span style="color: #00af00; text-decoration-color: #00af00">1,677</span>  dropout_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]        
 (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)                                                                         

</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">204,941</span> (800.55 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">204,941</span> (800.55 KB)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
--------------------------------------------------
Iteration 0
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.3242 - loss: 2.0102
Epoch 1: val_loss improved from inf to 1.59315, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">7s</span> 10ms/step - accuracy: 0.3244 - loss: 2.0095 - val_accuracy: 0.4209 - val_loss: 1.5932
Q 866+59    T 925    165  
Q 7720-921  T 6799   1122 
Q 699+5     T 704    110  
Q 5658-6295 T -637   -555 
Q 3676+3    T 3679   166  
Q 643+4     T 647    155  
Q 6825-42   T 6783   1222 
Q 426+4     T 430    115  
Q 747+5     T 752    115  
Q 716+43    T 759    111  

--------------------------------------------------
Iteration 1
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.4267 - loss: 1.5858
Epoch 1: val_loss improved from 1.59315 to 1.50408, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.4268 - loss: 1.5855 - val_accuracy: 0.4732 - val_loss: 1.5041
Q 30+32     T 62     33   
Q 4933+53   T 4986   4009 
Q 4-354     T -350   -304 
Q 74+5376   T 5450   5408 
Q 764+9095  T 9859   1000 
Q 3562+8    T 3570   5509 
Q 75-38     T 37     44   
Q 381-8656  T -8275  -600 
Q 8617-138  T 8479   1708 
Q 367+9913  T 10280  1000 

--------------------------------------------------
Iteration 2
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.4820 - loss: 1.4516
Epoch 1: val_loss improved from 1.50408 to 1.34984, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.4822 - loss: 1.4511 - val_accuracy: 0.5206 - val_loss: 1.3498
Q 189-1517  T -1328  -1155
Q 1879+34   T 1913   1885 
Q 84-1280   T -1196  -2115
Q 1267-8    T 1259   1225 
Q 1+385     T 386    388  
Q 9039+12   T 9051   9000 
Q 70+4536   T 4606   5455 
Q 9851-54   T 9797   9888 
Q 82+513    T 595    555  
Q 3+24      T 27     22   

--------------------------------------------------
Iteration 3
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.5169 - loss: 1.3557
Epoch 1: val_loss improved from 1.34984 to 1.30918, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.5170 - loss: 1.3553 - val_accuracy: 0.5417 - val_loss: 1.3092
Q 897-58    T 839    88   
Q 3344+5    T 3349   3333 
Q 476-7     T 469    470  
Q 583+552   T 1135   500  
Q 4696-6    T 4690   469  
Q 8+175     T 183    174  
Q 340+807   T 1147   100  
Q 19-821    T -802   -825 
Q 9+12      T 21     1    
Q 71+96     T 167    100  

--------------------------------------------------
Iteration 4
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.5396 - loss: 1.2810
Epoch 1: val_loss improved from 1.30918 to 1.24984, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.5396 - loss: 1.2809 - val_accuracy: 0.5544 - val_loss: 1.2498
Q 1+98      T 99     10   
Q 4+55      T 59     50   
Q 8329+909  T 9238   1388 
Q 5668-56   T 5612   566  
Q 802-109   T 693    800  
Q 652+6812  T 7464   1088 
Q 13+2      T 15     11   
Q 19+34     T 53     13   
Q 735-796   T -61    -1   
Q 8-276     T -268   -277 

--------------------------------------------------
Iteration 5
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 11ms/step - accuracy: 0.5543 - loss: 1.2281
Epoch 1: val_loss improved from 1.24984 to 1.20710, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.5543 - loss: 1.2281 - val_accuracy: 0.5658 - val_loss: 1.2071
Q 8874+2    T 8876   8882 
Q 3+726     T 729    727  
Q 1710-6544 T -4834  -545 
Q 2077+6883 T 8960   1000 
Q 3+6878    T 6881   6882 
Q 746+3     T 749    740  
Q 64-301    T -237   -30  
Q 17+2      T 19     11   
Q 1+25      T 26     22   
Q 2832+7666 T 10498  1100 

--------------------------------------------------
Iteration 6
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.5652 - loss: 1.1855
Epoch 1: val_loss improved from 1.20710 to 1.20436, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.5653 - loss: 1.1854 - val_accuracy: 0.5734 - val_loss: 1.2044
Q 6+6178    T 6184   6123 
Q 73+2938   T 3011   299  
Q 733-37    T 696    773  
Q 73-94     T -21    -3   
Q 193-8     T 185    198  
Q 259-88    T 171    25   
Q 817+4360  T 5177   4444 
Q 22-1      T 21     22   
Q 30+9202   T 9232   922  
Q 26-7743   T -7717  -7644

--------------------------------------------------
Iteration 7
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 17ms/step - accuracy: 0.5771 - loss: 1.1481
Epoch 1: val_loss improved from 1.20436 to 1.12699, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 18ms/step - accuracy: 0.5771 - loss: 1.1481 - val_accuracy: 0.5841 - val_loss: 1.1270
Q 115+2     T 117    110  
Q 278+6     T 284    278  
Q 764+9095  T 9859   1000 
Q 7-2682    T -2675  -2688
Q 82+6147   T 6229   6233 
Q 82-17     T 65     77   
Q 1+62      T 63     65   
Q 26-7743   T -7717  -7655
Q 154+17    T 171    155  
Q 85-2152   T -2067  -1145

--------------------------------------------------
Iteration 8
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.5869 - loss: 1.1218
Epoch 1: val_loss did not improve from 1.12699
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.5869 - loss: 1.1218 - val_accuracy: 0.5914 - val_loss: 1.1486
Q 3+82      T 85     85   
Q 896-89    T 807    89   
Q 610+642   T 1252   105  
Q 143+403   T 546    465  
Q 129+8     T 137    124  
Q 714+7     T 721    716  
Q 7895+82   T 7977   7999 
Q 661+530   T 1191   115  
Q 98+886    T 984    987  
Q 7895+82   T 7977   7999 

--------------------------------------------------
Iteration 9
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.5965 - loss: 1.0910
Epoch 1: val_loss did not improve from 1.12699
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.5965 - loss: 1.0908 - val_accuracy: 0.5961 - val_loss: 1.1479
Q 669-77    T 592    62   
Q 5+95      T 100    10   
Q 20-72     T -52    -55  
Q 8+6839    T 6847   6832 
Q 7425-98   T 7327   7340 
Q 91+899    T 990    980  
Q 19+291    T 310    390  
Q 9699+3    T 9702   960  
Q 930-66    T 864    897  
Q 4-2540    T -2536  -2530

--------------------------------------------------
Iteration 10
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.6048 - loss: 1.0668
Epoch 1: val_loss improved from 1.12699 to 1.08223, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 14ms/step - accuracy: 0.6048 - loss: 1.0668 - val_accuracy: 0.6044 - val_loss: 1.0822
Q 9+300     T 309    300  
Q 4869-6085 T -1216  -134 
Q 137+3707  T 3844   3444 
Q 52-43     T 9      1    
Q 5087-3755 T 1332   22   
Q 107+8     T 115    100  
Q 67+831    T 898    876  
Q 852+9     T 861    853  
Q 437+121   T 558    555  
Q 8697-3    T 8694   8782 

--------------------------------------------------
Iteration 11
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.6136 - loss: 1.0505
Epoch 1: val_loss did not improve from 1.08223
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6136 - loss: 1.0505 - val_accuracy: 0.6099 - val_loss: 1.0947
Q 5240-5    T 5235   5249 
Q 85+27     T 112    110  
Q 476+454   T 930    810  
Q 2-6300    T -6298  -620 
Q 3+580     T 583    589  
Q 175-3     T 172    174  
Q 936+2     T 938    937  
Q 7478+9132 T 16610  10000
Q 1069+9449 T 10518  1040 
Q 9+5752    T 5761   5722 

--------------------------------------------------
Iteration 12
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.6221 - loss: 1.0235
Epoch 1: val_loss did not improve from 1.08223
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6221 - loss: 1.0235 - val_accuracy: 0.6100 - val_loss: 1.1610
Q 252+278   T 530    44   
Q 8+2689    T 2697   2690 
Q 520+0     T 520    523  
Q 0-9445    T -9445  -944 
Q 41-48     T -7     -1   
Q 4+710     T 714    710  
Q 541-4     T 537    548  
Q 25-7491   T -7466  -744 
Q 34-3182   T -3148  -3011
Q 56-415    T -359   -34  

--------------------------------------------------
Iteration 13
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 14ms/step - accuracy: 0.6284 - loss: 1.0074
Epoch 1: val_loss did not improve from 1.08223
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 15ms/step - accuracy: 0.6284 - loss: 1.0073 - val_accuracy: 0.6172 - val_loss: 1.1279
Q 841-6797  T -5956  -5555
Q 572+464   T 1036   110  
Q 896-89    T 807    89   
Q 57-89     T -32    -22  
Q 259-88    T 171    150  
Q 63+3420   T 3483   3556 
Q 1995+6366 T 8361   655  
Q 540+561   T 1101   110  
Q 284+4     T 288    289  
Q 619-39    T 580    565  

--------------------------------------------------
Iteration 14
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.6358 - loss: 0.9845
Epoch 1: val_loss improved from 1.08223 to 1.02219, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.6358 - loss: 0.9846 - val_accuracy: 0.6312 - val_loss: 1.0222
Q 140-844   T -704   -755 
Q 575-398   T 177    22   
Q 690-66    T 624    654  
Q 446-341   T 105    2    
Q 40-56     T -16    -2   
Q 50-21     T 29     24   
Q 49+6262   T 6311   6322 
Q 879-407   T 472    433  
Q 0+774     T 774    774  
Q 50-3506   T -3456  -3465

--------------------------------------------------
Iteration 15
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.6389 - loss: 0.9717
Epoch 1: val_loss improved from 1.02219 to 1.01554, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6389 - loss: 0.9717 - val_accuracy: 0.6353 - val_loss: 1.0155
Q 408+359   T 767    766  
Q 3+127     T 130    124  
Q 572+2     T 574    579  
Q 918+5     T 923    923  
Q 8+414     T 422    427  
Q 6+77      T 83     86   
Q 0-6129    T -6129  -612 
Q 1602+305  T 1907   1199 
Q 9-84      T -75    -74  
Q 3941-52   T 3889   3999 

--------------------------------------------------
Iteration 16
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.6449 - loss: 0.9539
Epoch 1: val_loss did not improve from 1.01554
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6449 - loss: 0.9539 - val_accuracy: 0.6388 - val_loss: 1.0293
Q 936-59    T 877    891  
Q 561-139   T 422    449  
Q 1+3529    T 3530   3524 
Q 71+13     T 84     89   
Q 36+86     T 122    110  
Q 69-64     T 5      1    
Q 8+673     T 681    685  
Q 6+11      T 17     10   
Q 70-36     T 34     32   
Q 81+5      T 86     86   

--------------------------------------------------
Iteration 17
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.6516 - loss: 0.9376
Epoch 1: val_loss improved from 1.01554 to 0.97332, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6516 - loss: 0.9376 - val_accuracy: 0.6446 - val_loss: 0.9733
Q 5+20      T 25     24   
Q 8743+69   T 8812   877  
Q 62+6      T 68     66   
Q 58-740    T -682   -677 
Q 861+5     T 866    867  
Q 1+381     T 382    380  
Q 2022-144  T 1878   297  
Q 0+6256    T 6256   6257 
Q 960+65    T 1025   100  
Q 694-6     T 688    699  

--------------------------------------------------
Iteration 18
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.6594 - loss: 0.9183
Epoch 1: val_loss improved from 0.97332 to 0.95387, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 14ms/step - accuracy: 0.6594 - loss: 0.9183 - val_accuracy: 0.6505 - val_loss: 0.9539
Q 43+3      T 46     43   
Q 874-52    T 822    819  
Q 60+86     T 146    144  
Q 226+19    T 245    234  
Q 99-66     T 33     32   
Q 1470+888  T 2358   1445 
Q 541+9870  T 10411  1440 
Q 64+44     T 108    110  
Q 438+66    T 504    596  
Q 7210-3055 T 4155   255  

--------------------------------------------------
Iteration 19
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.6635 - loss: 0.9043
Epoch 1: val_loss did not improve from 0.95387
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6635 - loss: 0.9043 - val_accuracy: 0.6465 - val_loss: 1.0161
Q 909+279   T 1188   105  
Q 58-2325   T -2267  -226 
Q 6595-9    T 6586   6690 
Q 8-64      T -56    -55  
Q 319+7     T 326    324  
Q 5-491     T -486   -48  
Q 437-96    T 341    346  
Q 19+53     T 72     66   
Q 28-995    T -967   -966 
Q 949+5     T 954    955  

--------------------------------------------------
Iteration 20
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.6727 - loss: 0.8832
Epoch 1: val_loss did not improve from 0.95387
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.6727 - loss: 0.8833 - val_accuracy: 0.6528 - val_loss: 1.1004
Q 9+83      T 92     90   
Q 696-38    T 658    632  
Q 89+687    T 776    766  
Q 367+9913  T 10280  1055 
Q 5+207     T 212    212  
Q 8-195     T -187   -18  
Q 3+2999    T 3002   209  
Q 21+356    T 377    37   
Q 2689-60   T 2629   253  
Q 6+1397    T 1403   1309 

--------------------------------------------------
Iteration 21
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.6746 - loss: 0.8735
Epoch 1: val_loss improved from 0.95387 to 0.92566, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.6746 - loss: 0.8735 - val_accuracy: 0.6625 - val_loss: 0.9257
Q 714+7     T 721    710  
Q 206-20    T 186    198  
Q 0-7558    T -7558  -6558
Q 949+0     T 949    959  
Q 911+958   T 1869   1050 
Q 655-3     T 652    652  
Q 95-168    T -73    -88  
Q 417+2     T 419    410  
Q 922+7     T 929    929  
Q 55+820    T 875    877  

--------------------------------------------------
Iteration 22
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.6844 - loss: 0.8495
Epoch 1: val_loss improved from 0.92566 to 0.89148, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6844 - loss: 0.8496 - val_accuracy: 0.6698 - val_loss: 0.8915
Q 716+43    T 759    767  
Q 888+811   T 1699   1680 
Q 8+592     T 600    600  
Q 0-1684    T -1684  -168 
Q 140-844   T -704   -755 
Q 738+9     T 747    745  
Q 583+552   T 1135   1100 
Q 1+945     T 946    945  
Q 34+6154   T 6188   6109 
Q 46-0      T 46     46   

--------------------------------------------------
Iteration 23
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.6903 - loss: 0.8274
Epoch 1: val_loss improved from 0.89148 to 0.87252, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.6902 - loss: 0.8274 - val_accuracy: 0.6738 - val_loss: 0.8725
Q 33+4561   T 4594   4611 
Q 901+752   T 1653   1655 
Q 644-9667  T -9023  -8000
Q 443+5     T 448    440  
Q 6-73      T -67    -68  
Q 50-2      T 48     49   
Q 17-72     T -55    -53  
Q 45-96     T -51    -50  
Q 19+53     T 72     73   
Q 5664-989  T 4675   4777 

--------------------------------------------------
Iteration 24
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.6910 - loss: 0.8240
Epoch 1: val_loss did not improve from 0.87252
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.6910 - loss: 0.8240 - val_accuracy: 0.6754 - val_loss: 0.8924
Q 449+51    T 500    497  
Q 691+5     T 696    696  
Q 867-65    T 802    891  
Q 1467-60   T 1407   1437 
Q 9651-8    T 9643   9645 
Q 98+424    T 522    523  
Q 799+683   T 1482   1456 
Q 1550+668  T 2218   198  
Q 3407-6444 T -3037  -2955
Q 333+3     T 336    33   

--------------------------------------------------
Iteration 25
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7021 - loss: 0.7920
Epoch 1: val_loss did not improve from 0.87252
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7021 - loss: 0.7921 - val_accuracy: 0.6776 - val_loss: 0.8855
Q 48+84     T 132    132  
Q 1+86      T 87     87   
Q 609-6337  T -5728  -5645
Q 4541-851  T 3690   387  
Q 3+38      T 41     41   
Q 42+3      T 45     47   
Q 21+356    T 377    376  
Q 34-3182   T -3148  -3156
Q 7-1753    T -1746  -173 
Q 66+46     T 112    111  

--------------------------------------------------
Iteration 26
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7073 - loss: 0.7780
Epoch 1: val_loss improved from 0.87252 to 0.82851, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7073 - loss: 0.7780 - val_accuracy: 0.6926 - val_loss: 0.8285
Q 264-8     T 256    256  
Q 794+71    T 865    866  
Q 1+61      T 62     62   
Q 1+8981    T 8982   8990 
Q 20-5181   T -5161  -5170
Q 84+214    T 298    299  
Q 4-36      T -32    -32  
Q 218+0     T 218    218  
Q 6000+76   T 6076   6076 
Q 1968+5    T 1973   1969 

--------------------------------------------------
Iteration 27
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7119 - loss: 0.7618
Epoch 1: val_loss did not improve from 0.82851
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7119 - loss: 0.7618 - val_accuracy: 0.6932 - val_loss: 0.8301
Q 4589-96   T 4493   4595 
Q 73+9758   T 9831   9840 
Q 2618-2    T 2616   2619 
Q 873-9     T 864    865  
Q 86-71     T 15     6    
Q 72+63     T 135    134  
Q 96+259    T 355    356  
Q 3196+3217 T 6413   656  
Q 9026-247  T 8779   7799 
Q 650-10    T 640    640  

--------------------------------------------------
Iteration 28
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7191 - loss: 0.7412
Epoch 1: val_loss improved from 0.82851 to 0.79345, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7191 - loss: 0.7413 - val_accuracy: 0.7050 - val_loss: 0.7935
Q 1049-1    T 1048   1045 
Q 417+2     T 419    411  
Q 8-274     T -266   -266 
Q 1493-909  T 584    64   
Q 3+5344    T 5347   5347 
Q 56+49     T 105    104  
Q 738+77    T 815    712  
Q 2879-26   T 2853   2853 
Q 16-58     T -42    -43  
Q 2477+6    T 2483   2589 

--------------------------------------------------
Iteration 29
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.7264 - loss: 0.7223
Epoch 1: val_loss did not improve from 0.79345
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 12ms/step - accuracy: 0.7263 - loss: 0.7223 - val_accuracy: 0.7003 - val_loss: 0.8212
Q 9048+153  T 9201   9123 
Q 1765+859  T 2624   1344 
Q 6636+514  T 7150   7769 
Q 640+23    T 663    666  
Q 0-482     T -482   -482 
Q 639+9578  T 10217  10000
Q 3143-362  T 2781   278  
Q 13+376    T 389    399  
Q 561-710   T -149   -12  
Q 22+62     T 84     88   

--------------------------------------------------
Iteration 30
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7293 - loss: 0.7096
Epoch 1: val_loss did not improve from 0.79345
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7293 - loss: 0.7096 - val_accuracy: 0.6975 - val_loss: 0.8064
Q 206+1     T 207    206  
Q 0+3527    T 3527   3528 
Q 4701+1    T 4702   4711 
Q 277-1856  T -1579  -1544
Q 433+6112  T 6545   6562 
Q 5555-325  T 5230   5288 
Q 6780+71   T 6851   6890 
Q 56-77     T -21    -21  
Q 8099+2783 T 10882  1177 
Q 875+180   T 1055   1156 

--------------------------------------------------
Iteration 31
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7339 - loss: 0.6992
Epoch 1: val_loss did not improve from 0.79345
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7339 - loss: 0.6992 - val_accuracy: 0.7125 - val_loss: 0.7942
Q 24-47     T -23    -33  
Q 480+2     T 482    482  
Q 1952+83   T 2035   2959 
Q 56+4      T 60     60   
Q 2920+6    T 2926   2922 
Q 16-58     T -42    -42  
Q 35-808    T -773   -777 
Q 20-2791   T -2771  -2779
Q 78+0      T 78     78   
Q 79-72     T 7      1    

--------------------------------------------------
Iteration 32
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.7421 - loss: 0.6783
Epoch 1: val_loss improved from 0.79345 to 0.77864, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7421 - loss: 0.6783 - val_accuracy: 0.7157 - val_loss: 0.7786
Q 26-8990   T -8964  -8855
Q 2830+342  T 3172   2452 
Q 170-714   T -544   -544 
Q 7112+7527 T 14639  13442
Q 1224+7912 T 9136   9810 
Q 589+4     T 593    593  
Q 207+4747  T 4954   4989 
Q 2920+6    T 2926   2922 
Q 81+13     T 94     96   
Q 7564+7    T 7571   7559 

--------------------------------------------------
Iteration 33
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7466 - loss: 0.6644
Epoch 1: val_loss improved from 0.77864 to 0.74745, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7466 - loss: 0.6644 - val_accuracy: 0.7217 - val_loss: 0.7475
Q 5986+517  T 6503   6500 
Q 219-2     T 217    216  
Q 187+0     T 187    187  
Q 4452-7000 T -2548  -1667
Q 0-33      T -33    -33  
Q 4+219     T 223    223  
Q 4+3       T 7      6    
Q 9605+6049 T 15654  15458
Q 8314+9589 T 17903  17666
Q 3-7654    T -7651  -7652

--------------------------------------------------
Iteration 34
<span class="ansi-bold">276/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7517 - loss: 0.6520
Epoch 1: val_loss did not improve from 0.74745
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7518 - loss: 0.6519 - val_accuracy: 0.7253 - val_loss: 0.7679
Q 0-7035    T -7035  -7033
Q 214-5     T 209    209  
Q 5500-9    T 5491   5500 
Q 3270+4    T 3274   3276 
Q 438-794   T -356   -347 
Q 608-6     T 602    601  
Q 80+2647   T 2727   2723 
Q 24+4088   T 4112   4018 
Q 4+3565    T 3569   3578 
Q 5-1573    T -1568  -156 

--------------------------------------------------
Iteration 35
<span class="ansi-bold">276/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7596 - loss: 0.6315
Epoch 1: val_loss did not improve from 0.74745
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7596 - loss: 0.6316 - val_accuracy: 0.7280 - val_loss: 0.7719
Q 5-8780    T -8775  -8776
Q 29-6517   T -6488  -6498
Q 43+114    T 157    156  
Q 894-27    T 867    878  
Q 410-6     T 404    404  
Q 3-83      T -80    -80  
Q 97+119    T 216    216  
Q 980-955   T 25     3    
Q 73-664    T -591   -590 
Q 89-7165   T -7076  -7086

--------------------------------------------------
Iteration 36
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7631 - loss: 0.6180
Epoch 1: val_loss improved from 0.74745 to 0.72353, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7632 - loss: 0.6180 - val_accuracy: 0.7297 - val_loss: 0.7235
Q 310+74    T 384    387  
Q 67-4      T 63     63   
Q 368+2     T 370    370  
Q 45-69     T -24    -24  
Q 2-572     T -570   -560 
Q 92+62     T 154    153  
Q 8+561     T 569    579  
Q 80+8      T 88     88   
Q 9905-32   T 9873   9878 
Q 20+464    T 484    487  

--------------------------------------------------
Iteration 37
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7680 - loss: 0.6038
Epoch 1: val_loss improved from 0.72353 to 0.71882, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7681 - loss: 0.6038 - val_accuracy: 0.7403 - val_loss: 0.7188
Q 9-15      T -6     -    
Q 8+644     T 652    651  
Q 7866+5    T 7871   7860 
Q 21+531    T 552    542  
Q 0-5078    T -5078  -5078
Q 72+6052   T 6124   6022 
Q 11-83     T -72    -73  
Q 205+71    T 276    276  
Q 421-384   T 37     44   
Q 465+47    T 512    51   

--------------------------------------------------
Iteration 38
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.7761 - loss: 0.5848
Epoch 1: val_loss improved from 0.71882 to 0.67398, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7761 - loss: 0.5848 - val_accuracy: 0.7501 - val_loss: 0.6740
Q 154-71    T 83     14   
Q 510+124   T 634    644  
Q 4741+462  T 5203   5201 
Q 799+683   T 1482   1584 
Q 3910+9    T 3919   3919 
Q 476-467   T 9      10   
Q 86+859    T 945    954  
Q 6273-44   T 6229   6220 
Q 9707-8604 T 1103   100  
Q 73+9758   T 9831   9832 

--------------------------------------------------
Iteration 39
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.7822 - loss: 0.5672
Epoch 1: val_loss improved from 0.67398 to 0.65777, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7822 - loss: 0.5672 - val_accuracy: 0.7507 - val_loss: 0.6578
Q 84-1280   T -1196  -1106
Q 707+143   T 850    841  
Q 874-6     T 868    868  
Q 1+3323    T 3324   332  
Q 1486+9428 T 10914  9860 
Q 123+65    T 188    188  
Q 624+9     T 633    633  
Q 9-15      T -6     -    
Q 890-9014  T -8124  -8024
Q 81-6      T 75     75   

--------------------------------------------------
Iteration 40
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.7861 - loss: 0.5548
Epoch 1: val_loss did not improve from 0.65777
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.7861 - loss: 0.5548 - val_accuracy: 0.7555 - val_loss: 0.6674
Q 19-918    T -899   -809 
Q 2561-98   T 2463   2550 
Q 6809-0    T 6809   6709 
Q 2477+6    T 2483   2482 
Q 11-5883   T -5872  -5776
Q 8145-1795 T 6350   600  
Q 7+5382    T 5389   5389 
Q 4869-6085 T -1216  -164 
Q 6852+9    T 6861   6851 
Q 1-37      T -36    -36  

--------------------------------------------------
Iteration 41
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.7920 - loss: 0.5463
Epoch 1: val_loss improved from 0.65777 to 0.64964, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.7920 - loss: 0.5462 - val_accuracy: 0.7573 - val_loss: 0.6496
Q 7998-9    T 7989   7999 
Q 1596-3    T 1593   1693 
Q 3459-4184 T -725   -75  
Q 67+831    T 898    897  
Q 82+811    T 893    898  
Q 183+3     T 186    186  
Q 7742-49   T 7693   7688 
Q 911+7     T 918    918  
Q 7+81      T 88     88   
Q 2+7326    T 7328   7327 

--------------------------------------------------
Iteration 42
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.7987 - loss: 0.5266
Epoch 1: val_loss improved from 0.64964 to 0.64185, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.7987 - loss: 0.5266 - val_accuracy: 0.7659 - val_loss: 0.6418
Q 58+3141   T 3199   3289 
Q 0-957     T -957   -967 
Q 6+8315    T 8321   8321 
Q 32+806    T 838    848  
Q 3+3769    T 3772   3772 
Q 6-894     T -888   -887 
Q 19+1567   T 1586   1587 
Q 805+644   T 1449   1469 
Q 619+79    T 698    699  
Q 65-50     T 15     1    

--------------------------------------------------
Iteration 43
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 13ms/step - accuracy: 0.8060 - loss: 0.5078
Epoch 1: val_loss did not improve from 0.64185
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 14ms/step - accuracy: 0.8060 - loss: 0.5078 - val_accuracy: 0.7654 - val_loss: 0.6449
Q 786+37    T 823    814  
Q 1410-14   T 1396   1397 
Q 1+878     T 879    879  
Q 714+29    T 743    742  
Q 246+62    T 308    398  
Q 16-142    T -126   -126 
Q 3475-1    T 3474   3474 
Q 946+7     T 953    953  
Q 531+929   T 1460   1459 
Q 150+934   T 1084   1076 

--------------------------------------------------
Iteration 44
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.8120 - loss: 0.4945
Epoch 1: val_loss improved from 0.64185 to 0.61801, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8120 - loss: 0.4946 - val_accuracy: 0.7699 - val_loss: 0.6180
Q 177-3235  T -3058  -2978
Q 8117-1    T 8116   8115 
Q 492-4419  T -3927  -4877
Q 426+44    T 470    479  
Q 451+4830  T 5281   5200 
Q 7+342     T 349    349  
Q 6811+9    T 6820   6810 
Q 93-955    T -862   -869 
Q 38-752    T -714   -703 
Q 2-6330    T -6328  -6328

--------------------------------------------------
Iteration 45
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8183 - loss: 0.4787
Epoch 1: val_loss did not improve from 0.61801
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8183 - loss: 0.4787 - val_accuracy: 0.7640 - val_loss: 0.6728
Q 698+71    T 769    769  
Q 6188+672  T 6860   6700 
Q 449-6     T 443    443  
Q 8-9626    T -9618  -9608
Q 1+820     T 821    821  
Q 3+2999    T 3002   290  
Q 43+884    T 927    917  
Q 3582+4    T 3586   3687 
Q 501+1     T 502    502  
Q 6206+7    T 6213   6212 

--------------------------------------------------
Iteration 46
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 13ms/step - accuracy: 0.8223 - loss: 0.4665
Epoch 1: val_loss improved from 0.61801 to 0.60701, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 14ms/step - accuracy: 0.8223 - loss: 0.4665 - val_accuracy: 0.7748 - val_loss: 0.6070
Q 96-572    T -476   -477 
Q 6-721     T -715   -715 
Q 4606-237  T 4369   4378 
Q 83-753    T -670   -669 
Q 363+5     T 368    378  
Q 4815-644  T 4171   4199 
Q 22+94     T 116    107  
Q 92+9      T 101    90   
Q 23+56     T 79     89   
Q 92+9540   T 9632   9632 

--------------------------------------------------
Iteration 47
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8257 - loss: 0.4556
Epoch 1: val_loss improved from 0.60701 to 0.57086, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8257 - loss: 0.4556 - val_accuracy: 0.7831 - val_loss: 0.5709
Q 328-787   T -459   -351 
Q 5420+79   T 5499   5409 
Q 1-181     T -180   -180 
Q 705+5     T 710    700  
Q 4+210     T 214    214  
Q 2-374     T -372   -371 
Q 5343-1    T 5342   5348 
Q 5+278     T 283    283  
Q 6097+2    T 6099   6000 
Q 379+3682  T 4061   4130 

--------------------------------------------------
Iteration 48
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8317 - loss: 0.4434
Epoch 1: val_loss did not improve from 0.57086
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8317 - loss: 0.4434 - val_accuracy: 0.7829 - val_loss: 0.5985
Q 59-8881   T -8822  -8702
Q 64-4054   T -3990  -4999
Q 9-554     T -545   -546 
Q 7+2129    T 2136   2136 
Q 19-3383   T -3364  -3376
Q 3983-20   T 3963   3967 
Q 84-1280   T -1196  -1106
Q 66+3845   T 3911   3900 
Q 533-677   T -144   -127 
Q 6+300     T 306    307  

--------------------------------------------------
Iteration 49
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.8384 - loss: 0.4282
Epoch 1: val_loss did not improve from 0.57086
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 12ms/step - accuracy: 0.8384 - loss: 0.4282 - val_accuracy: 0.7785 - val_loss: 0.6609
Q 4424+1    T 4425   4426 
Q 11-2      T 9      9    
Q 67-62     T 5      6    
Q 766+8     T 774    774  
Q 4+130     T 134    135  
Q 7398-483  T 6915   6902 
Q 9-3794    T -3785  -3786
Q 130+872   T 1002   901  
Q 42+912    T 954    955  
Q 4565+2688 T 7253   756  

--------------------------------------------------
Iteration 50
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.8404 - loss: 0.4216
Epoch 1: val_loss did not improve from 0.57086
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8404 - loss: 0.4216 - val_accuracy: 0.7875 - val_loss: 0.5983
Q 3-641     T -638   -638 
Q 586+93    T 679    689  
Q 338-1     T 337    337  
Q 89+93     T 182    182  
Q 3614-74   T 3540   3549 
Q 281-551   T -270   -280 
Q 8405+6754 T 15159  15000
Q 9+352     T 361    360  
Q 58+231    T 289    280  
Q 0+36      T 36     36   

--------------------------------------------------
Iteration 51
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8481 - loss: 0.4035
Epoch 1: val_loss did not improve from 0.57086
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8481 - loss: 0.4035 - val_accuracy: 0.7914 - val_loss: 0.5745
Q 669-693   T -24    -16  
Q 714+0     T 714    714  
Q 5059+8    T 5067   5067 
Q 51-30     T 21     21   
Q 10-6788   T -6778  -6879
Q 714+0     T 714    714  
Q 8009-8    T 8001   8001 
Q 615-949   T -334   -356 
Q 8732-14   T 8718   8708 
Q 8-9278    T -9270  -9260

--------------------------------------------------
Iteration 52
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8489 - loss: 0.3986
Epoch 1: val_loss improved from 0.57086 to 0.53920, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8489 - loss: 0.3986 - val_accuracy: 0.7998 - val_loss: 0.5392
Q 6+5       T 11     10   
Q 115-76    T 39     42   
Q 7154-2    T 7152   7141 
Q 148+3203  T 3351   3440 
Q 1388-444  T 944    985  
Q 65+14     T 79     79   
Q 200-0     T 200    200  
Q 987-508   T 479    479  
Q 8641-1    T 8640   8630 
Q 31-7973   T -7942  -7942

--------------------------------------------------
Iteration 53
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8557 - loss: 0.3847
Epoch 1: val_loss did not improve from 0.53920
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8557 - loss: 0.3847 - val_accuracy: 0.7932 - val_loss: 0.6033
Q 9-544     T -535   -535 
Q 97-848    T -751   -749 
Q 288-642   T -354   -443 
Q 1763-3865 T -2102  -110 
Q 894-5     T 889    889  
Q 6+620     T 626    627  
Q 4-36      T -32    -32  
Q 325+3     T 328    328  
Q 3-938     T -935   -935 
Q 98+8      T 106    106  

--------------------------------------------------
Iteration 54
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.8616 - loss: 0.3696
Epoch 1: val_loss improved from 0.53920 to 0.53353, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.8615 - loss: 0.3698 - val_accuracy: 0.8066 - val_loss: 0.5335
Q 2605-35   T 2570   2689 
Q 3-729     T -726   -725 
Q 1+692     T 693    693  
Q 1606-227  T 1379   1439 
Q 785+842   T 1627   1627 
Q 7066+1999 T 9065   9005 
Q 0+51      T 51     51   
Q 5659+77   T 5736   5736 
Q 7704+0    T 7704   7603 
Q 888+81    T 969    969  

--------------------------------------------------
Iteration 55
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8617 - loss: 0.3677
Epoch 1: val_loss improved from 0.53353 to 0.53201, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8617 - loss: 0.3678 - val_accuracy: 0.8060 - val_loss: 0.5320
Q 6123+90   T 6213   6213 
Q 95+73     T 168    168  
Q 8065+506  T 8571   8601 
Q 71+8      T 79     79   
Q 1+266     T 267    26   
Q 572+8     T 580    580  
Q 6+3949    T 3955   395  
Q 38-9      T 29     29   
Q 89-522    T -433   -432 
Q 1726-379  T 1347   125  

--------------------------------------------------
Iteration 56
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.8642 - loss: 0.3609
Epoch 1: val_loss improved from 0.53201 to 0.46228, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 12ms/step - accuracy: 0.8642 - loss: 0.3609 - val_accuracy: 0.8271 - val_loss: 0.4623
Q 1606-227  T 1379   1439 
Q 827+935   T 1762   1762 
Q 997-2     T 995    995  
Q 5+13      T 18     18   
Q 10+496    T 506    507  
Q 48+489    T 537    536  
Q 0+572     T 572    572  
Q 67+5      T 72     72   
Q 2893+6210 T 9103   9008 
Q 765-89    T 676    676  

--------------------------------------------------
Iteration 57
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8652 - loss: 0.3580
Epoch 1: val_loss did not improve from 0.46228
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.8652 - loss: 0.3579 - val_accuracy: 0.8205 - val_loss: 0.4922
Q 22+973    T 995    985  
Q 66+46     T 112    112  
Q 2767-7184 T -4417  -3307
Q 19-3383   T -3364  -3366
Q 18-14     T 4      1    
Q 898+8     T 906    906  
Q 811+3     T 814    814  
Q 714+29    T 743    742  
Q 241+81    T 322    321  
Q 6437+4326 T 10763  1089 

--------------------------------------------------
Iteration 58
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8735 - loss: 0.3391
Epoch 1: val_loss did not improve from 0.46228
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8735 - loss: 0.3391 - val_accuracy: 0.8142 - val_loss: 0.5357
Q 93-81     T 12     1    
Q 762-52    T 710    700  
Q 440+729   T 1169   1089 
Q 4-2542    T -2538  -253 
Q 71+96     T 167    167  
Q 9-550     T -541   -541 
Q 64-4054   T -3990  -3900
Q 350+8     T 358    368  
Q 90+39     T 129    129  
Q 0+5983    T 5983   5983 

--------------------------------------------------
Iteration 59
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8730 - loss: 0.3378
Epoch 1: val_loss did not improve from 0.46228
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8730 - loss: 0.3378 - val_accuracy: 0.8067 - val_loss: 0.5652
Q 35-808    T -773   -763 
Q 309-542   T -233   -222 
Q 865-25    T 840    840  
Q 94+246    T 340    330  
Q 440+729   T 1169   1089 
Q 505+4413  T 4918   5808 
Q 501+1     T 502    502  
Q 151+473   T 624    716  
Q 7-505     T -498   -498 
Q 8+4648    T 4656   4656 

--------------------------------------------------
Iteration 60
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.8769 - loss: 0.3303
Epoch 1: val_loss did not improve from 0.46228
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.8769 - loss: 0.3304 - val_accuracy: 0.8225 - val_loss: 0.4868
Q 70-36     T 34     44   
Q 3+726     T 729    729  
Q 893-8993  T -8100  -8099
Q 561-710   T -149   -62  
Q 5-4182    T -4177  -4177
Q 7909+38   T 7947   7947 
Q 6000+76   T 6076   607  
Q 973-1     T 972    972  
Q 2616+2844 T 5460   640  
Q 0-70      T -70    -70  

--------------------------------------------------
Iteration 61
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.8814 - loss: 0.3215
Epoch 1: val_loss did not improve from 0.46228
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8814 - loss: 0.3215 - val_accuracy: 0.8112 - val_loss: 0.5376
Q 1081-51   T 1030   1000 
Q 7846-151  T 7695   7694 
Q 449-6     T 443    443  
Q 7478+9132 T 16610  16500
Q 23+88     T 111    111  
Q 1196-3    T 1193   1193 
Q 530+1414  T 1944   1874 
Q 249+9     T 258    258  
Q 51+114    T 165    166  
Q 8419-10   T 8409   8410 

--------------------------------------------------
Iteration 62
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.8802 - loss: 0.3216
Epoch 1: val_loss did not improve from 0.46228
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8802 - loss: 0.3215 - val_accuracy: 0.8219 - val_loss: 0.5039
Q 6+263     T 269    269  
Q 9774+6    T 9780   9780 
Q 312+0     T 312    312  
Q 380-37    T 343    343  
Q 2070-194  T 1876   1986 
Q 1996-8709 T -6713  -6108
Q 3283+2330 T 5613   650  
Q 847-141   T 706    707  
Q 98+363    T 461    461  
Q 1409-4    T 1405   1495 

--------------------------------------------------
Iteration 63
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8852 - loss: 0.3082
Epoch 1: val_loss did not improve from 0.46228
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8852 - loss: 0.3083 - val_accuracy: 0.8052 - val_loss: 0.5724
Q 5757-41   T 5716   5706 
Q 4095-398  T 3697   369  
Q 5+553     T 558    558  
Q 4887+9990 T 14877  15887
Q 218+0     T 218    218  
Q 3777+3    T 3780   3780 
Q 330+4731  T 5061   500  
Q 82-17     T 65     65   
Q 480+2     T 482    482  
Q 76-137    T -61    -50  

--------------------------------------------------
Iteration 64
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 13ms/step - accuracy: 0.8881 - loss: 0.3004
Epoch 1: val_loss did not improve from 0.46228
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.8881 - loss: 0.3004 - val_accuracy: 0.8179 - val_loss: 0.5323
Q 7-8       T -1     -    
Q 25+29     T 54     54   
Q 6445-9033 T -2588  -2200
Q 380-37    T 343    343  
Q 835+72    T 907    907  
Q 8649-673  T 7976   7986 
Q 4+210     T 214    215  
Q 4565+2688 T 7253   756  
Q 82+7090   T 7172   7172 
Q 68-878    T -810   -700 

--------------------------------------------------
Iteration 65
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.8911 - loss: 0.2931
Epoch 1: val_loss did not improve from 0.46228
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8911 - loss: 0.2931 - val_accuracy: 0.8118 - val_loss: 0.5834
Q 8+414     T 422    422  
Q 18+2      T 20     10   
Q 7158-760  T 6398   6598 
Q 482-4     T 478    478  
Q 6292+191  T 6483   6492 
Q 555-34    T 521    520  
Q 32-1414   T -1382  -1471
Q 8+4235    T 4243   4243 
Q 2542-3776 T -1234  -140 
Q 7-1435    T -1428  -142 

--------------------------------------------------
Iteration 66
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.8862 - loss: 0.3074
Epoch 1: val_loss did not improve from 0.46228
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8862 - loss: 0.3073 - val_accuracy: 0.8236 - val_loss: 0.5272
Q 16-40     T -24    -24  
Q 95-12     T 83     8    
Q 5027-41   T 4986   498  
Q 488+1075  T 1563   1563 
Q 0+31      T 31     31   
Q 1-752     T -751   -751 
Q 3859+5    T 3864   3864 
Q 6+3949    T 3955   395  
Q 5+80      T 85     85   
Q 82+64     T 146    146  

--------------------------------------------------
Iteration 67
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 11ms/step - accuracy: 0.8924 - loss: 0.2909
Epoch 1: val_loss improved from 0.46228 to 0.42539, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.8924 - loss: 0.2910 - val_accuracy: 0.8471 - val_loss: 0.4254
Q 86+0      T 86     86   
Q 6-721     T -715   -725 
Q 850+0     T 850    850  
Q 8-1784    T -1776  -1776
Q 0+5127    T 5127   5127 
Q 415+4948  T 5363   532  
Q 9-51      T -42    -42  
Q 185+9     T 194    194  
Q 3+196     T 199    199  
Q 1338-3    T 1335   1335 

--------------------------------------------------
Iteration 68
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.8948 - loss: 0.2838
Epoch 1: val_loss did not improve from 0.42539
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.8948 - loss: 0.2837 - val_accuracy: 0.8324 - val_loss: 0.4852
Q 64-795    T -731   -721 
Q 57-672    T -615   -605 
Q 494+43    T 537    537  
Q 70+24     T 94     96   
Q 10-51     T -41    -31  
Q 20-97     T -77    -77  
Q 679-1473  T -794   -89  
Q 6+4083    T 4089   4090 
Q 389+7023  T 7412   7412 
Q 24-47     T -23    -23  

--------------------------------------------------
Iteration 69
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9001 - loss: 0.2703
Epoch 1: val_loss did not improve from 0.42539
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9001 - loss: 0.2703 - val_accuracy: 0.8332 - val_loss: 0.4804
Q 1+61      T 62     62   
Q 29+1      T 30     20   
Q 45-278    T -233   -233 
Q 610+642   T 1252   1342 
Q 9-5097    T -5088  -5088
Q 643+4     T 647    647  
Q 9975+2149 T 12124  11204
Q 68-1517   T -1449  -1449
Q 121-3054  T -2933  -2913
Q 205-7040  T -6835  -6857

--------------------------------------------------
Iteration 70
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 11ms/step - accuracy: 0.8981 - loss: 0.2733
Epoch 1: val_loss did not improve from 0.42539
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 12ms/step - accuracy: 0.8981 - loss: 0.2733 - val_accuracy: 0.8360 - val_loss: 0.4622
Q 148+3203  T 3351   3441 
Q 7478+9132 T 16610  16500
Q 6+4962    T 4968   4968 
Q 7-919     T -912   -912 
Q 461+21    T 482    482  
Q 81-113    T -32    -2   
Q 85-257    T -172   -182 
Q 4943+67   T 5010   5000 
Q 87-8      T 79     89   
Q 53+78     T 131    131  

--------------------------------------------------
Iteration 71
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9021 - loss: 0.2670
Epoch 1: val_loss did not improve from 0.42539
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.9021 - loss: 0.2670 - val_accuracy: 0.8426 - val_loss: 0.4572
Q 93-23     T 70     70   
Q 78-78     T 0      8    
Q 86-59     T 27     27   
Q 2595-85   T 2510   2500 
Q 0+9957    T 9957   9957 
Q 4635+5201 T 9836   9856 
Q 476-7     T 469    469  
Q 7274-209  T 7065   7065 
Q 2178+566  T 2744   2754 
Q 7443+0    T 7443   7443 

--------------------------------------------------
Iteration 72
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9044 - loss: 0.2603
Epoch 1: val_loss did not improve from 0.42539
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9043 - loss: 0.2603 - val_accuracy: 0.8265 - val_loss: 0.5232
Q 2667+5216 T 7883   688  
Q 2482-8564 T -6082  -6001
Q 6781-3599 T 3182   3280 
Q 3835-735  T 3100   300  
Q 9-460     T -451   -451 
Q 4675+403  T 5078   4979 
Q 9703-2166 T 7537   617  
Q 9450+3    T 9453   9453 
Q 5891+7    T 5898   5898 
Q 7305-4    T 7301   7300 

--------------------------------------------------
Iteration 73
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9049 - loss: 0.2579
Epoch 1: val_loss did not improve from 0.42539
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9049 - loss: 0.2579 - val_accuracy: 0.8268 - val_loss: 0.5358
Q 4648-6972 T -2324  -1343
Q 200-0     T 200    100  
Q 7+818     T 825    825  
Q 2-860     T -858   -858 
Q 569+4036  T 4605   468  
Q 76-101    T -25    -4   
Q 7+7741    T 7748   7748 
Q 3+956     T 959    959  
Q 3622-8418 T -4796  -3908
Q 521+352   T 873    783  

--------------------------------------------------
Iteration 74
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 13ms/step - accuracy: 0.9038 - loss: 0.2587
Epoch 1: val_loss did not improve from 0.42539
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 14ms/step - accuracy: 0.9038 - loss: 0.2587 - val_accuracy: 0.8338 - val_loss: 0.4916
Q 17-763    T -746   -747 
Q 28-41     T -13    -1   
Q 1+571     T 572    572  
Q 61-4      T 57     58   
Q 7544-7    T 7537   7537 
Q 521+5     T 526    526  
Q 58-2325   T -2267  -2278
Q 549-311   T 238    238  
Q 6858-70   T 6788   6888 
Q 3216+3735 T 6951   6091 

--------------------------------------------------
Iteration 75
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9073 - loss: 0.2509
Epoch 1: val_loss did not improve from 0.42539
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9073 - loss: 0.2509 - val_accuracy: 0.8448 - val_loss: 0.4510
Q 224-35    T 189    189  
Q 3+9544    T 9547   9547 
Q 511-90    T 421    420  
Q 139-20    T 119    129  
Q 856-412   T 444    437  
Q 49-94     T -45    -45  
Q 1+37      T 38     38   
Q 85-525    T -440   -440 
Q 2+48      T 50     50   
Q 4644-7266 T -2622  -2800

--------------------------------------------------
Iteration 76
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.9115 - loss: 0.2394
Epoch 1: val_loss did not improve from 0.42539
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9115 - loss: 0.2395 - val_accuracy: 0.8475 - val_loss: 0.4537
Q 1606-227  T 1379   1489 
Q 78-78     T 0      0    
Q 4866-48   T 4818   4708 
Q 60-8      T 52     52   
Q 51+1960   T 2011   2010 
Q 878-7     T 871    871  
Q 487+7651  T 8138   8288 
Q 81+8816   T 8897   899  
Q 5133-491  T 4642   4532 
Q 438-498   T -60    14   

--------------------------------------------------
Iteration 77
<span class="ansi-bold">276/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.9129 - loss: 0.2356
Epoch 1: val_loss improved from 0.42539 to 0.42049, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 11ms/step - accuracy: 0.9129 - loss: 0.2357 - val_accuracy: 0.8519 - val_loss: 0.4205
Q 45-57     T -12    -12  
Q 706-2     T 704    704  
Q 8525+98   T 8623   8623 
Q 5-8909    T -8904  -890 
Q 7-6062    T -6055  -6046
Q 32-6      T 26     26   
Q 7866+5    T 7871   7871 
Q 1500+3393 T 4893   5903 
Q 94+8617   T 8711   8611 
Q 13+0      T 13     13   

--------------------------------------------------
Iteration 78
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.9120 - loss: 0.2399
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9120 - loss: 0.2399 - val_accuracy: 0.8262 - val_loss: 0.5479
Q 1-203     T -202   -201 
Q 252-93    T 159    169  
Q 566-7449  T -6883  -698 
Q 959-602   T 357    377  
Q 9862+49   T 9911   9811 
Q 177-3235  T -3058  -2978
Q 17+956    T 973    97   
Q 819-3928  T -3109  -3198
Q 3452+5    T 3457   3457 
Q 419-4     T 415    415  

--------------------------------------------------
Iteration 79
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 10ms/step - accuracy: 0.9121 - loss: 0.2385
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 12ms/step - accuracy: 0.9121 - loss: 0.2385 - val_accuracy: 0.8453 - val_loss: 0.4567
Q 608-6     T 602    602  
Q 0-240     T -240   -240 
Q 328-787   T -459   -479 
Q 2637+1    T 2638   2638 
Q 67-8035   T -7968  -7978
Q 894-0     T 894    894  
Q 0-70      T -70    -70  
Q 7352+195  T 7547   7437 
Q 8-9626    T -9618  -9508
Q 86+859    T 945    945  

--------------------------------------------------
Iteration 80
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9188 - loss: 0.2220
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9187 - loss: 0.2221 - val_accuracy: 0.8403 - val_loss: 0.4690
Q 11+57     T 68     68   
Q 72-1      T 71     71   
Q 6929+5    T 6934   6934 
Q 709-6     T 703    702  
Q 60-3136   T -3076  -3077
Q 733-37    T 696    707  
Q 383+932   T 1315   1316 
Q 627+7206  T 7833   783  
Q 53+78     T 131    131  
Q 30+32     T 62     61   

--------------------------------------------------
Iteration 81
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9163 - loss: 0.2256
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9163 - loss: 0.2256 - val_accuracy: 0.8339 - val_loss: 0.5160
Q 1839+6    T 1845   1845 
Q 94+623    T 717    718  
Q 6-73      T -67    -67  
Q 7014+32   T 7046   7057 
Q 8+175     T 183    183  
Q 342+0     T 342    342  
Q 236-431   T -195   -186 
Q 125-1     T 124    124  
Q 790+945   T 1735   1636 
Q 387+1     T 388    388  

--------------------------------------------------
Iteration 82
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.9192 - loss: 0.2232
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.9192 - loss: 0.2233 - val_accuracy: 0.8391 - val_loss: 0.5022
Q 81-113    T -32    -3   
Q 289+5723  T 6012   5002 
Q 2+5745    T 5747   5747 
Q 0+23      T 23     23   
Q 586+93    T 679    689  
Q 344-94    T 250    250  
Q 54-91     T -37    -37  
Q 18+115    T 133    133  
Q 61-4      T 57     57   
Q 9792+3537 T 13329  13459

--------------------------------------------------
Iteration 83
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.9158 - loss: 0.2294
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9158 - loss: 0.2294 - val_accuracy: 0.8353 - val_loss: 0.5180
Q 6794-3    T 6791   6781 
Q 0-8192    T -8192  -8192
Q 856-412   T 444    44   
Q 7663+7    T 7670   7670 
Q 4796+8    T 4804   4804 
Q 88-7643   T -7555  -7555
Q 4-722     T -718   -718 
Q 89+687    T 776    77   
Q 1+577     T 578    578  
Q 437+121   T 558    458  

--------------------------------------------------
Iteration 84
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9224 - loss: 0.2128
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.9224 - loss: 0.2129 - val_accuracy: 0.8350 - val_loss: 0.5159
Q 302+1167  T 1469   1489 
Q 6160+1    T 6161   6161 
Q 37-81     T -44    -44  
Q 6-565     T -559   -569 
Q 96+259    T 355    355  
Q 73-6477   T -6404  -640 
Q 130-9247  T -9117  -9107
Q 9-7693    T -7684  -7684
Q 566-3959  T -3393  -3393
Q 1879+0    T 1879   1879 

--------------------------------------------------
Iteration 85
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.9176 - loss: 0.2289
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9176 - loss: 0.2289 - val_accuracy: 0.8343 - val_loss: 0.5112
Q 4762-39   T 4723   4723 
Q 2994+8400 T 11394  11903
Q 960-7734  T -6774  -677 
Q 604+758   T 1362   1442 
Q 423+8140  T 8563   8583 
Q 76+542    T 618    618  
Q 319+7     T 326    326  
Q 202+152   T 354    373  
Q 315-8     T 307    307  
Q 904-1501  T -597   -59  

--------------------------------------------------
Iteration 86
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9206 - loss: 0.2146
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9206 - loss: 0.2146 - val_accuracy: 0.8470 - val_loss: 0.4771
Q 277+3     T 280    280  
Q 0-1090    T -1090  -1090
Q 6-10      T -4     -    
Q 4-6878    T -6874  -6874
Q 2+860     T 862    862  
Q 9645+9979 T 19624  10774
Q 9616+70   T 9686   9677 
Q 7087-654  T 6433   6423 
Q 37-7869   T -7832  -7832
Q 91+8133   T 8224   8224 

--------------------------------------------------
Iteration 87
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.9242 - loss: 0.2062
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 14ms/step - accuracy: 0.9241 - loss: 0.2063 - val_accuracy: 0.8456 - val_loss: 0.4622
Q 3413+4302 T 7715   770  
Q 5+1200    T 1205   1205 
Q 9+7107    T 7116   7116 
Q 22+2702   T 2724   2724 
Q 6188+672  T 6860   6860 
Q 6-895     T -889   -889 
Q 9412+986  T 10398  10089
Q 1009+9    T 1018   1019 
Q 45-4      T 41     41   
Q 5+207     T 212    212  

--------------------------------------------------
Iteration 88
<span class="ansi-bold">276/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9207 - loss: 0.2169
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9207 - loss: 0.2168 - val_accuracy: 0.8460 - val_loss: 0.4682
Q 92-75     T 17     9    
Q 5555-325  T 5230   5220 
Q 8-2462    T -2454  -2454
Q 245-728   T -483   -583 
Q 77-75     T 2      7    
Q 7918-170  T 7748   7749 
Q 628+723   T 1351   1351 
Q 180-1     T 179    179  
Q 476+454   T 930    900  
Q 69+625    T 694    694  

--------------------------------------------------
Iteration 89
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.9268 - loss: 0.2011
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9268 - loss: 0.2011 - val_accuracy: 0.8511 - val_loss: 0.4494
Q 259+828   T 1087   108  
Q 228+385   T 613    513  
Q 2036+3    T 2039   2039 
Q 32-6      T 26     26   
Q 8+880     T 888    888  
Q 2-784     T -782   -782 
Q 577+12    T 589    599  
Q 8465+78   T 8543   8543 
Q 3129-44   T 3085   3085 
Q 3+5958    T 5961   5961 

--------------------------------------------------
Iteration 90
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 13ms/step - accuracy: 0.9272 - loss: 0.1989
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 14ms/step - accuracy: 0.9272 - loss: 0.1989 - val_accuracy: 0.8438 - val_loss: 0.4935
Q 28-995    T -967   -967 
Q 692+4     T 696    696  
Q 419-452   T -33    -33  
Q 70+7      T 77     77   
Q 433-3     T 430    420  
Q 204+13    T 217    227  
Q 494+43    T 537    537  
Q 4552+6    T 4558   4558 
Q 3614-74   T 3540   3530 
Q 78-3538   T -3460  -3450

--------------------------------------------------
Iteration 91
<span class="ansi-bold">280/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9229 - loss: 0.2130
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9229 - loss: 0.2129 - val_accuracy: 0.8566 - val_loss: 0.4266
Q 392-6878  T -6486  -6287
Q 125+596   T 721    711  
Q 0+5127    T 5127   5127 
Q 366+72    T 438    448  
Q 897-58    T 839    839  
Q 2+996     T 998    998  
Q 98+551    T 649    659  
Q 8864-25   T 8839   8839 
Q 725-360   T 365    475  
Q 156+402   T 558    568  

--------------------------------------------------
Iteration 92
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.9304 - loss: 0.1898
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.9304 - loss: 0.1898 - val_accuracy: 0.8553 - val_loss: 0.4461
Q 8-1784    T -1776  -1776
Q 71+938    T 1009   1009 
Q 391-9239  T -8848  -8758
Q 1596-3    T 1593   1693 
Q 43+7968   T 8011   8022 
Q 477-9     T 468    468  
Q 4413+5    T 4418   4409 
Q 853-7     T 846    846  
Q 78-7976   T -7898  -7898
Q 38-9      T 29     39   

--------------------------------------------------
Iteration 93
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.9275 - loss: 0.1985
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 13ms/step - accuracy: 0.9275 - loss: 0.1985 - val_accuracy: 0.8497 - val_loss: 0.4844
Q 22+2702   T 2724   272  
Q 696+1     T 697    697  
Q 5009-970  T 4039   4089 
Q 2-700     T -698   -798 
Q 492+0     T 492    492  
Q 73-8679   T -8606  -8686
Q 6071-1068 T 5003   4990 
Q 1-521     T -520   -520 
Q 559-722   T -163   -262 
Q 1079-692  T 387    377  

--------------------------------------------------
Iteration 94
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.9316 - loss: 0.1885
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9315 - loss: 0.1886 - val_accuracy: 0.8512 - val_loss: 0.4452
Q 958-3518  T -2560  -1560
Q 19+5490   T 5509   5509 
Q 7443+0    T 7443   7443 
Q 854-81    T 773    783  
Q 1+577     T 578    578  
Q 7380-522  T 6858   6888 
Q 257+3436  T 3693   379  
Q 8207-87   T 8120   8120 
Q 1085+8    T 1093   1093 
Q 17-108    T -91    -90  

--------------------------------------------------
Iteration 95
<span class="ansi-bold">278/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9314 - loss: 0.1892
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.9314 - loss: 0.1893 - val_accuracy: 0.8505 - val_loss: 0.4556
Q 784-1     T 783    783  
Q 590-6642  T -6052  -6962
Q 738+9     T 747    747  
Q 4047-519  T 3528   358  
Q 84-6052   T -5968  -5978
Q 302-4655  T -4353  -4453
Q 99-66     T 33     33   
Q 417+2     T 419    419  
Q 299+11    T 310    310  
Q 6+1397    T 1403   1403 

--------------------------------------------------
Iteration 96
<span class="ansi-bold">281/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 11ms/step - accuracy: 0.9296 - loss: 0.1926
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 12ms/step - accuracy: 0.9296 - loss: 0.1926 - val_accuracy: 0.8529 - val_loss: 0.4817
Q 20-73     T -53    -53  
Q 86-2      T 84     84   
Q 560+58    T 618    618  
Q 188+96    T 284    284  
Q 2608-806  T 1802   280  
Q 1048-2523 T -1475  -105 
Q 5-7944    T -7939  -7939
Q 2824-399  T 2425   2525 
Q 1622+53   T 1675   1666 
Q 84+8      T 92     92   

--------------------------------------------------
Iteration 97
<span class="ansi-bold">277/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 9ms/step - accuracy: 0.9328 - loss: 0.1849
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 10ms/step - accuracy: 0.9327 - loss: 0.1850 - val_accuracy: 0.8510 - val_loss: 0.4827
Q 85+0      T 85     85   
Q 5+406     T 411    411  
Q 856-412   T 444    44   
Q 2+599     T 601    601  
Q 473-8767  T -8294  -8204
Q 5658-6295 T -637   41   
Q 665+3     T 668    668  
Q 7046+3803 T 10849  10070
Q 894-5     T 889    889  
Q 3-8071    T -8068  -8068

--------------------------------------------------
Iteration 98
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 8ms/step - accuracy: 0.9274 - loss: 0.1993
Epoch 1: val_loss did not improve from 0.42049
<span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 9ms/step - accuracy: 0.9274 - loss: 0.1993 - val_accuracy: 0.8561 - val_loss: 0.4575
Q 45+962    T 1007   908  
Q 15-0      T 15     15   
Q 94+94     T 188    188  
Q 6500-1798 T 4702   450  
Q 4525-7    T 4518   4408 
Q 6087+8    T 6095   6095 
Q 3-940     T -937   -937 
Q 6160+1    T 6161   6161 
Q 8+3883    T 3891   3891 
Q 892+6     T 898    898  

--------------------------------------------------
Iteration 99
<span class="ansi-bold">279/282</span> <span class="ansi-green-fg"></span><span class="ansi-white-fg"></span> <span class="ansi-bold">0s</span> 12ms/step - accuracy: 0.9328 - loss: 0.1843
Epoch 1: val_loss improved from 0.42049 to 0.40294, saving model to best_model_3.h5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">282/282</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 14ms/step - accuracy: 0.9327 - loss: 0.1844 - val_accuracy: 0.8635 - val_loss: 0.4029
Q 917+6894  T 7811   7601 
Q 610+642   T 1252   1342 
Q 4-1486    T -1482  -1482
Q 2-61      T -59    -59  
Q 4-293     T -289   -289 
Q 58+17     T 75     76   
Q 24+4088   T 4112   4012 
Q 6607-36   T 6571   6570 
Q 6617+6    T 6623   6623 
Q 20-97     T -77    -78  
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>The improved Model 3 shows significant improvements over Model 2 (including the attention mechanism) in terms of both training and validation performance. Specifically:</p>
<p>Training Accuracy:</p>
<ul>
<li>Model 3 achieves around 92%, compared to Model 2s 90%.</li>
</ul>
<p>Validation Accuracy:</p>
<ul>
<li>Model 3 reaches 86%, which is a 5% improvement over Model 2s 81%.</li>
</ul>
<p>This improvement highlights that Model 3 generalizes better to unseen data, likely due to the structural enhancements made to the model.</p>
<p>Key Changes in Model 3:</p>
<ul>
<li><p>GRU Layers: Switched from LSTM to GRU for more efficient learning with fewer parameters, capturing temporal dependencies effectively.</p>
</li>
<li><p>Convolutional Layer (Conv1D): Added to extract local features, improving the models ability to recognize important patterns in the sequence.</p>
</li>
<li><p>Global Max Pooling: Helps retain the most significant features from the attention output, aiding generalization and reducing dimensionality.</p>
</li>
<li><p>Dropout: Prevents overfitting, improving the model's ability to generalize to unseen data.</p>
</li>
<li><p>Iterations: changed max iterations to 100 to enable the model to fully converege</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<hr/>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Part-2:-A-language-translation-model-with-attention">Part 2: A language translation model with attention<a class="anchor-link" href="#Part-2:-A-language-translation-model-with-attention"></a></h2>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>In this part of the problem set we are going to implement a translation with a Sequence to Sequence Network and Attention model.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ol start="0">
<li>Please go over the NLP From Scratch: Translation with a Sequence to Sequence Network and Attention <a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">tutorial</a>. This attention model is very similar to what was learned in class (Luong), but a bit different. What are the main differences between  Badahnau and Luong attention mechanisms?</li>
</ol>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.a) Using <code>!wget</code>, <code>!unzip</code> , download and extract the <a href="https://www.manythings.org/anki/">hebrew-english</a> sentence pairs text file to the Colab <code>content/</code>  folder (or local folder if not using Colab).
1.b) The <code>heb.txt</code> must be parsed and cleaned (see tutorial for requirements or change the code as you see fit).</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>2.a) Use the tutorial example to build  and train a Hebrew to English translation model with attention (using the parameters in the code cell below). Apply the same <code>eng_prefixes</code> filter to limit the train/test data.<br/>
2.b) Evaluate your trained model randomly on 20 sentences.<br/>
2.c) Show the attention plot for 5 random sentences.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ol start="3">
<li>Do you think this model performs well? Why or why not? What are its limitations/disadvantages? What would you do to improve it?</li>
</ol>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ol start="4">
<li>Using any neural network architecture of your liking, build  a model with the aim to beat the model in 2.a. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better.</li>
</ol>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ol start="0">
<li>The main differences between Bahdanau and Luong attention mechanisms lie in how they calculate attention scores. Bahdanau attention (also known as additive attention) uses a feed-forward neural network to compute a compatibility function between the decoder's hidden state and encoder states. In contrast, Luong attention (multiplicative attention) directly calculates the attention score by performing a dot product between the decoder's hidden state and encoder states. Luong attention generally performs faster due to its simpler computation and is often considered more efficient in practice.</li>
</ol>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>1.a) - I have manually downloaded the relevant txt file and uploaded it to my drive.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">SOS_token</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">EOS_token</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">class</span> <span class="nc">Lang</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">"SOS"</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">"EOS"</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_words</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Count SOS and EOS</span>

    <span class="k">def</span> <span class="nf">addSentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">addWord</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_words</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">n_words</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_words</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>


<span class="c1"># Turn a Unicode string to plain ASCII, thanks to</span>
<span class="c1"># https://stackoverflow.com/a/518232/2809427</span>
<span class="k">def</span> <span class="nf">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">'NFD'</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s1">'Mn'</span>
    <span class="p">)</span>

<span class="c1"># Lowercase, trim, and remove non-letter characters</span>
<span class="k">def</span> <span class="nf">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"([.!?])"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">" \1"</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="c1">#s = re.sub(r"[^a-zA-Z!?]+", r" ", s)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"[^a-zA-Z\u0590-\u05FF!?]+"</span><span class="p">,</span> <span class="sa">r</span><span class="s2">" "</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>  <span class="c1"># Keep Hebrew characters</span>
    <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">readLangs</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Reading lines..."</span><span class="p">)</span>

    <span class="c1"># Read the file and split into lines</span>
    <span class="c1">#lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'/content/drive/MyDrive/ps3/heb.txt'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'utf-8'</span><span class="p">)</span><span class="o">.</span>\
        <span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

    <span class="c1"># Split every line into pairs and normalize</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[[</span><span class="n">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">)]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>

    <span class="c1"># Reverse pairs, make Lang instances</span>
    <span class="k">if</span> <span class="n">reverse</span><span class="p">:</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">]</span>
        <span class="n">input_lang</span> <span class="o">=</span> <span class="n">Lang</span><span class="p">(</span><span class="n">lang2</span><span class="p">)</span>
        <span class="n">output_lang</span> <span class="o">=</span> <span class="n">Lang</span><span class="p">(</span><span class="n">lang1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">input_lang</span> <span class="o">=</span> <span class="n">Lang</span><span class="p">(</span><span class="n">lang1</span><span class="p">)</span>
        <span class="n">output_lang</span> <span class="o">=</span> <span class="n">Lang</span><span class="p">(</span><span class="n">lang2</span><span class="p">)</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span>

<span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">eng_prefixes</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">"i am "</span><span class="p">,</span> <span class="s2">"i m "</span><span class="p">,</span>
    <span class="s2">"he is"</span><span class="p">,</span> <span class="s2">"he s "</span><span class="p">,</span>
    <span class="s2">"she is"</span><span class="p">,</span> <span class="s2">"she s "</span><span class="p">,</span>
    <span class="s2">"you are"</span><span class="p">,</span> <span class="s2">"you re "</span><span class="p">,</span>
    <span class="s2">"we are"</span><span class="p">,</span> <span class="s2">"we re "</span><span class="p">,</span>
    <span class="s2">"they are"</span><span class="p">,</span> <span class="s2">"they re "</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">filterPair</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="k">if</span> <span class="s1">'cc by france attribution tatoeba org'</span> <span class="ow">in</span> <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="s1">'cc by france attribution tatoeba org'</span> <span class="ow">in</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span> <span class="ow">and</span> \
        <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span><span class="c1"># and \</span>
        <span class="c1">#p[1].startswith(eng_prefixes)</span>


<span class="k">def</span> <span class="nf">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span> <span class="k">if</span> <span class="n">filterPair</span><span class="p">(</span><span class="n">pair</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">prepareData</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">readLangs</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Read </span><span class="si">%s</span><span class="s2"> sentence pairs"</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">))</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="n">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Trimmed to </span><span class="si">%s</span><span class="s2"> sentence pairs"</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Counting words..."</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">input_lang</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">output_lang</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Counted words:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_lang</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output_lang</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">output_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span>

<span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">prepareData</span><span class="p">(</span><span class="s1">'eng'</span><span class="p">,</span> <span class="s1">'heb'</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading lines...
Read 128133 sentence pairs
Trimmed to 32968 sentence pairs
Counting words...
Counted words:
heb 14485
eng 6808
[' ', 'this stinks']
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">EncoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>

<span class="k">class</span> <span class="nc">DecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">target_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">SOS_token</span><span class="p">)</span>
        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_step</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">)</span>
            <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">target_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Teacher forcing: Feed the target as the next input</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target_tensor</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Teacher forcing</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Without teacher forcing: use its own predictions as the next input</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># detach from history as input</span>

        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="kc">None</span> <span class="c1"># We return `None` for consistency in the training loop</span>

    <span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>

<span class="k">class</span> <span class="nc">BahdanauAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BahdanauAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Wa</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ua</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Va</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">keys</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Va</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Wa</span><span class="p">(</span><span class="n">query</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ua</span><span class="p">(</span><span class="n">keys</span><span class="p">)))</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">keys</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="n">weights</span>

<span class="k">class</span> <span class="nc">AttnDecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttnDecoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">BahdanauAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">target_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">SOS_token</span><span class="p">)</span>
        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">attentions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_step</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">)</span>
            <span class="n">attentions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">target_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Teacher forcing: Feed the target as the next input</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target_tensor</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Teacher forcing</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Without teacher forcing: use its own predictions as the next input</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># detach from history as input</span>

        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attentions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">attentions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">attentions</span>


    <span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">hidden</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">context</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="n">input_gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">embedded</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">input_gru</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">attn_weights</span>

<span class="k">def</span> <span class="nf">indexesFromSentence</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">lang</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">tensorFromSentence</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="n">indexes</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
    <span class="n">indexes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">indexes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tensorsFromPair</span><span class="p">(</span><span class="n">pair</span><span class="p">):</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">target_tensor</span> <span class="o">=</span> <span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">output_lang</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">prepareData</span><span class="p">(</span><span class="s1">'eng'</span><span class="p">,</span> <span class="s1">'heb'</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">target_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pairs</span><span class="p">):</span>
        <span class="n">inp_ids</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span> <span class="n">inp</span><span class="p">)</span>
        <span class="n">tgt_ids</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">output_lang</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span>
        <span class="n">inp_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
        <span class="n">tgt_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">inp_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">inp_ids</span>
        <span class="n">target_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tgt_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">tgt_ids</span>

    <span class="n">train_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                               <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">train_dataloader</span>

<span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span>
          <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span> <span class="o">=</span> <span class="n">data</span>

        <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
        <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span>
            <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
            <span class="n">target_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">asMinutes</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">s</span> <span class="o">/</span> <span class="mi">60</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">-=</span> <span class="n">m</span> <span class="o">*</span> <span class="mi">60</span>
    <span class="k">return</span> <span class="s1">'</span><span class="si">%d</span><span class="s1">m </span><span class="si">%d</span><span class="s1">s'</span> <span class="o">%</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">timeSince</span><span class="p">(</span><span class="n">since</span><span class="p">,</span> <span class="n">percent</span><span class="p">):</span>
    <span class="n">now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">now</span> <span class="o">-</span> <span class="n">since</span>
    <span class="n">es</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="p">(</span><span class="n">percent</span><span class="p">)</span>
    <span class="n">rs</span> <span class="o">=</span> <span class="n">es</span> <span class="o">-</span> <span class="n">s</span>
    <span class="k">return</span> <span class="s1">'</span><span class="si">%s</span><span class="s1"> (- </span><span class="si">%s</span><span class="s1">)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">asMinutes</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">asMinutes</span><span class="p">(</span><span class="n">rs</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
               <span class="n">print_every</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">plot_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">print_loss_total</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Reset every print_every</span>
    <span class="n">plot_loss_total</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Reset every plot_every</span>

    <span class="n">encoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">decoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_epoch</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
        <span class="n">print_loss_total</span> <span class="o">+=</span> <span class="n">loss</span>
        <span class="n">plot_loss_total</span> <span class="o">+=</span> <span class="n">loss</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">print_loss_avg</span> <span class="o">=</span> <span class="n">print_loss_total</span> <span class="o">/</span> <span class="n">print_every</span>
            <span class="n">print_loss_total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="si">%s</span><span class="s1"> (</span><span class="si">%d</span><span class="s1"> </span><span class="si">%d%%</span><span class="s1">) </span><span class="si">%.4f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">timeSince</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">/</span> <span class="n">n_epochs</span><span class="p">),</span>
                                        <span class="n">epoch</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">/</span> <span class="n">n_epochs</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">print_loss_avg</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">plot_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">plot_loss_avg</span> <span class="o">=</span> <span class="n">plot_loss_total</span> <span class="o">/</span> <span class="n">plot_every</span>
            <span class="n">plot_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">plot_loss_avg</span><span class="p">)</span>
            <span class="n">plot_loss_total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">showPlot</span><span class="p">(</span><span class="n">plot_losses</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">switch_backend</span><span class="p">(</span><span class="s1">'agg'</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.ticker</span> <span class="k">as</span> <span class="nn">ticker</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">showPlot</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="c1"># this locator puts ticks at regular intervals</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="n">base</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">loc</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>

        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
        <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">decoder_attn</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">decoded_ids</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="n">decoded_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">decoded_ids</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">idx</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">EOS_token</span><span class="p">:</span>
                <span class="n">decoded_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">'&lt;EOS&gt;'</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="n">decoded_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_lang</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">idx</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
    <span class="k">return</span> <span class="n">decoded_words</span><span class="p">,</span> <span class="n">decoder_attn</span>

<span class="k">def</span> <span class="nf">evaluateRandomly</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">pair</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'&gt;'</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'='</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">output_words</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">)</span>
        <span class="n">output_sentence</span> <span class="o">=</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'&lt;'</span><span class="p">,</span> <span class="n">output_sentence</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderRNN</span><span class="p">(</span><span class="n">input_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">AttnDecoderRNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">train</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading lines...
Read 128133 sentence pairs
Trimmed to 113428 sentence pairs
Counting words...
Counted words:
heb 34801
eng 12303
7m 16s (- 65m 32s) (5 10%) 1.6488
14m 24s (- 57m 38s) (10 20%) 0.8945
21m 34s (- 50m 20s) (15 30%) 0.6539
28m 46s (- 43m 9s) (20 40%) 0.5271
35m 52s (- 35m 52s) (25 50%) 0.4460
43m 5s (- 28m 43s) (30 60%) 0.3902
50m 18s (- 21m 33s) (35 70%) 0.3496
57m 28s (- 14m 22s) (40 80%) 0.3183
64m 35s (- 7m 10s) (45 90%) 0.2933
71m 37s (- 0m 0s) (50 100%) 0.2733
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderRNN</span><span class="p">(</span><span class="n">input_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">AttnDecoderRNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">train</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading lines...
Read 128133 sentence pairs
Trimmed to 32968 sentence pairs
Counting words...
Counted words:
heb 14485
eng 6808
9m 9s (- 82m 23s) (5 10%) 2.3773
18m 34s (- 74m 16s) (10 20%) 1.0765
28m 1s (- 65m 22s) (15 30%) 0.5732
37m 30s (- 56m 15s) (20 40%) 0.3475
46m 52s (- 46m 52s) (25 50%) 0.2426
56m 11s (- 37m 27s) (30 60%) 0.1877
65m 29s (- 28m 3s) (35 70%) 0.1569
74m 48s (- 18m 42s) (40 80%) 0.1375
83m 59s (- 9m 19s) (45 90%) 0.1246
93m 10s (- 0m 0s) (50 100%) 0.1163
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">evaluateRandomly</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>&gt;   
= they were thin
&lt; they were thin &lt;EOS&gt;

&gt;  
= there were problems
&lt; there were problems solvers &lt;EOS&gt;

&gt;  
= that s convenient
&lt; that s uncomfortable &lt;EOS&gt;

&gt;    ?
= are you famous ?
&lt; are you famous ? &lt;EOS&gt;

&gt;   
= he has a car
&lt; he has a car started

&gt;  
= tom is an extrovert
&lt; tom may extroverted &lt;EOS&gt;

&gt;  
= we were wasted
&lt; we were completely exhausted &lt;EOS&gt;

&gt;   
= take that away
&lt; take that away &lt;EOS&gt;

&gt;    
= i like watching people
&lt; i like watching people &lt;EOS&gt;

&gt;   
= tom set a trap
&lt; tom set is without &lt;EOS&gt;

&gt;    
= i like the competition
&lt; i like the competition &lt;EOS&gt;

&gt;   
= he speaks arabic
&lt; he speaks arabic &lt;EOS&gt;

&gt;  
= i missed you
&lt; i missed you missed you

&gt;   
= i forgave his mistake
&lt; i forgave his mistake &lt;EOS&gt;

&gt;  
= don t be afraid
&lt; don t be afraid &lt;EOS&gt;

&gt;    
= tom blamed himself
&lt; tom blamed himself &lt;EOS&gt;

&gt;   
= operations are already underway
&lt; operations are already underway &lt;EOS&gt;

&gt;    
= they re my cousins
&lt; they re my cousins &lt;EOS&gt;

&gt;   
= let tom answer
&lt; let tom answer in &lt;EOS&gt;

&gt;  
= my arms are tired
&lt; my arms are tired &lt;EOS&gt;

</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[31]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.ticker</span> <span class="k">as</span> <span class="nn">ticker</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="c1"># Make sure Matplotlib renders RTL text correctly</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'axes.unicode_minus'</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># To avoid issues with special characters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'font.family'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'sans-serif'</span>  <span class="c1"># Use the default font family</span>
<span class="k">def</span> <span class="nf">showAttention</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">output_words</span><span class="p">,</span> <span class="n">attentions</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attentions</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'bone'</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cax</span><span class="p">)</span>

    <span class="c1"># Set up axes</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">input_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="s1">'&lt;EOS&gt;'</span><span class="p">]</span>
    <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">output_words</span>

    <span class="c1"># Reverse the input_tokens and output_tokens to fix the RTL problem</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)))</span>  <span class="c1"># Set x-tick positions</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'right'</span><span class="p">)</span>  <span class="c1"># Reverse order for Hebrew</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">)))</span>  <span class="c1"># Set y-tick positions</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'right'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>  <span class="c1"># Align Hebrew text to the right</span>

    <span class="c1"># Show label at every tick</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  <span class="c1"># Explicitly show the plot in Colab</span>



<span class="k">def</span> <span class="nf">evaluateAndShowAttention</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">):</span>
    <span class="n">output_words</span><span class="p">,</span> <span class="n">attentions</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">,</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'input ='</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'output ='</span><span class="p">,</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">))</span>
    <span class="n">showAttention</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">output_words</span><span class="p">,</span> <span class="n">attentions</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">output_words</span><span class="p">),</span> <span class="p">:])</span>


<span class="n">evaluateAndShowAttention</span><span class="p">(</span><span class="s1">'    ?'</span><span class="p">)</span>

<span class="n">evaluateAndShowAttention</span><span class="p">(</span><span class="s1">'   '</span><span class="p">)</span>

<span class="n">evaluateAndShowAttention</span><span class="p">(</span><span class="s1">' '</span><span class="p">)</span>

<span class="n">evaluateAndShowAttention</span><span class="p">(</span><span class="s1">' '</span><span class="p">)</span>

<span class="n">evaluateAndShowAttention</span><span class="p">(</span><span class="s1">' '</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>input =     ?
output = do you want ? &lt;EOS&gt;
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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
"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>input =    
output = please wash the dishes &lt;EOS&gt;
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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
"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>input =  
output = i missed you snoring &lt;EOS&gt;
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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
"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>input =  
output = you were charming &lt;EOS&gt;
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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
"/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>input =  
output = my arms are tired &lt;EOS&gt;
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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
"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">evaluateRandomly</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>&gt;    !
= don t change a thing
&lt; stop not something alike &lt;EOS&gt;

&gt;    
= i m not a big tv watcher
&lt; i don t expect big heavy television television &lt;EOS&gt;

&gt;     ?
= do you want any of this stuff ?
&lt; do you want anything about this ? &lt;EOS&gt;

&gt;    
= boxers need quick reflexes
&lt; few eggs courageous and hide quickly &lt;EOS&gt;

&gt;    
= please wash the dishes
&lt; please wash the dishes ? &lt;EOS&gt;

&gt;    
= we climbed up the steep mountain
&lt; we climbed up the steep mountain up steep steep lost

&gt;  
= you were charming
&lt; you were good attractive &lt;EOS&gt;

&gt;     ?
= have you finished the work yet ?
&lt; have you finished the work yet ? &lt;EOS&gt;

&gt;     ?
= is it japanese food ?
&lt; is fish japanese fish ? &lt;EOS&gt;

&gt;    ?
= who really knows ?
&lt; who knows really really ? &lt;EOS&gt;

</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ol start="3">
<li>The original model performs reasonably well for simpler tasks but has some limitations.</li>
</ol>
<p>Strengths:</p>
<ul>
<li>Efficiency: Simple architecture with a unidirectional GRU, making it computationally efficient for small datasets or shorter sentences.</li>
<li>Simplicity: Easy to train without complex components like attention.</li>
</ul>
<p>Limitations:</p>
<ul>
<li>Unidirectional GRU: Limits the model's ability to capture context from both directions, reducing its performance on ambiguous or long sentences.</li>
<li>No Regularization: Without batch normalization or dropout, the model may overfit or struggle with stability in training.</li>
</ul>
<p>Suggestions for Improvement:</p>
<ul>
<li>Use Bidirectional Encoder: Capture context from both directions to improve understanding.</li>
<li>Introduce Regularization: Add dropout or batch normalization to prevent overfitting and improve stability.</li>
</ul>
<p>Overall, the model works well for simple tasks but needs improvements like attention and bidirectional encoding for better performance on complex sequences.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>4 - defining the model:</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[43]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">EncoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

        <span class="c1"># Combine embedding and dropout into one layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># Use bidirectional GRU to capture context from both directions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="n">embedded</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Ensure the shape is (batch_size, seq_len, hidden_size)</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Combine bidirectional hidden states</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>


<span class="k">class</span> <span class="nc">BahdanauAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Combine all linear transformations into one for efficiency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">keys</span><span class="p">):</span>
        <span class="c1"># Expand query to match keys dimension</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keys</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Concatenate instead of separate linear layers</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">query</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">query</span> <span class="o">*</span> <span class="n">keys</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">keys</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="n">weights</span>

<span class="k">class</span> <span class="nc">AttnDecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>

        <span class="c1"># Combine embedding and dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">BahdanauAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="c1"># Use a single layer GRU with larger hidden size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Add batch normalization for stable training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">target_tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">SOS_token</span><span class="p">)</span>
        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">attentions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Pre-allocate tensors for efficiency</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">MAX_LENGTH</span> <span class="k">if</span> <span class="n">target_tensor</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">target_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">all_decoder_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_step</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="n">all_decoder_outputs</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoder_output</span>

            <span class="k">if</span> <span class="n">target_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target_tensor</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

            <span class="n">attentions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="c1"># Apply log_softmax once at the end instead of every step</span>
        <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">all_decoder_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attentions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">attentions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">attentions</span>

    <span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

        <span class="c1"># More efficient attention computation</span>
        <span class="n">context</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">encoder_outputs</span><span class="p">)</span>

        <span class="n">gru_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">embedded</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">gru_input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">attn_weights</span>



<span class="k">def</span> <span class="nf">indexesFromSentence</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">lang</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">tensorFromSentence</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="n">indexes</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
    <span class="n">indexes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">indexes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tensorsFromPair</span><span class="p">(</span><span class="n">pair</span><span class="p">):</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">target_tensor</span> <span class="o">=</span> <span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">output_lang</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">prepareData</span><span class="p">(</span><span class="s1">'eng'</span><span class="p">,</span> <span class="s1">'heb'</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">target_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">MAX_LENGTH</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pairs</span><span class="p">):</span>
        <span class="n">inp_ids</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span> <span class="n">inp</span><span class="p">)</span>
        <span class="n">tgt_ids</span> <span class="o">=</span> <span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">output_lang</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span>
        <span class="n">inp_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
        <span class="n">tgt_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">inp_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">inp_ids</span>
        <span class="n">target_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">tgt_ids</span><span class="p">)]</span> <span class="o">=</span> <span class="n">tgt_ids</span>

    <span class="n">train_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                               <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">target_ids</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">train_dataloader</span>

<span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span>
          <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span> <span class="o">=</span> <span class="n">data</span>

        <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
        <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span>
            <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
            <span class="n">target_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">asMinutes</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">s</span> <span class="o">/</span> <span class="mi">60</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">-=</span> <span class="n">m</span> <span class="o">*</span> <span class="mi">60</span>
    <span class="k">return</span> <span class="s1">'</span><span class="si">%d</span><span class="s1">m </span><span class="si">%d</span><span class="s1">s'</span> <span class="o">%</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">timeSince</span><span class="p">(</span><span class="n">since</span><span class="p">,</span> <span class="n">percent</span><span class="p">):</span>
    <span class="n">now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">now</span> <span class="o">-</span> <span class="n">since</span>
    <span class="n">es</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="p">(</span><span class="n">percent</span><span class="p">)</span>
    <span class="n">rs</span> <span class="o">=</span> <span class="n">es</span> <span class="o">-</span> <span class="n">s</span>
    <span class="k">return</span> <span class="s1">'</span><span class="si">%s</span><span class="s1"> (- </span><span class="si">%s</span><span class="s1">)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">asMinutes</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">asMinutes</span><span class="p">(</span><span class="n">rs</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
               <span class="n">print_every</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">plot_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">print_loss_total</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Reset every print_every</span>
    <span class="n">plot_loss_total</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Reset every plot_every</span>

    <span class="n">encoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">decoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_epoch</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
        <span class="n">print_loss_total</span> <span class="o">+=</span> <span class="n">loss</span>
        <span class="n">plot_loss_total</span> <span class="o">+=</span> <span class="n">loss</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">print_loss_avg</span> <span class="o">=</span> <span class="n">print_loss_total</span> <span class="o">/</span> <span class="n">print_every</span>
            <span class="n">print_loss_total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="si">%s</span><span class="s1"> (</span><span class="si">%d</span><span class="s1"> </span><span class="si">%d%%</span><span class="s1">) </span><span class="si">%.4f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">timeSince</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">/</span> <span class="n">n_epochs</span><span class="p">),</span>
                                        <span class="n">epoch</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">/</span> <span class="n">n_epochs</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">print_loss_avg</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">plot_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">plot_loss_avg</span> <span class="o">=</span> <span class="n">plot_loss_total</span> <span class="o">/</span> <span class="n">plot_every</span>
            <span class="n">plot_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">plot_loss_avg</span><span class="p">)</span>
            <span class="n">plot_loss_total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">showPlot</span><span class="p">(</span><span class="n">plot_losses</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">switch_backend</span><span class="p">(</span><span class="s1">'agg'</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.ticker</span> <span class="k">as</span> <span class="nn">ticker</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">showPlot</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="c1"># this locator puts ticks at regular intervals</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="n">base</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">loc</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>

        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
        <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">decoder_attn</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_outputs</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">decoded_ids</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="n">decoded_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">decoded_ids</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">idx</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">EOS_token</span><span class="p">:</span>
                <span class="n">decoded_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">'&lt;EOS&gt;'</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="n">decoded_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_lang</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">idx</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
    <span class="k">return</span> <span class="n">decoded_words</span><span class="p">,</span> <span class="n">decoder_attn</span>

<span class="k">def</span> <span class="nf">evaluateRandomly</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">pair</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'&gt;'</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'='</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">output_words</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">)</span>
        <span class="n">output_sentence</span> <span class="o">=</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'&lt;'</span><span class="p">,</span> <span class="n">output_sentence</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>training the model:</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[46]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">get_dataloader</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderRNN</span><span class="p">(</span><span class="n">input_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">AttnDecoderRNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_lang</span><span class="o">.</span><span class="n">n_words</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">train</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading lines... 
Read 128133 sentence pairs 
Trimmed to 32968 sentence pairs 
Counting words... 
Counted words: 
heb 34801 
eng 12303 
1m 14s (- 11m 10s) (5 10%) 1.9520 
2m 28s (- 9m 52s) (10 20%) 0.6128 
3m 47s (- 8m 50s) (15 30%) 0.3663 
5m 1s (- 7m 32s) (20 40%) 0.2766 
6m 15s (- 6m 15s) (25 50%) 0.2301 
7m 28s (- 4m 59s) (30 60%) 0.2024 
8m 42s (- 3m 44s) (35 70%) 0.1821 
9m 55s (- 2m 28s) (40 80%) 0.1686 
11m 9s (- 1m 14s) (45 90%) 0.1570 
12m 24s (- 0m 0s) (50 100%) 0.1479
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[42]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">evaluateRandomly</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>&gt;    
= i have some food
&lt; i have some food &lt;EOS&gt;

&gt;   
= dublin is in ireland
&lt; dublin is in ireland &lt;EOS&gt;

&gt;   
= he suddenly stopped
&lt; he suddenly stopped suddenly &lt;EOS&gt;

&gt;  
= trust in me
&lt; trust in me &lt;EOS&gt;

&gt;  
= i slept soundly
&lt; i slept soundly &lt;EOS&gt;

&gt;   
= he is still standing
&lt; he s still standing &lt;EOS&gt;

&gt;  
= i ll tell everyone
&lt; i like tom impressed &lt;EOS&gt;

&gt; 
= i volunteered
&lt; i washed my mind &lt;EOS&gt;

&gt;    
= tom danced with mary
&lt; tom danced with tom &lt;EOS&gt;

&gt;    
= i have your diary
&lt; i have your diary &lt;EOS&gt;

&gt;   
= it smells wonderful
&lt; it smells wonderful &lt;EOS&gt;

&gt;   
= we should help
&lt; we need help &lt;EOS&gt;

&gt;  
= i noticed
&lt; i noticed that &lt;EOS&gt;

&gt;   
= you go first
&lt; you go first &lt;EOS&gt;

&gt;  
= i remain puzzled
&lt; i remain puzzled &lt;EOS&gt;

&gt;   
= the kitten wanted in
&lt; the kitten wanted me &lt;EOS&gt;

&gt;   
= he will excuse me
&lt; he will excuse me &lt;EOS&gt;

&gt;  
= happy holidays
&lt; happy kitten &lt;EOS&gt;

&gt;    
= stop talking loudly
&lt; stop talking loudly &lt;EOS&gt;

&gt;   
= tom was cool
&lt; tom was cool cool &lt;EOS&gt;

</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>I made several changes in my model compared to the original one:</p>
<ul>
<li><p>Bidirectional GRU in Encoder: In the original model, the encoder uses a unidirectional GRU. I switched to a bidirectional GRU in the encoder, which allows the model to process the input sequence in both directions, capturing more context and improving the models understanding of the input.</p>
</li>
<li><p>Bahdanau Attention: The original model does not incorporate any attention mechanism. I added the Bahdanau attention mechanism in the decoder, which enables the model to focus on different parts of the encoder's output while decoding, improving the model's ability to handle longer sequences and provide more contextually accurate translations.</p>
</li>
<li><p>Combined Embedding and Dropout Layer: In the original model, embedding and dropout are handled separately. I combined them into a single embedding_dropout layer for a more efficient and compact implementation.</p>
</li>
<li><p>Batch Normalization in Decoder: The original model does not use batch normalization, while I introduced batch normalization in the decoder. This helps stabilize the training process and improves generalization by reducing internal covariate shift.</p>
</li>
<li><p>Efficient Attention Computation: The attention mechanism in the original model uses separate layers to compute attention energy and context. In my model, I streamlined this by combining the query, key, and value projections into a single linear transformation for better efficiency.</p>
</li>
</ul>
<p>Performance Comparison:</p>
<ul>
<li><p>Training Time and Loss Convergence: My model converges faster during training, with the loss decreasing more quickly than the original model. This is likely due to the bidirectional GRU and the more efficient attention mechanism.</p>
</li>
<li><p>Training Loss: My model shows a steady decrease in loss, indicating better learning, while the original model takes longer to reduce the loss.</p>
</li>
<li><p>Translation Quality: Both models perform similarly in terms of translation quality based on a manual evaluation of 20 words. This suggests that while the architectural changes improve training speed and efficiency, they do not dramatically change translation performance in this case.</p>
</li>
<li><p>Generalization and Stability: My model is more robust due to the use of batch normalization and attention, potentially improving its ability to generalize, especially on longer or more complex sequences.</p>
</li>
</ul>
<p>In conclusion, while both models show similar translation quality, my model is more efficient and scalable, with faster convergence and better generalization, making it a more robust solution overall.</p>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
